{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# README\n",
    "\n",
    "### Purpose of this notebook\n",
    "- Create the application dataframe.\n",
    "- Preprocess application.\n",
    "\n",
    "### Steps\n",
    "\n",
    "#### Create the application dataframe\n",
    "1. Read the application text\n",
    "2. Find the width and height of each application page\n",
    "\n",
    "#### Preprocess application\n",
    "1. Split application into multi-document (currently year 111 only)\n",
    "2. Extract self-statement from application\n",
    "3. Preprocess self-statement"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Application Preprocess"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import os\n",
    "import re\n",
    "import json\n",
    "import string\n",
    "from collections import Counter\n",
    "from itertools import chain\n",
    "from importlib import reload\n",
    "\n",
    "from tqdm import tqdm\n",
    "tqdm.pandas(desc=\"progress: \")\n",
    "\n",
    "# Chinese character set\n",
    "from zhon import hanzi\n",
    "import opencc\n",
    "\n",
    "# Utility variable\n",
    "import sys\n",
    "sys.path.insert(0, '../..')\n",
    "\n",
    "# var\n",
    "import var.var as V\n",
    "import var.path as P\n",
    "\n",
    "# utils\n",
    "import utils.data as D\n",
    "import utils.io as IO\n",
    "import utils.get_path as GP\n",
    "import utils.preprocess as PP"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "MIN_CHUNK_LEN = 6\n",
    "MIN_ZH_SENT_LEN = 10\n",
    "MIN_EN_SENT_LEN = 6"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cc = opencc.OpenCC('s2tw')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pysbd\n",
    "zh_sent_segmenter = pysbd.Segmenter(language=\"zh\")\n",
    "en_sent_segmenter = pysbd.Segmenter(language=\"en\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Read Data from DataFrame"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_applications = D.read_df_applications()\n",
    "df_applications.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_applications.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Extract self-statement"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "self_statement_keyword = \"# The content is removed due to confidential concerns.\"\n",
    "self_statement_keyword_list = [\"# The content is removed due to confidential concerns.\"]\n",
    "study_plan_keyword_list = [\"# The content is removed due to confidential concerns.\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def extract_self_statement(row):\n",
    "    boundaries = row['boundaries']\n",
    "    app = row['application_pages']\n",
    "    \n",
    "    cover_pages = [app[pn] for pn in boundaries]\n",
    "    \n",
    "    self_statement_pn = -1\n",
    "    idx = -1\n",
    "    \n",
    "    ## for self-statement after year 111 (inclusive)\n",
    "    for i, (pn, page) in enumerate(zip(boundaries, cover_pages)):\n",
    "        if self_statement_keyword in page:\n",
    "            self_statement_pn = pn\n",
    "            idx = i + 1\n",
    "            break\n",
    "    \n",
    "    if self_statement_pn != -1:\n",
    "        ## for self-statement after year 111 (inclusive)\n",
    "        next_pn = boundaries[idx]\n",
    "        \n",
    "        start_page = self_statement_pn+1\n",
    "        end_page = next_pn\n",
    "        \n",
    "        ss_pages = []\n",
    "        sp_pages = []\n",
    "    else:\n",
    "        ## for self-statement before year 110 (inclusive)\n",
    "        ss_pages = [\n",
    "            pn+4 for pn, page in enumerate(app[4:]) if sum([\n",
    "                True for kw in self_statement_keyword_list if kw in page.lower().replace(' ', '')\n",
    "            ])\n",
    "        ]\n",
    "        sp_pages = [\n",
    "            pn+4 for pn, page in enumerate(app[4:]) if sum([\n",
    "                True for kw in study_plan_keyword_list if kw in page.lower().replace(' ', '')\n",
    "            ])\n",
    "        ]\n",
    "\n",
    "        if ss_pages == [] and sp_pages == []:\n",
    "            start_page = 5\n",
    "            end_page = 10\n",
    "        elif ss_pages == []:\n",
    "            end_page = sp_pages[-1] + 4\n",
    "            start_page = max(5, end_page - 10)\n",
    "        elif sp_pages == []:\n",
    "            start_page = ss_pages[0]\n",
    "            end_page = start_page+10\n",
    "        else:\n",
    "            start_page = ss_pages[0]\n",
    "            end_page = min(start_page + 10, sp_pages[-1] + 4)\n",
    "            \n",
    "    return app[start_page:end_page]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_applications['self_statement'] = df_applications.progress_apply(\n",
    "    extract_self_statement, axis=1\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_applications.reset_index(drop=True, inplace=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Preprocess self-statement"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Self-statement cleaning and sentence segmentation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def zh_self_statement_cleaning_and_sentence_spliting(ss):\n",
    "    if not ss:\n",
    "        return []\n",
    "\n",
    "    ## Convert Simplied Chinese to Traditional Chinese\n",
    "    ss = cc.convert(ss)\n",
    "    ## Remove > symbol\n",
    "    ss = ss.replace('>', '')\n",
    "    \n",
    "    ## replace english comma surrounded by Chinese characters with Chinese comma\n",
    "    re_ch_p = '[{}]'.format(hanzi.characters + hanzi.punctuation)\n",
    "    p = \"(?<={}),|,(?={})\".format(re_ch_p, re_ch_p)\n",
    "    ss = re.sub(p, '，', ss)\n",
    "    ## replace english semicolon surrounded by Chinese characters with Chinese comma\n",
    "    re_ch_p = '[{}]'.format(hanzi.characters + hanzi.punctuation)\n",
    "    p = \"(?<={});|;(?={})\".format(re_ch_p, re_ch_p)\n",
    "    ss = re.sub(p, '；', ss)\n",
    "    ## replace english exclamation mark surrounded by Chinese characters with Chinese exclamation mark\n",
    "    re_ch_p = '[{}]'.format(hanzi.characters + hanzi.punctuation)\n",
    "    p = \"(?<={})!|!(?={})\".format(re_ch_p, re_ch_p)\n",
    "    ss = re.sub(p, '！', ss)\n",
    "    ## replace english period surrounded by Chinese characters with Chinese period\n",
    "    p = \"(?<={})\\.(?=\\D)|(?<=\\D)\\.(?={})\".format(re_ch_p, re_ch_p)\n",
    "    ss = re.sub(p, '。', ss)\n",
    "    \n",
    "    ## split whitespace between chinese character (except for english ssence)\n",
    "    ss = ''.join(PP.split_whitespace_btn_ch_character(ss))\n",
    "    ## segment sentence by pybsd library\n",
    "    ss_sent = zh_sent_segmenter.segment(ss)\n",
    "    ## segment number bullet point\n",
    "    re_split_num_bullet = \"((?<!\\d)\\d+\\.(?!\\d))\"\n",
    "    ss_sent = list(chain.from_iterable([re.split(re_split_num_bullet, _s) for _s in ss_sent]))\n",
    "    ## segment chinese number bullet point\n",
    "    re_split_ch_num_bullet = \"([{}]+、)\".format(PP.CH_NUMBER)\n",
    "    ss_sent = list(chain.from_iterable([re.split(re_split_ch_num_bullet, _s) for _s in ss_sent]))\n",
    "    ## segment chinese number bullet point\n",
    "    re_split_ch_num_bullet = \"(\\([{}]+\\))\".format(PP.CH_NUMBER)\n",
    "    ss_sent = list(chain.from_iterable([re.split(re_split_ch_num_bullet, _s) for _s in ss_sent]))\n",
    "    ## segment bullet point\n",
    "    re_split_bullet = \"([{}])\".format(PP.BULLET_POINT)\n",
    "    ss_sent = list(chain.from_iterable([re.split(re_split_bullet, _s) for _s in ss_sent]))\n",
    "    \n",
    "    ## remove preceeding or trailing whitespace\n",
    "    ss_sent = [_s.strip() for _s in ss_sent]\n",
    "    \n",
    "    ## remove duplicate sentences\n",
    "    ss_sent_buf = []\n",
    "    for _s in ss_sent:\n",
    "        if _s not in ss_sent_buf:\n",
    "            ss_sent_buf.append(_s)\n",
    "    ss_sent = ss_sent_buf\n",
    "    \n",
    "    ## remove sentences that is a substring of another sentences\n",
    "    ss_sent_buf = []\n",
    "    for _s in ss_sent:\n",
    "        is_substring = False\n",
    "        for _ss in ss_sent:\n",
    "            if _s == _ss:\n",
    "                continue    \n",
    "            if _s in _ss:\n",
    "                is_substring = True\n",
    "        if not is_substring:\n",
    "            ss_sent_buf.append(_s)\n",
    "    ss_sent = ss_sent_buf\n",
    "    \n",
    "    ## hope to remove title or heading\n",
    "    ss_sent = [_s.strip() for _s in ss_sent if PP.get_sent_len(_s) > MIN_ZH_SENT_LEN]\n",
    "    \n",
    "    return ss_sent"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def en_self_statement_cleaning_and_sentence_spliting(ss):\n",
    "    if not ss:\n",
    "        return []\n",
    "\n",
    "    ss = ss.replace('\\n', ' ')\n",
    "    \n",
    "    ss_sent = en_sent_segmenter.segment(ss)\n",
    "    ss_sent = list(chain.from_iterable([en_sent_segmenter.segment(_s) for _s in ss_sent]))\n",
    "    \n",
    "    ## segment bullet point\n",
    "    re_split_bullet = \"([{}])\".format(PP.BULLET_POINT)\n",
    "    ss_sent = list(chain.from_iterable([re.split(re_split_bullet, _s) for _s in ss_sent]))\n",
    "\n",
    "    ## remove duplicate sentences\n",
    "    ss_sent_buf = []\n",
    "    for _s in ss_sent:\n",
    "        if _s not in ss_sent_buf:\n",
    "            ss_sent_buf.append(_s)\n",
    "    ss_sent = ss_sent_buf\n",
    "    \n",
    "    ## remove sentences that is a substring of another sentences\n",
    "    ss_sent_buf = []\n",
    "    for _s in ss_sent:\n",
    "        is_substring = False\n",
    "        for _ss in ss_sent:\n",
    "            if _s == _ss:\n",
    "                continue    \n",
    "            if _s in _ss:\n",
    "                is_substring = True\n",
    "        if not is_substring:\n",
    "            ss_sent_buf.append(_s)\n",
    "    ss_sent = ss_sent_buf\n",
    "    \n",
    "    ## hope to remove title or heading\n",
    "    ss_sent = [_s.strip() for _s in ss_sent if PP.get_sent_len(_s) > MIN_EN_SENT_LEN]\n",
    "    \n",
    "    return ss_sent"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def self_statement_cleaning_and_sentence_spliting(ss_pages):\n",
    "    if not ss_pages:\n",
    "        return []\n",
    "    \n",
    "    ss = ''.join(ss_pages)\n",
    "    \n",
    "    ## check the language of the document\n",
    "    zh_char_count = sum([1 for ch in ss if PP.is_zh_character(ch)])\n",
    "    zh_char_rate = zh_char_count / len(ss)\n",
    "    \n",
    "    if zh_char_rate < 0.1: ## english document preprocess\n",
    "        ss_sent = en_self_statement_cleaning_and_sentence_spliting(ss)\n",
    "    else: ## chinese document preprocess\n",
    "        ss_sent = zh_self_statement_cleaning_and_sentence_spliting(ss)\n",
    "\n",
    "    return ss_sent"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "df_applications['self_statement_sent'] = df_applications['self_statement'].progress_apply(\n",
    "    self_statement_cleaning_and_sentence_spliting\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Transformer as language model\n",
    "- Remove noise sentence by perplexity score generated by transformer (encoder-decoder model)\n",
    "    - the perplexity for generating the sentence itself\n",
    "- Source: https://gist.github.com/yuchenlin/eb63e2d0513f70cfc9bb85fa5a78953b"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from torch import Tensor"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "GPU_NUM = 0\n",
    "device = torch.device(GPU_NUM)\n",
    "\n",
    "PERPLEXITY_THRESHOLD = 5"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import MBartForConditionalGeneration, MBart50TokenizerFast"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "mbart_tokenizer = MBart50TokenizerFast.from_pretrained(\"facebook/mbart-large-50\")\n",
    "mbart_model = MBartForConditionalGeneration.from_pretrained(\"facebook/mbart-large-50\").to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def noise_sentence_removal(sents, tokenizer, model):\n",
    "    ## Can not process in batch because the loss would be merged together\n",
    "    if not sents:\n",
    "        return []\n",
    "    \n",
    "    normal_sent = []\n",
    "    \n",
    "    for sent in sents:\n",
    "        input_ids = torch.tensor(\n",
    "            tokenizer.encode(sent, truncation=True, max_length=1020)\n",
    "        ).unsqueeze(0)  # Batch size 1\n",
    "        input_ids = input_ids.to(device)\n",
    "\n",
    "        with torch.no_grad():\n",
    "            outputs = model(input_ids, labels=input_ids)\n",
    "\n",
    "        loss = outputs[0]\n",
    "        sentence_ppl = loss.item()\n",
    "        \n",
    "        if sentence_ppl < PERPLEXITY_THRESHOLD:\n",
    "            normal_sent.append(sent)\n",
    "    \n",
    "    return normal_sent"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "self_statement_sent_noise_removed = []\n",
    "\n",
    "for _, row in tqdm(df_applications.iterrows(), total=df_applications.shape[0]):\n",
    "    _year = row['year']\n",
    "    ss_sent = row['self_statement_sent']\n",
    "    \n",
    "    ss_sent = noise_sentence_removal(ss_sent, mbart_tokenizer, mbart_model)\n",
    "    self_statement_sent_noise_removed.append(ss_sent)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_applications['self_statement_sent'] = self_statement_sent_noise_removed"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Generate chunks for aligning with comment clustering model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "re_split_chunks = \"[！？｡。，；,!;?\\n]|(?<=\\D)\\.(?= )\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def self_statement_chunk_spliting(ss_sent):\n",
    "    if not ss_sent:\n",
    "        return None\n",
    "    \n",
    "    ss = '。'.join(ss_sent)\n",
    "#     s = cc.convert(s)\n",
    "    ss = ss.replace('>', '')\n",
    "    \n",
    "    ## Add 。 spliter before number bullet\n",
    "    p = '((?<!\\d)\\d+\\.(?!\\d)|[★●◆➢])'\n",
    "    ss = re.sub(p, r'。', ss)\n",
    "    \n",
    "    ## split ss_chunkence with punctuation\n",
    "    punc_list = re.findall(re_split_chunks, ss.strip())\n",
    "    ss_chunk = re.split(re_split_chunks, ss.strip())\n",
    "    \n",
    "    ## combine split sentence with punctuation\n",
    "    buf = []\n",
    "    for i in range(len(ss_chunk)):\n",
    "        try:\n",
    "            buf.append(ss_chunk[i] + punc_list[i])\n",
    "        except:\n",
    "            ## end of the list\n",
    "            buf.append(ss_chunk[i])\n",
    "    ss_chunk = buf\n",
    "    \n",
    "    ## split whitespace between chinese character (except for english sentence)\n",
    "    ss_chunk = list(chain.from_iterable([PP.split_whitespace_btn_ch_character(_s) for _s in ss_chunk]))\n",
    "    ## remove empty string\n",
    "    ss_chunk = [_s for _s in ss_chunk if not PP.is_empty_sent(_s)]\n",
    "    ## remove preceeding punctuation\n",
    "    ss_chunk = [_s if _s[0] not in (hanzi.non_stops + \"。\" + V.EN_PUNC_NON_STOPS) else _s[1:] for _s in ss_chunk]\n",
    "    ss_chunk = [_s.strip() for _s in ss_chunk]\n",
    "    ss_chunk = [_s for _s in ss_chunk if not PP.is_empty_sent(_s)]\n",
    "    ## remove trailing punctuation if it is none stop punctuation\n",
    "    ss_chunk = [_s if _s[-1] not in (hanzi.non_stops + \"。.;\" + V.EN_PUNC_NON_STOPS) else _s[:-1] for _s in ss_chunk]\n",
    "    ss_chunk = [_s.strip() for _s in ss_chunk]\n",
    "    ss_chunk = [_s for _s in ss_chunk if not PP.is_empty_sent(_s)]\n",
    "    ## remove too small chunks\n",
    "    ss_chunk = [_s for _s in ss_chunk if PP.get_sent_len(_s) > MIN_CHUNK_LEN]\n",
    "    \n",
    "    return ss_chunk"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_applications['self_statement_chunk'] = df_applications['self_statement_sent'].progress_apply(\n",
    "    self_statement_chunk_spliting\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_applications['self_statement_sent'] = df_applications['self_statement_sent'].apply(lambda l: l if l else [])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_applications['self_statement_chunk'] = df_applications['self_statement_chunk'].apply(lambda l: l if l else [])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for col in df_applications.columns:\n",
    "    print(df_applications[col].isna().value_counts())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_applications.tail()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Save the results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "D.write_df_applications(df_applications, file='csv')\n",
    "D.write_df_applications(df_applications, file='pkl')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "research_venv",
   "language": "python",
   "name": "research_venv"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
