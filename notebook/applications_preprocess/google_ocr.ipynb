{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "8310ffc1",
   "metadata": {},
   "source": [
    "# README\n",
    "\n",
    "### Purpose of this notebook\n",
    "- Use Google vision API (OCR) to detect the text content in the application in pdf format.\n",
    "\n",
    "### Steps\n",
    "0. Setup the environment for gcp and create a gcp project (not done in this notebook).\n",
    "1. Create a bucket inside the project to store application files.\n",
    "2. Upload the files to the cloud and check the result.\n",
    "3. Use google vision API to detect the text inside the application.\n",
    "4. Download the raw detection results (in a batch of json files) to local machine.\n",
    "5. Post-process the detection results and get pure text for each application.\n",
    "\n",
    "For preprocessing the text, go to `application_preprocess` notebook."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dc9132fd",
   "metadata": {},
   "outputs": [],
   "source": [
    "# gcp\n",
    "from google.cloud import storage\n",
    "from google.cloud import vision\n",
    "\n",
    "import os\n",
    "import json\n",
    "import re\n",
    "from importlib import reload\n",
    "import time\n",
    "import pickle\n",
    "import pandas as pd\n",
    "from tqdm import tqdm\n",
    "\n",
    "# Utility variable\n",
    "import sys\n",
    "sys.path.insert(0, '../..')\n",
    "\n",
    "# var\n",
    "import var.var as V\n",
    "import var.path as P\n",
    "\n",
    "# utils\n",
    "import utils.data as D\n",
    "import utils.io as IO"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5e913628",
   "metadata": {},
   "outputs": [],
   "source": [
    "PROJECT_ID = os.environ.get('GOOGLE_RESEARCH_PROJECT_ID')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ad72ba47",
   "metadata": {},
   "source": [
    "## Upload files to bucket"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f0918daa",
   "metadata": {},
   "source": [
    "### Create new bucket"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b6a0f291",
   "metadata": {},
   "outputs": [],
   "source": [
    "RESEARCH_BUCKET_NAME = 'nthu-idea-lab-jason-research'\n",
    "storage_client = storage.Client()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "52029543",
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_bucket_class_location(bucket_name, storage_class=\"COLDLINE\", location=\"ASIA-EAST1\"):\n",
    "    \"\"\"\n",
    "    Create a new bucket in the US region with the coldline storage\n",
    "    class\n",
    "    \"\"\"\n",
    "    # bucket_name = \"your-new-bucket-name\"\n",
    "\n",
    "#     storage_client = storage.Client()\n",
    "\n",
    "    bucket = storage_client.bucket(bucket_name)\n",
    "    bucket.storage_class = storage_class\n",
    "    new_bucket = storage_client.create_bucket(bucket, location=location)\n",
    "\n",
    "    print(\n",
    "        \"Created bucket {} in {} with storage class {}\".format(\n",
    "            new_bucket.name, new_bucket.location, new_bucket.storage_class\n",
    "        )\n",
    "    )\n",
    "    return new_bucket"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "19d30a02",
   "metadata": {},
   "outputs": [],
   "source": [
    "create_bucket_class_location(RESEARCH_BUCKET_NAME)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cf39e272",
   "metadata": {},
   "source": [
    "### List buckets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3339c211",
   "metadata": {},
   "outputs": [],
   "source": [
    "def list_buckets():\n",
    "    \"\"\"Lists all buckets.\"\"\"\n",
    "\n",
    "#     storage_client = storage.Client()\n",
    "    buckets = storage_client.list_buckets()\n",
    "\n",
    "    for bucket in buckets:\n",
    "        print(bucket.name)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "501c0255",
   "metadata": {},
   "outputs": [],
   "source": [
    "list_buckets()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6bd57d92",
   "metadata": {},
   "source": [
    "### List file in buckets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "688934c2",
   "metadata": {},
   "outputs": [],
   "source": [
    "def list_blobs(bucket_name):\n",
    "    \"\"\"Lists all the blobs in the bucket.\"\"\"\n",
    "    # bucket_name = \"your-bucket-name\"\n",
    "\n",
    "#     storage_client = storage.Client()\n",
    "\n",
    "    # Note: Client.list_blobs requires at least package version 1.17.0.\n",
    "    blobs = storage_client.list_blobs(bucket_name)\n",
    "\n",
    "    for blob in blobs:\n",
    "        print(blob.name)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "203446ce",
   "metadata": {},
   "outputs": [],
   "source": [
    "# list_blobs(RESEARCH_BUCKET_NAME)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4b2cfc67",
   "metadata": {},
   "outputs": [],
   "source": [
    "def list_blobs_with_prefix(bucket_name, prefix, delimiter=None, _print=True, _return=True):\n",
    "    \"\"\"Lists all the blobs in the bucket that begin with the prefix.\n",
    "\n",
    "    This can be used to list all blobs in a \"folder\", e.g. \"public/\".\n",
    "\n",
    "    The delimiter argument can be used to restrict the results to only the\n",
    "    \"files\" in the given \"folder\". Without the delimiter, the entire tree under\n",
    "    the prefix is returned. For example, given these blobs:\n",
    "\n",
    "        a/1.txt\n",
    "        a/b/2.txt\n",
    "\n",
    "    If you specify prefix ='a/', without a delimiter, you'll get back:\n",
    "\n",
    "        a/1.txt\n",
    "        a/b/2.txt\n",
    "\n",
    "    However, if you specify prefix='a/' and delimiter='/', you'll get back\n",
    "    only the file directly under 'a/':\n",
    "\n",
    "        a/1.txt\n",
    "\n",
    "    As part of the response, you'll also get back a blobs.prefixes entity\n",
    "    that lists the \"subfolders\" under `a/`:\n",
    "\n",
    "        a/b/\n",
    "    \"\"\"\n",
    "\n",
    "#     storage_client = storage.Client()\n",
    "\n",
    "    # Note: Client.list_blobs requires at least package version 1.17.0.\n",
    "    blobs = storage_client.list_blobs(bucket_name, prefix=prefix, delimiter=delimiter)\n",
    "\n",
    "    blob_names = []\n",
    "    for blob in blobs:\n",
    "        blob_names.append(blob.name)\n",
    "    \n",
    "    if _print:\n",
    "        print(\"Blobs:\")\n",
    "        for blob in blob_names:\n",
    "            print(blob)\n",
    "\n",
    "        if delimiter:\n",
    "            print(\"Prefixes:\")\n",
    "            for prefix in blobs.prefixes:\n",
    "                print(prefix)\n",
    "    \n",
    "    if _return:\n",
    "        return blob_names"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b7cc379a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# list_blobs_with_prefix(\n",
    "#     bucket_name=RESEARCH_BUCKET_NAME, \n",
    "#     prefix='', \n",
    "#     delimiter='/'\n",
    "# )"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0e6efc84",
   "metadata": {},
   "source": [
    "### Upload file to bucket"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e29f5b6c",
   "metadata": {},
   "outputs": [],
   "source": [
    "def upload_blob(bucket_name, source_file_name, destination_blob_name):\n",
    "    \"\"\"Uploads a file to the bucket.\"\"\"\n",
    "    # The ID of your GCS bucket\n",
    "    # bucket_name = \"your-bucket-name\"\n",
    "    # The path to your file to upload\n",
    "    # source_file_name = \"local/path/to/file\"\n",
    "    # The ID of your GCS object\n",
    "    # destination_blob_name = \"storage-object-name\"\n",
    "\n",
    "#     storage_client = storage.Client()\n",
    "    bucket = storage_client.bucket(bucket_name)\n",
    "    blob = bucket.blob(destination_blob_name)\n",
    "\n",
    "    blob.upload_from_filename(source_file_name)\n",
    "\n",
    "    print(\n",
    "        \"File {} uploaded to {}.\".format(\n",
    "            source_file_name, destination_blob_name\n",
    "        )\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "94474c11",
   "metadata": {},
   "outputs": [],
   "source": [
    "P.FP_FULL_APPLICATIONS_PDF_DIR"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6657bb62",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "## only process applications from year 110 & 111 first\n",
    "for _dir in P.FP_FULL_APPLICATIONS_PDF_DIR:\n",
    "    print(\"local directory: {}\".format(_dir))\n",
    "    \n",
    "    gcp_dir = '/'.join(_dir.split('/')[2:])\n",
    "    print(\"gcp bucket: {}\".format(gcp_dir))\n",
    "    \n",
    "    if '112' not in _dir:\n",
    "        continue\n",
    "    \n",
    "    for _file in tqdm(os.listdir(_dir)):\n",
    "        source_file_name = os.path.join(_dir, _file)\n",
    "#         print(source_file_name)\n",
    "        ## gcp Cloud Storage operates with a flat namespace \n",
    "        ## ,which means that folders don't actually exist within Cloud Storage.\n",
    "        destination_blob_name = os.path.join(gcp_dir, _file)\n",
    "        print(destination_blob_name)\n",
    "        \n",
    "        upload_blob(\n",
    "            bucket_name = RESEARCH_BUCKET_NAME, \n",
    "            source_file_name = source_file_name, \n",
    "            destination_blob_name = destination_blob_name\n",
    "        )\n",
    "        \n",
    "    IO.print_dividing_line()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f26a4a0b",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_gcs_source_uri(_year, _id, bucket_name=RESEARCH_BUCKET_NAME):\n",
    "    return 'gs://{}/data/applications/full_application/{}/pdf/{}.pdf'.format(\n",
    "        bucket_name, _year, _id\n",
    "    )\n",
    "\n",
    "def get_gcs_source_uri_prefix(_year):\n",
    "    return 'data/applications/full_application/{}/pdf/'.format(_year)\n",
    "    \n",
    "def get_gcs_destination_uri(_year, _id=None, bucket_name=RESEARCH_BUCKET_NAME):\n",
    "    if _id:\n",
    "        return 'gs://{}/data/applications/full_application/{}/txt_ocr_raw/{}/'.format(\n",
    "            bucket_name, _year, _id\n",
    "        )\n",
    "    else:\n",
    "        return 'gs://{}/data/applications/full_application/{}/txt_ocr_raw/'.format(\n",
    "            bucket_name, _year\n",
    "        )\n",
    "\n",
    "def get_gcs_destination_uri_prefix(_year, _id=None):\n",
    "    if _id:\n",
    "        return 'data/applications/full_application/{}/txt_ocr_raw/{}/'.format(_year, _id)\n",
    "    else:\n",
    "        return 'data/applications/full_application/{}/txt_ocr_raw/'.format(_year)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "67a9c63b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# list_blobs(RESEARCH_BUCKET_NAME)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4e7850c2",
   "metadata": {},
   "outputs": [],
   "source": [
    "get_gcs_source_uri_prefix(112)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0e7678b5",
   "metadata": {},
   "outputs": [],
   "source": [
    "list_blobs_with_prefix(\n",
    "    bucket_name=RESEARCH_BUCKET_NAME, \n",
    "    prefix=get_gcs_source_uri_prefix(112), \n",
    "    delimiter='',\n",
    "    _print=True,\n",
    "    _return=False\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "55b943e3",
   "metadata": {},
   "source": [
    "## Detect text in pdf files"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "311b303b",
   "metadata": {},
   "outputs": [],
   "source": [
    "def async_detect_document(gcs_source_uri, gcs_destination_uri, _print=False):\n",
    "    if _print:\n",
    "        start_time = time.time()\n",
    "        print('Source: {}, waiting for the operation to finish.'.format(gcs_source_uri))\n",
    "    \n",
    "    \"\"\"OCR with PDF/TIFF as source files on GCS\"\"\"\n",
    "    # Supported mime_types are: 'application/pdf' and 'image/tiff'\n",
    "    mime_type = 'application/pdf'\n",
    "\n",
    "    # How many pages should be grouped into each json output file.\n",
    "    batch_size = 3\n",
    "\n",
    "    client = vision.ImageAnnotatorClient()\n",
    "\n",
    "    feature = vision.Feature(\n",
    "        type_=vision.Feature.Type.DOCUMENT_TEXT_DETECTION)\n",
    "\n",
    "    gcs_source = vision.GcsSource(uri=gcs_source_uri)\n",
    "    input_config = vision.InputConfig(\n",
    "        gcs_source=gcs_source, mime_type=mime_type)\n",
    "\n",
    "    gcs_destination = vision.GcsDestination(uri=gcs_destination_uri)\n",
    "    output_config = vision.OutputConfig(\n",
    "        gcs_destination=gcs_destination, batch_size=batch_size)\n",
    "\n",
    "    async_request = vision.AsyncAnnotateFileRequest(\n",
    "        features=[feature], input_config=input_config,\n",
    "        output_config=output_config)\n",
    "\n",
    "    operation = client.async_batch_annotate_files(\n",
    "        requests=[async_request])\n",
    "    \n",
    "    ## wait for the operation to complete\n",
    "    response = operation.result()\n",
    "#     gcs_output_uri = response.output_config.gcs_destination.uri\n",
    "    if _print:\n",
    "        print(\"Output written to GCS: {}\".format(gcs_destination_uri))\n",
    "\n",
    "        end_time = time.time()\n",
    "        print(\"Execute time: {:.2f} sec\".format(end_time - start_time))\n",
    "    \n",
    "    return response"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "774fd914",
   "metadata": {},
   "source": [
    "### Convert test files"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6dcaaa0c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# _id_test_list = [\n",
    "    \"# The content is removed due to confidential concerns.\"\n",
    "# ]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c2058171",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# for _year, _id in tqdm(_id_test_list):\n",
    "#     gcs_source_uri = get_gcs_source_uri(_year, _id)\n",
    "#     gcs_destination_uri = get_gcs_destination_uri(_year, _id)\n",
    "\n",
    "#     print(gcs_source_uri)\n",
    "#     print(gcs_destination_uri)\n",
    "#     res = async_detect_document(gcs_source_uri, gcs_destination_uri)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ba411055",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# list_blobs_with_prefix(\n",
    "#     bucket_name=RESEARCH_BUCKET_NAME, \n",
    "#     prefix=get_gcs_source_uri_prefix(106), \n",
    "#     delimiter='',\n",
    "#     _print=True,\n",
    "#     _return=False\n",
    "# )"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7e799d89",
   "metadata": {},
   "source": [
    "## Convert all pdf files with OCR"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "786046f5",
   "metadata": {},
   "outputs": [],
   "source": [
    "for _year in range(112, 113):\n",
    "    gcs_source_uri_prefix = get_gcs_source_uri_prefix(_year)\n",
    "\n",
    "    blobs = list_blobs_with_prefix(\n",
    "        bucket_name=RESEARCH_BUCKET_NAME, \n",
    "        prefix=gcs_source_uri_prefix, \n",
    "        delimiter='',\n",
    "        _print=False,\n",
    "        _return=True\n",
    "    )\n",
    "    \n",
    "    print(\"Year: {}\".format(_year))\n",
    "    \n",
    "    for blob in tqdm(blobs):\n",
    "        _id = blob.split('/')[-1][:-4] ## remove file extension and get id\n",
    "        \n",
    "        gcs_source_uri = get_gcs_source_uri(_year, _id)\n",
    "        gcs_destination_uri = get_gcs_destination_uri(_year, _id)\n",
    "        \n",
    "        res = list_blobs_with_prefix(\n",
    "            bucket_name=RESEARCH_BUCKET_NAME, \n",
    "            prefix=get_gcs_destination_uri_prefix(_year, _id), \n",
    "            delimiter='',\n",
    "            _print=False,\n",
    "            _return=True\n",
    "        )\n",
    "        \n",
    "        ## no need to convert if the file is already converted\n",
    "        if len(res) > 0:\n",
    "#             print(_id)\n",
    "            continue\n",
    "        \n",
    "#         print(gcs_source_uri)\n",
    "#         print(gcs_destination_uri)\n",
    "        \n",
    "        _ = async_detect_document(gcs_source_uri, gcs_destination_uri)\n",
    "        \n",
    "    IO.print_dividing_line()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b6890042",
   "metadata": {},
   "source": [
    "## List converted files"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d46143a4",
   "metadata": {},
   "outputs": [],
   "source": [
    "list_blobs_with_prefix(\n",
    "    bucket_name=RESEARCH_BUCKET_NAME, \n",
    "    prefix=get_gcs_destination_uri_prefix(112), \n",
    "    delimiter='',\n",
    "    _print=True,\n",
    "    _return=False\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "831705c1",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "list_blobs_with_prefix(\n",
    "    bucket_name=RESEARCH_BUCKET_NAME, \n",
    "    prefix=get_gcs_destination_uri_prefix(112), \n",
    "    delimiter='',\n",
    "    _print=True,\n",
    "    _return=False\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b1bda494",
   "metadata": {},
   "source": [
    "### Download the detect results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b6771fa5",
   "metadata": {},
   "outputs": [],
   "source": [
    "def download_blob(bucket_name, source_blob_name, destination_file_name, _print=False):\n",
    "    \"\"\"Downloads a blob from the bucket.\"\"\"\n",
    "    # The ID of your GCS bucket\n",
    "    # bucket_name = \"your-bucket-name\"\n",
    "\n",
    "    # The ID of your GCS object\n",
    "    # source_blob_name = \"storage-object-name\"\n",
    "\n",
    "    # The path to which the file should be downloaded\n",
    "    # destination_file_name = \"local/path/to/file\"\n",
    "\n",
    "#     storage_client = storage.Client()\n",
    "\n",
    "    bucket = storage_client.bucket(bucket_name)\n",
    "\n",
    "    # Construct a client side representation of a blob.\n",
    "    # Note `Bucket.blob` differs from `Bucket.get_blob` as it doesn't retrieve\n",
    "    # any content from Google Cloud Storage. As we don't need additional data,\n",
    "    # using `Bucket.blob` is preferred here.\n",
    "    blob = bucket.blob(source_blob_name)\n",
    "    blob.download_to_filename(destination_file_name)\n",
    "\n",
    "    if _print:\n",
    "        print(\n",
    "            \"Downloaded storage object {} from bucket {} to local file {}.\".format(\n",
    "                source_blob_name, bucket_name, destination_file_name\n",
    "            )\n",
    "        )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4b1e3a84",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "for _year in range(112, 113):\n",
    "    print(\"Year: {}\".format(_year))\n",
    "    \n",
    "    gcs_destination_uri_prefix = get_gcs_destination_uri_prefix(_year)\n",
    "#     print(\"Bucket source prefix: {}\".format(gcs_destination_uri_prefix))\n",
    "    \n",
    "    blobs = list_blobs_with_prefix(\n",
    "        bucket_name=RESEARCH_BUCKET_NAME, \n",
    "        prefix=gcs_destination_uri_prefix, \n",
    "        delimiter='',\n",
    "        _print=False,\n",
    "        _return=True\n",
    "    )\n",
    "    \n",
    "#     print(len(blobs))\n",
    "    \n",
    "    for blob in tqdm(blobs):\n",
    "        local_dir_prefix = '/'.join(blob.split('/')[:-1])\n",
    "        local_dir = os.path.join('../..', local_dir_prefix)\n",
    "        local_file_name = os.path.join('../..', blob)\n",
    "        \n",
    "        try:\n",
    "            os.makedirs(local_dir)\n",
    "        except:\n",
    "            pass\n",
    "        \n",
    "        download_blob(\n",
    "            bucket_name = RESEARCH_BUCKET_NAME, \n",
    "            source_blob_name = blob,\n",
    "            destination_file_name = local_file_name\n",
    "        )\n",
    "    \n",
    "    IO.print_dividing_line()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f20c525c",
   "metadata": {},
   "source": [
    "## Post process detect results\n",
    "- organize the split responses into one single file"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "42bd8458",
   "metadata": {},
   "outputs": [],
   "source": [
    "P.FP_FULL_APPLICATIONS_TXT_OCR_DIR"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "76091ff6",
   "metadata": {},
   "outputs": [],
   "source": [
    "P.YEAR_DIRS"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "91fe6c45",
   "metadata": {},
   "outputs": [],
   "source": [
    "P.FP_FULL_APPLICATIONS_TXT_OCR_RAW_DIR"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "094a1cd6",
   "metadata": {},
   "outputs": [],
   "source": [
    "P.FP_FULL_APPLICATIONS_TXT_OCR_DIR"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "92aebfef",
   "metadata": {},
   "outputs": [],
   "source": [
    "for year, year_txt_ocr_raw_dir, year_txt_ocr_dir in zip(\n",
    "    P.YEAR_DIRS, P.FP_FULL_APPLICATIONS_TXT_OCR_RAW_DIR, P.FP_FULL_APPLICATIONS_TXT_OCR_DIR):\n",
    "    \n",
    "    if year != '112':\n",
    "        continue\n",
    "        \n",
    "    try:\n",
    "        os.makedirs(year_txt_ocr_raw_dir)\n",
    "    except:\n",
    "        pass\n",
    "    \n",
    "    try:\n",
    "        os.makedirs(year_txt_ocr_dir)\n",
    "    except:\n",
    "        pass\n",
    "    \n",
    "    print(\"Year: {}\".format(year))\n",
    "    \n",
    "    _ids = os.listdir(year_txt_ocr_raw_dir)\n",
    "    _ids = [f for f in _ids if 'ipynb_checkpoints' not in f]\n",
    "    print(_ids)\n",
    "\n",
    "    for _id in tqdm(_ids):\n",
    "#         print(_id)\n",
    "        _dir = os.path.join(year_txt_ocr_raw_dir, _id)\n",
    "    \n",
    "        files = os.listdir(_dir)\n",
    "        files = [f for f in files if 'output' in f]\n",
    "        files = sorted(files, key=lambda f: int(f.split('-')[1]))\n",
    "#         print(files)\n",
    "\n",
    "        app_texts = []\n",
    "        \n",
    "        for file in files:\n",
    "            rfp = os.path.join(_dir, file)\n",
    "#             print(rfp)\n",
    "            \n",
    "            with open(rfp, 'r') as rf:\n",
    "                res = json.load(rf)\n",
    "#                 print(fp)\n",
    "                for page in res['responses']:\n",
    "                    try:\n",
    "                        page_text = page['fullTextAnnotation']['text']\n",
    "                    except:\n",
    "                        page_text = \"\"\n",
    "                        \n",
    "                    app_texts.append(page_text)\n",
    "        \n",
    "        wfp = os.path.join(year_txt_ocr_dir, \"{}.json\".format(_id))\n",
    "        with open(wfp, 'w') as wf:\n",
    "            ## write page texts to file\n",
    "            json.dump(app_texts, wf)\n",
    "#             print(app_texts)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5e9e62c7",
   "metadata": {},
   "source": [
    "## Check the detect results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "70abe172",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "for year, year_txt_ocr_dir in zip(P.YEAR_DIRS, P.FP_FULL_APPLICATIONS_TXT_OCR_DIR):\n",
    "    if year != '112':\n",
    "        continue\n",
    "    \n",
    "    print(year)\n",
    "    \n",
    "    for app in os.listdir(year_txt_ocr_dir):\n",
    "        if \".json\" not in app:\n",
    "            continue\n",
    "        \n",
    "        print(year, app)\n",
    "        fp = os.path.join(year_txt_ocr_dir, app)\n",
    "            \n",
    "        with open(fp, 'r') as f:\n",
    "            app_texts = json.load(f)\n",
    "            print(\"Number of pages: {}\".format(len(app_texts)))\n",
    "            \n",
    "            for pn, page_text in enumerate(app_texts, 1):\n",
    "                IO.print_dividing_line(\"Page {}\".format(pn))\n",
    "                print(page_text)\n",
    "    \n",
    "        IO.print_dividing_line()\n",
    "    \n",
    "    IO.print_dividing_line()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7aa24317",
   "metadata": {},
   "source": [
    "# =========================================\n",
    "# Danger Zone\n",
    "# ========================================="
   ]
  },
  {
   "cell_type": "markdown",
   "id": "91752c9b",
   "metadata": {},
   "source": [
    "## Rename File"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4de82224",
   "metadata": {},
   "outputs": [],
   "source": [
    "def rename_blob(bucket_name, blob_name, new_name):\n",
    "    \"\"\"Renames a blob.\"\"\"\n",
    "    # The ID of your GCS bucket\n",
    "    # bucket_name = \"your-bucket-name\"\n",
    "    # The ID of the GCS object to rename\n",
    "    # blob_name = \"your-object-name\"\n",
    "    # The new ID of the GCS object\n",
    "    # new_name = \"new-object-name\"\n",
    "\n",
    "    storage_client = storage.Client()\n",
    "    bucket = storage_client.bucket(bucket_name)\n",
    "    blob = bucket.blob(blob_name)\n",
    "\n",
    "    new_blob = bucket.rename_blob(blob, new_name)\n",
    "\n",
    "    print(\"Blob {} has been renamed to {}\".format(blob.name, new_blob.name))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8ff6e217",
   "metadata": {},
   "source": [
    "### DELETE OBJECT"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6a29fdda",
   "metadata": {},
   "outputs": [],
   "source": [
    "def delete_blob(bucket_name, blob_name):\n",
    "    \"\"\"Deletes a blob from the bucket.\"\"\"\n",
    "    # bucket_name = \"your-bucket-name\"\n",
    "    # blob_name = \"your-object-name\"\n",
    "\n",
    "    storage_client = storage.Client()\n",
    "\n",
    "    bucket = storage_client.bucket(bucket_name)\n",
    "    blob = bucket.blob(blob_name)\n",
    "    blob.delete()\n",
    "\n",
    "    print(\"Blob {} deleted.\".format(blob_name))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9f92bd3b",
   "metadata": {},
   "source": [
    "### DELETE BUCKET"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "423f9ec6",
   "metadata": {},
   "outputs": [],
   "source": [
    "def delete_bucket(bucket_name):\n",
    "    \"\"\"Deletes a bucket. The bucket must be empty.\"\"\"\n",
    "    # bucket_name = \"your-bucket-name\"\n",
    "\n",
    "#     storage_client = storage.Client()\n",
    "\n",
    "    bucket = storage_client.get_bucket(bucket_name)\n",
    "    bucket.delete()\n",
    "\n",
    "    print(\"Bucket {} deleted\".format(bucket.name))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "research_venv",
   "language": "python",
   "name": "research_venv"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
