{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# README\n",
    "\n",
    "### Purpose of this notebook\n",
    "- Preprocess recommendation letters."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Application Preprocess"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import os\n",
    "import re\n",
    "import string\n",
    "from itertools import chain\n",
    "from collections import defaultdict\n",
    "from importlib import reload\n",
    "\n",
    "from tqdm import tqdm\n",
    "tqdm.pandas(desc=\"progress: \")\n",
    "\n",
    "# Chinese character set\n",
    "from zhon import hanzi\n",
    "import opencc\n",
    "\n",
    "# Utility variable\n",
    "import sys\n",
    "sys.path.insert(0, '../..')\n",
    "\n",
    "# var\n",
    "import var.var as V\n",
    "import var.path as P\n",
    "\n",
    "# utils\n",
    "import utils.coverage as C\n",
    "import utils.data as D\n",
    "import utils.io as IO\n",
    "import utils.get_path as GP\n",
    "import utils.preprocess as PP"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "MIN_CHUNK_LEN = 6"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cc = opencc.OpenCC('s2tw')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## CKIP tools"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from ckip_transformers.nlp import CkipWordSegmenter, CkipPosTagger, CkipNerChunker"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "ws_driver  = CkipWordSegmenter(device=0)\n",
    "pos_driver = CkipPosTagger(device=0)\n",
    "ner_driver = CkipNerChunker(device=0)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Read Data from DataFrame"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_applications = D.read_df_applications()\n",
    "df_applications.tail()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "df_recommendation_letters = D.read_df_recommendation_letters()\n",
    "df_recommendation_letters.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def recommendation_letters_paragraph_combine(row):\n",
    "    row = row.dropna()\n",
    "    row = [str(p) for p in row[3:9]]\n",
    "    return '\\n'.join(row)\n",
    "    \n",
    "df_recommendation_letters['all_paragraph_sent'] = \\\n",
    "    df_recommendation_letters.apply(recommendation_letters_paragraph_combine, axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "df_recommendation_letters.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Extract and map the information of reference for each individual recommendation letter"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Map the recommendation letter between dataframe and application page"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import jieba"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def extract_application_recommendation_letter_span(row, debug=False):\n",
    "    try:\n",
    "        rl_span_idx = row['section_span']['推薦信']\n",
    "        rl_span = row['application_pages'][rl_span_idx[0]:rl_span_idx[1]]\n",
    "    except:\n",
    "        rl_span = []\n",
    "    \n",
    "    if debug:\n",
    "        print(rl_span)\n",
    "        pass\n",
    "\n",
    "    return rl_span"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def extract_application_recommendation_letters(row, debug=False):\n",
    "    _year = row['year']\n",
    "    \n",
    "    rl_span = extract_application_recommendation_letter_span(row, debug)\n",
    "    \n",
    "    rl_fp_idx = []\n",
    "    rl_title = \"Letter of Reference\"\n",
    "\n",
    "    for pid, page in enumerate(rl_span):\n",
    "        if rl_title in page:\n",
    "            rl_fp_idx.append(pid)\n",
    "            \n",
    "    rls = []\n",
    "    \n",
    "    if _year == 111:\n",
    "        for i in range(len(rl_fp_idx)):\n",
    "            try:\n",
    "                rls.append(rl_span[rl_fp_idx[i]:rl_fp_idx[i+1]])\n",
    "            except:\n",
    "                rls.append(rl_span[rl_fp_idx[i]:])\n",
    "    elif _year >= 112:\n",
    "        for i in range(len(rl_fp_idx)):\n",
    "            try:\n",
    "                rls.append(rl_span[rl_fp_idx[i]:rl_fp_idx[i+2]])\n",
    "            except:\n",
    "                rls.append(rl_span[rl_fp_idx[i]:])\n",
    "                \n",
    "    if debug:\n",
    "        print(rl_fp_idx)\n",
    "        pass\n",
    "            \n",
    "    return rls, rl_fp_idx"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def map_recommendation_letters(row, debug=False):\n",
    "    _year = row['year']\n",
    "    _id = row['id']\n",
    "    \n",
    "    ## only process recommendation letter after year 111\n",
    "    if _year < 111:\n",
    "        return\n",
    "    \n",
    "    df_rls = df_recommendation_letters.query('`year` == {} and `id` == {}'.format(_year, _id))\n",
    "    app_rls, app_rl_fp_idx = extract_application_recommendation_letters(row, debug)\n",
    "    \n",
    "    if debug:\n",
    "        print(_year, _id)\n",
    "        pass\n",
    "    \n",
    "    ## calculate the coverage score of the recommendation letters \n",
    "    ## between dataframe and application pages \n",
    "    map_result_buf = {}\n",
    "    for df_rl_idx, df_rl in df_rls.iterrows():\n",
    "        key = ''.join(df_rl.to_list()[3:9])\n",
    "        key = jieba.lcut(key)\n",
    "    \n",
    "        rls_coverage_score = {}\n",
    "        for app_rl_idx, app_rl in zip(app_rl_fp_idx, app_rls):\n",
    "            if key == []:\n",
    "                coverage = 0\n",
    "            else:\n",
    "                target = ''.join(app_rl)\n",
    "                coverage = C.calculate_coverage(key, target)\n",
    "            rls_coverage_score[app_rl_idx] = coverage\n",
    "        \n",
    "        map_result_buf[df_rl_idx] = rls_coverage_score\n",
    "    \n",
    "    df_map_result = pd.DataFrame(map_result_buf)\n",
    "    \n",
    "    if debug:\n",
    "        print(df_map_result)\n",
    "    \n",
    "    ## find the mapped recommendation letters\n",
    "    map_result = defaultdict(None)\n",
    "    for app_rl_idx, row in df_map_result.iterrows():\n",
    "        df_rl_idx_max = row.idxmax()\n",
    "        map_result[df_rl_idx_max] = app_rl_idx\n",
    "    ## deal with non-mapped outliers of recommendation letter from dataframe\n",
    "    for df_rl_idx in df_map_result.columns:\n",
    "        try:\n",
    "            map_result[df_rl_idx]\n",
    "        except:\n",
    "            map_result[df_rl_idx] = None\n",
    "    \n",
    "    if debug:\n",
    "        print(map_result)\n",
    "        \n",
    "    return map_result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "map_app_df_rl_series = df_applications.apply(map_recommendation_letters, axis=1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Extract the information of reference for each individual recommendation letter"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def extract_recommendation_letter_information(row, debug=False):\n",
    "    _year = row['year']\n",
    "    _id = row['id']\n",
    "    \n",
    "    if debug:\n",
    "        print(_year, _id)\n",
    "\n",
    "    rls, rl_fp_idx = extract_application_recommendation_letters(row, debug)\n",
    "    \n",
    "    if debug:\n",
    "        rls, rl_fp_idx\n",
    "    \n",
    "    ## extract the information of the reference\n",
    "    info_start = \"服務機關\"\n",
    "    info_end = \"推薦人填寫部份\"\n",
    "    \n",
    "    info_list = {}\n",
    "    \n",
    "    ## remove unnecessary information\n",
    "    for rl, idx in zip(rls, rl_fp_idx):\n",
    "        page = ''.join(rl)\n",
    "\n",
    "        info_si = page.find(info_start)\n",
    "        info_ei = page.find(info_end)\n",
    "\n",
    "        info = page[info_si:info_ei]\n",
    "\n",
    "        remove_keywords = [\n",
    "            '-', '推薦人', 'Information', 'lof', 'of', 'Reference', '姓名', 'Name',\n",
    "            '服務機關', 'Institute', '職稱', 'Position', '電話', 'Phone', 'Number', \n",
    "            '電子郵件', 'mail Address', 'E',\n",
    "            '\\d{2,}',\n",
    "            '[{}]'.format(string.punctuation),\n",
    "        ]\n",
    "        \n",
    "        remove_keywords_sub = [\n",
    "            '清華學院學士班甲組', '畢業', 'Background'\n",
    "        ]\n",
    "\n",
    "        infos = info.split('\\n')\n",
    "\n",
    "        for i in range(len(infos)):        \n",
    "            for rkw in remove_keywords:\n",
    "                if \"、\" in infos[i] or '@' in infos[i]:\n",
    "                    infos[i] = ''\n",
    "\n",
    "                infos[i] = re.sub(rkw, '', infos[i])\n",
    "                \n",
    "            for rkw in remove_keywords_sub:\n",
    "                if rkw in infos[i]:\n",
    "                    infos[i] = ''\n",
    "\n",
    "        infos = [info.strip() for info in infos if info]\n",
    "        infos = [info for info in infos if len(info) >= 2]\n",
    "            \n",
    "        ## for 112, find name of info\n",
    "        if _year == 112:\n",
    "            remain = rl[0].find(\"姓名\", page.find(\"姓名\")+2)\n",
    "            name_candidates = rl[0][remain:].split('\\n')\n",
    "            name_candidates = [s.replace(\"姓名\", '') for s in name_candidates]\n",
    "            name_candidates = [s.replace(\"Name\", '') for s in name_candidates]\n",
    "            name_candidates = [re.sub('[{}]'.format(string.punctuation), '', s) for s in name_candidates]\n",
    "            name_candidates = [s.strip() for s in name_candidates]\n",
    "            name_candidates = [s for s in name_candidates if s]\n",
    "\n",
    "            ## find name\n",
    "            ws  = ws_driver(name_candidates, batch_size=1024, show_progress=False)\n",
    "            pos = pos_driver(ws, batch_size=1024, show_progress=False)\n",
    "            ner = ner_driver(name_candidates, batch_size=1024, show_progress=False)\n",
    "            \n",
    "            if debug:\n",
    "                print(name_candidates)\n",
    "                print(\"start to find name\")\n",
    "            \n",
    "            name = \"\"\n",
    "            for sentence, sentence_ws, sentence_pos, sentence_ner in zip(name_candidates, ws, pos, ner):\n",
    "                if debug:\n",
    "                    print(sentence)\n",
    "\n",
    "                ## find NER with 'PERSON'\n",
    "                only_contain_person = False\n",
    "\n",
    "                ners = []\n",
    "                for entity in sentence_ner:\n",
    "                    ners.append(entity.ner)\n",
    "                    if debug:\n",
    "                        print(entity)\n",
    "\n",
    "                if debug:\n",
    "                    print(pack_ws_pos_sentece(sentence_ws, sentence_pos))\n",
    "                    print()\n",
    "\n",
    "                if ners != ['PERSON']:\n",
    "                    continue\n",
    "\n",
    "                ## check pos is 'Nb' or 'FW' + 'WHITESPACE'\n",
    "                if set(sentence_pos) == {'Nb'} or set(sentence_pos) == {'FW', 'WHITESPACE'}:\n",
    "                    name = sentence\n",
    "                    if debug:\n",
    "                        print(\"name:\", name)\n",
    "                    break\n",
    "            \n",
    "            if name != \"\":\n",
    "                infos.insert(0, name)\n",
    "    \n",
    "        info_list[idx] = infos\n",
    "    \n",
    "        if debug:\n",
    "            print(\"Final info:\")\n",
    "            for info in infos:\n",
    "                print(info)\n",
    "            print('--')\n",
    "    \n",
    "    if debug:\n",
    "        IO.print_dividing_line()\n",
    "        \n",
    "    return info_list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "app_rl_info_series = df_applications.progress_apply(extract_recommendation_letter_information, axis=1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Match the information of reference to the recommendation letter dataframe"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "map_app_df_rl_series"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "app_rl_info_series"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "mapped_info = defaultdict(list)\n",
    "\n",
    "for mapping, info_dict in zip(map_app_df_rl_series, app_rl_info_series):\n",
    "    if not mapping:\n",
    "        continue\n",
    "#     print(mapping, info_dict)\n",
    "    \n",
    "    for df_rl_idx, app_rl_idx in mapping.items():\n",
    "        try:\n",
    "            info = info_dict[app_rl_idx]\n",
    "        except:\n",
    "            info = []\n",
    "        mapped_info[df_rl_idx] = info"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "mapped_info"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "list_infos = []\n",
    "\n",
    "for idx, _ in df_recommendation_letters.iterrows():\n",
    "    list_infos.append(mapped_info[idx])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "df_recommendation_letters['info'] = list_infos"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_recommendation_letters['info'].value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "df_recommendation_letters.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "df_recommendation_letters.tail()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### test data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# test_tuple = [\n",
    "#    \"# The content is removed due to confidential concerns.\"\n",
    "# ]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Preprocess recommendation letters sentences"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def recommendation_letter_preprocess(text):\n",
    "    re_ch_p = '[{}]'.format(hanzi.characters + hanzi.punctuation)\n",
    "    \n",
    "    ## replace english comma surrounded by Chinese characters with Chinese comma\n",
    "    p = \"(?<={}),|,(?={})\".format(re_ch_p, re_ch_p)\n",
    "    text = re.sub(p, '，', text)\n",
    "    ## replace english semicolon surrounded by Chinese characters with Chinese comma\n",
    "    p = \"(?<={});|;(?={})\".format(re_ch_p, re_ch_p)\n",
    "    text = re.sub(p, '；', text)\n",
    "    ## replace english period surrounded by Chinese characters with Chinese period\n",
    "    p = \"(?<={})\\.(?=\\D)|(?<=\\D)\\.(?={})\".format(re_ch_p, re_ch_p)\n",
    "    text = re.sub(p, '。', text)\n",
    "    ## replace '&amp;' with '&'\n",
    "    text = re.sub('&amp;', '&', text)\n",
    "            \n",
    "    return text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# for _year, _id, _ in test_tuple:\n",
    "#     row = df_recommendation_letters.query('`year` == {} and `id` == {}'.format(_year, _id))\n",
    "#     text = row['all_paragraph_sent'].to_list()[1]\n",
    "#     text = recommendation_letter_preprocess(text)\n",
    "    \n",
    "#     print(text)\n",
    "        \n",
    "#     IO.print_dividing_line()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_recommendation_letters['all_paragraph_sent'] = df_recommendation_letters['all_paragraph_sent'].progress_apply(recommendation_letter_preprocess)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Split into sentences with Chinese and english punctuation\n",
    "- Can not use nltk to tokenize Chinese sentences"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "re_split_stop_punc = \"([！？｡。；!;?])\"\n",
    "re_split_eng_period = \"((?<!\\d)\\.)\"\n",
    "re_split_num_bullet = \"((?<!\\d)\\d+\\.(?!\\d))\"\n",
    "re_split_ch_num_bullet = \"([一二三四五六七八九十壹貳參肆伍陸柒捌玖拾]、)\"\n",
    "re_split_bullet = \"([★●◆➢])\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def recommendation_letter_split_sentences(sent):\n",
    "    if not sent:\n",
    "        return []\n",
    "    \n",
    "    if type(sent) == str:\n",
    "        sent = [sent]\n",
    "    \n",
    "    def split_paragraph(paragraph, p, punc_location):\n",
    "        ## split sentence with punctuation\n",
    "        punc_list = re.findall(p, paragraph.strip())\n",
    "        buf_sent = re.split(p, paragraph.strip())\n",
    "        \n",
    "        ## combine split sentence with punctuation\n",
    "        if punc_location == \"back\":\n",
    "            p_sent = []\n",
    "            i = 0\n",
    "        elif punc_location == \"front\":\n",
    "            try:\n",
    "                p_sent = [buf_sent[0]]\n",
    "            except:\n",
    "                pass\n",
    "            i = 1\n",
    "            \n",
    "        while i < len(buf_sent):\n",
    "            try:\n",
    "                p_sent.append(buf_sent[i] + buf_sent[i+1])\n",
    "                i += 2\n",
    "            except:\n",
    "                ## end of the list\n",
    "                p_sent.append(buf_sent[i])\n",
    "                i += 1\n",
    "        \n",
    "        return p_sent\n",
    "        \n",
    "    ## split paragraph into sentences\n",
    "    sent = list(chain.from_iterable([split_paragraph(_s, re_split_stop_punc, \"back\") for _s in sent]))\n",
    "    sent = list(chain.from_iterable([split_paragraph(_s, re_split_eng_period, \"back\") for _s in sent]))\n",
    "    sent = list(chain.from_iterable([split_paragraph(_s, re_split_bullet, \"front\") for _s in sent]))\n",
    "    sent = list(chain.from_iterable([split_paragraph(_s, re_split_num_bullet, \"front\") for _s in sent]))\n",
    "    sent = list(chain.from_iterable([split_paragraph(_s, re_split_ch_num_bullet, \"front\") for _s in sent]))\n",
    "    ## remove empty string\n",
    "    sent = [_s.strip() for _s in sent if not PP.is_empty_sent(_s)]\n",
    "    sent = [_s.strip() for _s in sent if not PP.is_empty_sent(_s)]\n",
    "    \n",
    "    ## remove duplicate sentences\n",
    "    sent_buf = []\n",
    "    for _s in sent:\n",
    "        if _s not in sent_buf:\n",
    "            sent_buf.append(_s)\n",
    "    sent = sent_buf\n",
    "    \n",
    "    ## remove sentences that is a substring of another sentences\n",
    "    sent_buf = []\n",
    "    for _s in sent:\n",
    "        is_substring = False\n",
    "        for _ss in sent:\n",
    "            if _s == _ss:\n",
    "                continue    \n",
    "            if _s in _ss:\n",
    "                is_substring = True\n",
    "        if not is_substring:\n",
    "            sent_buf.append(_s)\n",
    "    sent = sent_buf\n",
    "\n",
    "    return sent"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# for _year, _id, _ in test_tuple:\n",
    "#     row = df_recommendation_letters.query('`year` == {} and `id` == {}'.format(_year, _id))\n",
    "#     text = row['all_paragraph_sent'].to_list()[1]\n",
    "    \n",
    "#     for sent in recommendation_letter_split_sentences(text):\n",
    "#         print(sent)\n",
    "        \n",
    "#     IO.print_dividing_line()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_recommendation_letters['all_paragraph_sent'] = df_recommendation_letters['all_paragraph_sent'].progress_apply(recommendation_letter_split_sentences)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Generate chunks for aligning with comment clustering model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "re_split_chunks = \"[！？｡。，；,!;?\\n]|(?<=\\D)\\.(?= )|[一二三四五六七八九十壹貳參肆伍陸柒捌玖拾]、\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def recommendation_letter_generate_chunks(sent):\n",
    "    if not sent:\n",
    "        return []\n",
    "    \n",
    "    s = '。'.join(sent)\n",
    "    s = s.replace('>', '')\n",
    "    \n",
    "    ## Add 。 spliter before number bullet\n",
    "    p = '((?<!\\d)\\d+\\.(?!\\d)|[★●◆➢]|[一二三四五六七八九十壹貳參肆伍陸柒捌玖拾]、)'\n",
    "    s = re.sub(p, r'。', s)\n",
    "    \n",
    "    ## split sentence with punctuation\n",
    "    punc_list = re.findall(re_split_chunks, s.strip())\n",
    "    sent = re.split(re_split_chunks, s.strip())\n",
    "    \n",
    "    ## combine split sentence with punctuation\n",
    "    buf = []\n",
    "    for i in range(len(sent)):\n",
    "        try:\n",
    "            buf.append(sent[i] + punc_list[i])\n",
    "        except:\n",
    "            ## end of the list\n",
    "            buf.append(sent[i])\n",
    "    sent = buf\n",
    "    \n",
    "    ## split whitespace between chinese character (except for english sentence)\n",
    "    sent = list(chain.from_iterable([PP.split_whitespace_btn_ch_character(_s) for _s in sent]))\n",
    "    ## remove empty string\n",
    "    sent = [_s for _s in sent if not PP.is_empty_sent(_s)]\n",
    "    ## remove preceeding punctuation\n",
    "    sent = [_s if _s[0] not in (hanzi.non_stops + \"。\" + V.EN_PUNC_NON_STOPS) else _s[1:] for _s in sent]\n",
    "    sent = [_s.strip() for _s in sent]\n",
    "    sent = [_s for _s in sent if not PP.is_empty_sent(_s)]\n",
    "    ## remove trailing punctuation if it is none stop punctuation\n",
    "    sent = [_s if _s[-1] not in (hanzi.non_stops + \"。.;\" + V.EN_PUNC_NON_STOPS) else _s[:-1] for _s in sent]\n",
    "    sent = [_s.strip() for _s in sent]\n",
    "    sent = [_s for _s in sent if not PP.is_empty_sent(_s)]\n",
    "    ## remove too small chunks\n",
    "    sent = [_s for _s in sent if PP.get_sent_len(_s) > MIN_CHUNK_LEN]\n",
    "    \n",
    "    return sent"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# for _year, _id, _ in test_tuple:\n",
    "#     row = df_recommendation_letters.query('`year` == {} and `id` == {}'.format(_year, _id))\n",
    "#     sent = row['all_paragraph_sent'].to_list()[0]\n",
    "    \n",
    "#     for chunk in recommendation_letter_generate_chunks(sent):\n",
    "#         print(chunk)\n",
    "        \n",
    "#     IO.print_dividing_line()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_recommendation_letters['all_paragraph_chunk'] = df_recommendation_letters['all_paragraph_sent'].progress_apply(recommendation_letter_generate_chunks)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_recommendation_letters['all_paragraph_sent'] = df_recommendation_letters['all_paragraph_sent'].apply(\n",
    "    lambda sent: sent if sent else []\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_recommendation_letters['all_paragraph_chunk'] = df_recommendation_letters['all_paragraph_chunk'].apply(\n",
    "    lambda chunk: chunk if chunk else []\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "df_recommendation_letters.tail()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Save the results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "D.write_df_recommendation_letters(df_recommendation_letters, file='csv')\n",
    "D.write_df_recommendation_letters(df_recommendation_letters, file='pkl')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "research_venv",
   "language": "python",
   "name": "research_venv"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
