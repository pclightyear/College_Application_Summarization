{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# README\n",
    "\n",
    "### Purpose of this notebook\n",
    "- Tokenize comments with different tokenizer including:\n",
    "    - CKIP\n",
    "    - Articut\n",
    "    - MONPA (under development)\n",
    "\n",
    "## Steps\n",
    "1. Read the tokenized comment dataframe.\n",
    "2. Import the tokenizer module and tokenize the comment accordingly.\n",
    "3. (Optional) Term frequency observation."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "from importlib import reload\n",
    "\n",
    "# Utility variable\n",
    "import sys\n",
    "sys.path.insert(0, '../..')\n",
    "\n",
    "# utils\n",
    "import utils.data as D\n",
    "import utils.articut as A"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Read the comment data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_comments = D.read_df_comments()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "df_tokenized_comments = df_comments[['year', 'id', 'group', 'committee_number', 'comment', 'comment_length']]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# df_tokenized_comments = D.read_df_tokenized_comments()\n",
    "# df_tokenized_comments.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Sentence Tokenization and POS tagging with CKIP"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from ckip_transformers.nlp import CkipWordSegmenter, CkipPosTagger, CkipNerChunker"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "MODEL = 'ckiplab/albert-tiny-chinese'\n",
    "WS_MODEL = \"{}-ws\".format(MODEL)\n",
    "POS_MODEL = \"{}-pos\".format(MODEL)\n",
    "NER_MODEL = \"{}-ner\".format(MODEL)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initialize drivers\n",
    "ws_driver = CkipWordSegmenter(level=3, model_name=WS_MODEL, device=0)\n",
    "pos_driver = CkipPosTagger(level=3, model_name=POS_MODEL, device=0)\n",
    "ner_driver = CkipNerChunker(level=3, model_name=NER_MODEL, device=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if MODEL == 'ckiplab/albert-tiny-chinese':\n",
    "    BATCH_SIZE = 256\n",
    "elif MODEL == 'ckiplab/albert-base-chinese':\n",
    "    BATCH_SIZE = 128\n",
    "elif MODEL == 'ckiplab/bert-base-chinese':\n",
    "    BATCH_SIZE = 128"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# list of comments\n",
    "comments = list(df_comments.comment)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# word segmentation\n",
    "comments_ws = ws_driver(comments, batch_size=BATCH_SIZE)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_tokenized_comments['ckip_comment_ws'] = comments_ws"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# part of speech\n",
    "comments_pos = pos_driver(comments_ws, batch_size=128)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_tokenized_comments['ckip_comment_pos'] = comments_pos"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# name entity recognition\n",
    "comment_ner = ner_driver(comments, batch_size=128)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_tokenized_comments['ckip_comment_ner'] = comment_ner"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "df_tokenized_comments.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "D.write_df_tokenized_comments(df_tokenized_comments, file='csv')\n",
    "D.write_df_tokenized_comments(df_tokenized_comments, file='pkl')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Sentence Tokenization and POS tagging with Articut"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Articut lv2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "buf_lv2 = df_tokenized_comments.comment.progress_apply(A.articut_cut, lv=\"lv2\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_tokenized_comments['articut_lv2'] = buf_lv2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "D.write_df_tokenized_comments(df_tokenized_comments, file='csv')\n",
    "D.write_df_tokenized_comments(df_tokenized_comments, file='pkl')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Articut lv3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "buf_lv3 = df_tokenized_comments.comment.progress_apply(A.articut_cut, lv=\"lv3\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_tokenized_comments['articut_lv3'] = buf_lv3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "D.write_df_tokenized_comments(df_tokenized_comments, file='csv')\n",
    "D.write_df_tokenized_comments(df_tokenized_comments, file='pkl')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Articut wiki lv2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "buf_wiki_lv2 = df_tokenized_comments.comment.progress_apply(A.articut_cut, lv=\"lv2\", wikiDataBOOL=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_tokenized_comments['articut_wiki_lv2'] = buf_wiki_lv2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "D.write_df_tokenized_comments(df_tokenized_comments, file='csv')\n",
    "D.write_df_tokenized_comments(df_tokenized_comments, file='pkl')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Articut wiki lv3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "buf_wiki_lv3 = df_tokenized_comments.comment.progress_apply(A.articut_cut, lv=\"lv3\", wikiDataBOOL=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_tokenized_comments['articut_wiki_lv3'] = buf_wiki_lv3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "D.write_df_tokenized_comments(df_tokenized_comments, file='csv')\n",
    "D.write_df_tokenized_comments(df_tokenized_comments, file='pkl')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "df_tokenized_comments"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Term frequency observation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from collections import Counter"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pos_filter = ['UserDefined', 'ENTITY_noun', 'ENTITY_oov']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cnt = Counter()\n",
    "cnt_pos_filtered = Counter()\n",
    "\n",
    "for res in df_tokenized_comments.articut_lv2:\n",
    "    if res['status'] == False:\n",
    "        continue\n",
    "#     print(res)\n",
    "    \n",
    "    for sent_token in res['result_obj']:\n",
    "        for token in sent_token:\n",
    "            if token['pos'] == 'PUNCTUATION':\n",
    "                continue\n",
    "        \n",
    "#             p = token['text']\n",
    "            p = (token['text'], token['pos'])\n",
    "            cnt[p] += 1\n",
    "        \n",
    "            ## pos filter\n",
    "            for pos in pos_filter:\n",
    "                if pos in token['pos']:\n",
    "                    cnt_pos_filtered[p] += 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cnt.total()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "cnt.most_common(3000)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cnt_pos_filtered.most_common(500)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "research_venv",
   "language": "python",
   "name": "research_venv"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
