{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# README\n",
    "\n",
    "### Purpose of this notebook\n",
    "- Split comments into sentences.\n",
    "\n",
    "## Steps\n",
    "1. Finely split sentence with punctuation.\n",
    "2. Use BERT next sentence prediction to concatenate sentence back (a bottom-up approach).\n",
    "3. Perform EDA to observe the split results."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "\n",
    "import re\n",
    "import os\n",
    "import json\n",
    "from importlib import reload\n",
    "\n",
    "from itertools import chain\n",
    "from collections import defaultdict\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "from tqdm import tqdm\n",
    "tqdm.pandas(desc=\"progress: \")\n",
    "\n",
    "# Utility variable\n",
    "import sys\n",
    "sys.path.insert(0, '../..')\n",
    "\n",
    "# var\n",
    "import var.var as V\n",
    "\n",
    "# utils\n",
    "import utils.articut as A\n",
    "import utils.data as D\n",
    "import utils.io as IO\n",
    "import utils.preprocess as PP"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "with open(\"../../var/articut_dict.json\") as f:\n",
    "    keyword_dict = json.load(f)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Read the dataframe"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_comments = D.read_df_comments()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_comments.grade.value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_comments['comment_length'] = df_comments['comment'].apply(\n",
    "    lambda s: len(s) if not PP.is_empty_sent(s) else np.NaN\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Sentences Preprocess"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Split sentence with Chinese and english punctuation\n",
    "- Can not use nltk to tokenize Chinese sentences"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "import string\n",
    "from zhon import hanzi # Chinese text processing package"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'＂＃＄％＆＇（）＊＋，－／：；＜＝＞＠［＼］＾＿｀｛｜｝～｟｠｢｣､\\u3000、〃〈〉《》「」『』【】〔〕〖〗〘〙〚〛〜〝〞〟〰〾〿–—‘’‛“”„‟…‧﹏﹑﹔·'"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "hanzi.non_stops"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'！？｡。'"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "hanzi.stops"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'!;?.'"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "V.EN_PUNC_STOPS"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'(?<=[^\\\\d])(\\\\.)(?=[^\\\\d])'"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "V.NO_DIGIT_SURROUNDING_PERIOD"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'[！？｡。，、；,!;?\\n]|(?<=\\\\D)\\\\.(?= )'"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "re_split_punc = \"[！？｡。，、；,!;?\\n]|(?<=\\D)\\.(?= )\"\n",
    "re_split_punc"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "def split_sentence(s):\n",
    "    s = '\\n' + str(s)\n",
    "    \n",
    "    ## remove grade comment\n",
    "    p = '|'.join(PP.GRADE_COMMENT).replace('(', '\\(').replace(')', '\\)')\n",
    "    s = re.sub(p, '', s)\n",
    "    ## remove time token\n",
    "    p = '20\\d\\d[年]*|10[1-9][ ]*[年|年度|學年|學年度]*|11\\d[ ]*[年|年度|學年|學年度]*'\n",
    "    s = re.sub(p, '', s)\n",
    "    ## remove competition rank\n",
    "    ch_number = \"一二三四五六七八九十\"\n",
    "    p = '第[{}\\d]+名|{}'.format(ch_number, '|'.join(keyword_dict['prize']))\n",
    "    s = re.sub(p, '', s)\n",
    "    ## remove ordinal hint\n",
    "    p = '第[{}\\d]+屆'.format(ch_number)\n",
    "    s = re.sub(p, '', s)\n",
    "    ## Add \\n spliter before number bullet\n",
    "    p = '((?<!\\d)\\d+\\.(?!\\d)|★)'\n",
    "    s = re.sub(p, r'\\n',s)\n",
    "    ## replace english comma surrounded by Chinese characters with Chinese comma\n",
    "    re_ch_p = '[{}]'.format(hanzi.characters + hanzi.punctuation)\n",
    "    p = \"(?<={}),|,(?={})\".format(re_ch_p, re_ch_p)\n",
    "    s = re.sub(p, '，', s)\n",
    "    ## replace english period surrounded by Chinese characters with Chinese period\n",
    "    p = \"(?<={})\\.(?=\\D)|(?<=\\D)\\.(?={})\".format(re_ch_p, re_ch_p)\n",
    "    s = re.sub(p, '。', s)\n",
    "    \n",
    "    ## split sentence with punctuation\n",
    "    punc_list = re.findall(re_split_punc, s.strip())\n",
    "    sent = re.split(re_split_punc, s.strip())\n",
    "\n",
    "    ## combine split sentence with punctuation\n",
    "    buf = []\n",
    "    for i in range(len(sent)):\n",
    "        try:\n",
    "            buf.append(sent[i] + punc_list[i])\n",
    "        except:\n",
    "            ## end of the list\n",
    "            buf.append(sent[i])\n",
    "    sent = buf\n",
    "    \n",
    "    ## split whitespace between chinese character (except for english sentence)\n",
    "    sent = list(chain.from_iterable([split_whitespace(_s) for _s in sent]))\n",
    "    ## remove empty string\n",
    "    sent = [_s for _s in sent if not PP.is_empty_sent(_s)]\n",
    "    ## remove preceeding punctuation\n",
    "    sent = [_s if _s[0] not in (hanzi.non_stops + \"。\" + V.EN_PUNC_NON_STOPS) else _s[1:] for _s in sent]\n",
    "    sent = [_s.strip() for _s in sent]\n",
    "    sent = [_s for _s in sent if not PP.is_empty_sent(_s)]\n",
    "    ## remove trailing punctuation if it is none stop punctuation\n",
    "    sent = [_s if _s[-1] not in (hanzi.non_stops + \"。.;\" + V.EN_PUNC_NON_STOPS) else _s[:-1] for _s in sent]\n",
    "    sent = [_s.strip() for _s in sent]\n",
    "    sent = [_s for _s in sent if not PP.is_empty_sent(_s)]\n",
    "    \n",
    "    return sent\n",
    "\n",
    "def split_whitespace(s):\n",
    "    ## remove multiple whitespaces\n",
    "    s = re.sub('\\s{2,}', ' ', s)\n",
    "    \n",
    "    ## All Chinese characters and punctuations\n",
    "    re_ch_p = '[{}]'.format(hanzi.characters + hanzi.punctuation)\n",
    "    ## Whitespaces between Chinese characters and punctuations\n",
    "    ws_btn_ch = '(?<={})\\s(?={})'.format(re_ch_p, re_ch_p)\n",
    "    \n",
    "    sent = re.split(ws_btn_ch, s)\n",
    "    \n",
    "    return sent"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Testing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"# The content is removed due to confidential concerns.\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Split comments"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_comments.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_comments['split_comment'] = df_comments['comment'].apply(split_sentence)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "for _, row in df_comments.sample(100, random_state=42).iterrows():\n",
    "    _comment = row['comment']\n",
    "    _split_comment = row['split_comment']\n",
    "    \n",
    "    if not PP.is_empty_sent(_comment):\n",
    "        print(_comment)\n",
    "        print(_split_comment)\n",
    "        IO.print_dividing_line()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Merge sentences with BERT next sentence prediction"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Prepare BERT model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [],
   "source": [
    "import utils.next_sentence_prediction as NSP"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import BertTokenizer, BertForNextSentencePrediction"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_name = \"hfl/chinese-macbert-base\"\n",
    "# model_name = \"bert-base-multilingual-cased\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of the model checkpoint at hfl/chinese-macbert-base were not used when initializing BertForNextSentencePrediction: ['cls.predictions.decoder.weight', 'cls.predictions.transform.dense.bias', 'cls.predictions.transform.LayerNorm.weight', 'cls.predictions.transform.LayerNorm.bias', 'cls.predictions.bias', 'cls.predictions.decoder.bias', 'cls.predictions.transform.dense.weight']\n",
      "- This IS expected if you are initializing BertForNextSentencePrediction from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing BertForNextSentencePrediction from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n"
     ]
    }
   ],
   "source": [
    "tokenizer = BertTokenizer.from_pretrained(model_name)\n",
    "model = BertForNextSentencePrediction.from_pretrained(model_name)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Merge utility function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [],
   "source": [
    "def merge_split_sentence(split_sent):\n",
    "    if len(split_sent) <= 1:\n",
    "        return split_sent\n",
    "    \n",
    "    merged_sent = [split_sent[0]]\n",
    "    \n",
    "    for i in range(1, len(split_sent)):\n",
    "        prompt = merged_sent[-1]\n",
    "        next_sentence = split_sent[i]\n",
    "        \n",
    "        ## merge if predicted as next sentence\n",
    "        if NSP.is_next_sentence(model, tokenizer, prompt, next_sentence):\n",
    "            merged_sent[-1] = prompt + next_sentence\n",
    "        ## not merge\n",
    "        else:\n",
    "            merged_sent.append(split_sent[i])\n",
    "            \n",
    "    return merged_sent"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_comments['split_comment_nsp'] = df_comments['split_comment'].progress_apply(merge_split_sentence)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for _, row in df_comments.sample(100, random_state=42).iterrows():\n",
    "    _comment = row['comment']\n",
    "    _split_comment = row['split_comment']\n",
    "    _split_comment_nsp = row['split_comment_nsp']\n",
    "    \n",
    "    if not PP.is_empty_sent(_comment):\n",
    "        print(_comment)\n",
    "        print(_split_comment)\n",
    "        print(_split_comment_nsp)\n",
    "        IO.print_dividing_line()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Dataframe for split comments"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "split_comments_row_data = []\n",
    "split_comments_original_comment_dict = defaultdict(list)\n",
    "split_comments_applicant_dict = defaultdict(list)\n",
    "split_comments_committee_dict = defaultdict(list)\n",
    "\n",
    "for _, row in df_comments.iterrows():\n",
    "    _year = row['year']\n",
    "    _id = row['id']\n",
    "    _committee_number = row['committee_number']\n",
    "    original_comment = row['comment']\n",
    "    grade = row['grade']\n",
    "    split_comment = row['split_comment']\n",
    "    \n",
    "    if not split_comment:\n",
    "        continue\n",
    "    \n",
    "    for sc in split_comment:\n",
    "        split_comments_row_data.append({\n",
    "            \"split_comment\": sc,\n",
    "            \"grade\": grade,\n",
    "        })\n",
    "        \n",
    "        split_comments_original_comment_dict[sc].append(((_year, _id, _committee_number), original_comment))\n",
    "        split_comments_applicant_dict[sc].append((_year, _id))\n",
    "        split_comments_committee_dict[sc].append((_year, _id, _committee_number))\n",
    "        \n",
    "df_split_comments = pd.DataFrame(split_comments_row_data)\n",
    "df_split_comments"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [],
   "source": [
    "split_comments_list = df_comments['split_comment'].to_list()\n",
    "split_comments_list = [_list for _list in split_comments_list if _list]\n",
    "split_comments = list(chain.from_iterable(split_comments_list))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "len(split_comments)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_split_comments['split_comment_length'] = df_split_comments['split_comment'].apply(len)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Draw the distribution of the number of split sentences in each comment"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [],
   "source": [
    "# BINS = 10"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# plt.hist([len(l) for l in split_comments_list], bins=BINS)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Draw the length distribution of split sentences"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# plt.hist([len(c) for c in split_comments], bins=BINS)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Length below 40"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# plt.hist([len(c) for c in split_comments if len(c) < 40], bins=10)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Content of short sentences"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [],
   "source": [
    "from collections import Counter"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# counter = Counter([len(c) for c in split_comments])\n",
    "# sorted(counter.items(), key=lambda x:x[0])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 在不同的 split comment 長度中 sample split comment"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [],
   "source": [
    "# df_split_comments_length_group = df_split_comments.groupby(['split_comment_length'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# for length, group in df_split_comments_length_group:\n",
    "#     print(length)\n",
    "    \n",
    "#     num_samples = min(group.shape[0], 30)\n",
    "    \n",
    "#     print(group['split_comment'].sample(num_samples, random_state=42))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 在不同的 bin 中 sample split_comment"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [],
   "source": [
    "# bins = pd.cut(df_split_comments['split_comment_length'], BINS)\n",
    "# df_split_comments_bin_group = df_split_comments.groupby(bins)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# for _bin, group in df_split_comments_bin_group:\n",
    "#     print(_bin)\n",
    "    \n",
    "#     num_samples = min(group.shape[0], 50)\n",
    "    \n",
    "#     print(group['split_comment'].sample(num_samples, random_state=42))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Filter out comments below length threshold"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [],
   "source": [
    "LEN_THRESHOLD = 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [],
   "source": [
    "keep_idx = df_split_comments['split_comment_length'].apply(lambda l: l > LEN_THRESHOLD)\n",
    "df_split_comments = df_split_comments[keep_idx]\n",
    "\n",
    "split_comments = [c for c in split_comments if len(c) > LEN_THRESHOLD]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Split comments with no duplicate data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_split_comments_no_duplicate = df_split_comments.drop_duplicates()\n",
    "sc_duplicate_check_group = df_split_comments_no_duplicate.groupby('split_comment')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_split_comments_no_duplicate.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "## if same split comment receive different grade, then change the grade label to unknown\n",
    "new_grade_list = []\n",
    "\n",
    "for _, row in df_split_comments_no_duplicate.iterrows():\n",
    "    sc = row['split_comment']\n",
    "    grade = row['grade']\n",
    "    g = sc_duplicate_check_group.get_group(sc)\n",
    "    \n",
    "    if g.shape[0] > 1:\n",
    "        print(g)\n",
    "        new_grade_list.append('P')\n",
    "    else:\n",
    "        new_grade_list.append(grade)\n",
    "        \n",
    "df_split_comments_no_duplicate['grade'] = new_grade_list\n",
    "df_split_comments_no_duplicate = df_split_comments_no_duplicate.drop_duplicates()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_split_comments_no_duplicate.reset_index(drop=True, inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [],
   "source": [
    "split_comments_no_duplicate = df_split_comments_no_duplicate['split_comment'].to_list()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "len(split_comments_no_duplicate)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "len(split_comments)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Append applicant and committee information for calculating uniqueness"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "D.read_df_split_comments_no_duplicate()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 82,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_104536/1546132328.py:1: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  df_split_comments_no_duplicate['applicants'] = df_split_comments_no_duplicate['split_comment'].apply(\n"
     ]
    }
   ],
   "source": [
    "df_split_comments_no_duplicate['applicants'] = df_split_comments_no_duplicate['split_comment'].apply(\n",
    "    lambda sc: split_comments_applicant_dict[sc]\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 83,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_104536/425451214.py:1: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  df_split_comments_no_duplicate['committee'] = df_split_comments_no_duplicate['split_comment'].apply(\n"
     ]
    }
   ],
   "source": [
    "df_split_comments_no_duplicate['committee'] = df_split_comments_no_duplicate['split_comment'].apply(\n",
    "    lambda sc: split_comments_committee_dict[sc]\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_split_comments_no_duplicate['original_comment'] = df_split_comments_no_duplicate['split_comment'].apply(\n",
    "    lambda sc: split_comments_original_comment_dict[sc]\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Split comments tokenization with Articut"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Gather texts to create request batch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 177,
   "metadata": {},
   "outputs": [],
   "source": [
    "def is_valid_split_token(sents, split_token='＄'):\n",
    "    for sent in sents:\n",
    "        if split_token in sent:\n",
    "            return False\n",
    "        \n",
    "    return True"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 178,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_articut_requests(sents, split_token='＄', max_len=2000):\n",
    "    ## check if the split token is valid\n",
    "    if not is_valid_split_token(sents, split_token):\n",
    "        print(\"Not valid split token!\")\n",
    "        return []\n",
    "    \n",
    "    request_str_buffer = []\n",
    "    str_buf = \"\"\n",
    "    \n",
    "    for sent in sents:\n",
    "        append_str = sent + split_token\n",
    "        if len(sent) >= max_len:\n",
    "            print(\"Too long sentence detected！\")\n",
    "            return []\n",
    "        \n",
    "        if len(str_buf) + len(sent) + 1 < max_len:\n",
    "            ## append the sent into the string buffer\n",
    "            str_buf = str_buf + append_str\n",
    "        else:\n",
    "            ## flush the string buffer to request\n",
    "            request_str_buffer.append(str_buf)\n",
    "            str_buf = append_str\n",
    "    \n",
    "    request_str_buffer.append(str_buf)\n",
    "    \n",
    "    ## chech if missing any sent\n",
    "    assert len(sents) == sum(\n",
    "        [len(request_str.split(split_token)) - 1 for request_str in request_str_buffer]\n",
    "    )\n",
    "    \n",
    "    return request_str_buffer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "SPLITTER = '＄'\n",
    "request_str_buffer = create_articut_requests(split_comments_no_duplicate, split_token=SPLITTER)\n",
    "print(\"Number of requests: {}\".format(len(request_str_buffer)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "articut_res_buffer = []\n",
    "\n",
    "for request_str in tqdm(request_str_buffer):\n",
    "    res = A.articut_cut(request_str, wikiDataBOOL=True, sleep=False)\n",
    "    articut_res_buffer.append(res)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 181,
   "metadata": {},
   "outputs": [],
   "source": [
    "split_comment_articut_res_buffer = []\n",
    "\n",
    "for i, (request_str, res) in enumerate(zip(request_str_buffer, articut_res_buffer)):\n",
    "    num_str = len(request_str.split(SPLITTER)) - 1\n",
    "    \n",
    "    if res['status'] != True:\n",
    "        print(i)\n",
    "    \n",
    "    exec_time = res['exec_time']\n",
    "    result_pos = res['result_pos']\n",
    "    result_segmentation = res['result_segmentation']\n",
    "    result_obj = res['result_obj']\n",
    "    level = res['level']\n",
    "    version = res['version']\n",
    "    msg = res['msg']\n",
    "    word_count_balance = res['word_count_balance']\n",
    "    \n",
    "    ## split result_pos\n",
    "    split_result_pos = []\n",
    "    splitter = SPLITTER\n",
    "    buf = []\n",
    "    \n",
    "    for pos in result_pos:\n",
    "        if pos != splitter:\n",
    "            buf.append(pos)\n",
    "        elif pos == splitter and len(buf) > 0:\n",
    "            ## flush the buffer\n",
    "            split_result_pos.append(buf)\n",
    "            buf = []\n",
    "            \n",
    "    ## split result_segmentation\n",
    "    splitter = SPLITTER\n",
    "    split_result_segmentation = [seg for seg in result_segmentation.split(splitter) if seg != '']\n",
    "    ## remove preceeding '/'\n",
    "    split_result_segmentation = [seg[1:] if seg[0] == '/' else seg for seg in split_result_segmentation ]\n",
    "    ## remove trailing '/'\n",
    "    split_result_segmentation = [seg[:-1] if seg[-1] == '/' else seg for seg in split_result_segmentation]\n",
    "    \n",
    "    ## split result_obj\n",
    "    split_result_obj = []\n",
    "    splitter = [{'text': SPLITTER, 'pos': 'PUNCTUATION'}]\n",
    "    buf = []\n",
    "    \n",
    "    for obj in result_obj:\n",
    "        if obj != splitter:\n",
    "            buf.append(obj)\n",
    "        elif obj == splitter and len(buf) > 0:\n",
    "            ## flush the buffer\n",
    "            split_result_obj.append(buf)\n",
    "            buf = []\n",
    "            \n",
    "    assert len(split_result_pos) == num_str\n",
    "    assert len(split_result_segmentation) == num_str\n",
    "    assert len(split_result_obj) == num_str\n",
    "    \n",
    "    for pos, seg, obj in zip(split_result_pos, split_result_segmentation, split_result_obj):\n",
    "        split_comment_articut_res_buffer.append({\n",
    "            'exec_time': exec_time / len(split_result_pos),\n",
    "            'result_pos': pos,\n",
    "            'result_segmentation': seg,\n",
    "            'result_obj': obj,\n",
    "            'level': level,\n",
    "            'version': version,\n",
    "            'status': True,\n",
    "            'msg': msg,\n",
    "            'word_count_balance': word_count_balance,\n",
    "        })\n",
    "        \n",
    "assert len(split_comment_articut_res_buffer) == len(split_comments_no_duplicate)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 182,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_split_comments_no_duplicate['articut_wiki_lv2'] = split_comment_articut_res_buffer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_split_comments_no_duplicate.tail()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "df_split_comments_no_duplicate"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Observe split comment with next sentence prediction"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 115,
   "metadata": {},
   "outputs": [],
   "source": [
    "split_comments_nsp_list = df_comments['split_comment_nsp'].to_list()\n",
    "split_comments_nsp_list = [_list for _list in split_comments_nsp_list if _list != ['0']]\n",
    "split_comments_nsp = list(chain.from_iterable(split_comments_nsp_list))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# len(split_comments_nsp)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# df_split_comments_nsp = pd.DataFrame({\"split_comment_nsp\": split_comments_nsp})\n",
    "# df_split_comments_nsp"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {},
   "outputs": [],
   "source": [
    "# df_split_comments_nsp['split_comment_nsp_length'] = \\\n",
    "#     df_split_comments_nsp['split_comment_nsp'].apply(len)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Draw the distribution of the number of split sentences in each comment"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {},
   "outputs": [],
   "source": [
    "# BINS = 10"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# plt.hist([len(l) for l in split_comments_nsp_list], bins=BINS)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Draw the length distribution of split sentences"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# plt.hist([len(c) for c in split_comments_nsp], bins=BINS)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Length below 50"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# plt.hist([len(c) for c in split_comments_nsp if len(c) < 50], bins=10)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Content of short sentences"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "metadata": {},
   "outputs": [],
   "source": [
    "from collections import Counter"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# counter = Counter([len(c) for c in split_comments_nsp])\n",
    "# sorted(counter.items(), key=lambda x:x[0])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 在不同的 split comment 長度中 sample split comment"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# df_split_comments_nsp_length_group = df_split_comments_nsp.groupby(['split_comment_nsp_length'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# for length, group in df_split_comments_nsp_length_group:\n",
    "#     print(length)\n",
    "    \n",
    "#     num_samples = min(group.shape[0], 30)\n",
    "    \n",
    "#     print(group['split_comment_nsp'].sample(num_samples, random_state=42))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 在不同的 bin 中 sample split_comment"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# bins = pd.cut(df_split_comments_nsp['split_comment_nsp_length'], BINS)\n",
    "# df_split_comments_nsp_bin_group = df_split_comments_nsp.groupby(bins)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# for _bin, group in df_split_comments_nsp_bin_group:\n",
    "#     print(_bin)\n",
    "    \n",
    "#     num_samples = min(group.shape[0], 50)\n",
    "    \n",
    "#     print(group['split_comment_nsp'].sample(num_samples, random_state=42))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Filter out comments with lenght threshold"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "metadata": {},
   "outputs": [],
   "source": [
    "LEN_THRESHOLD = 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "metadata": {},
   "outputs": [],
   "source": [
    "split_comments_nsp = [c for c in split_comments_nsp if len(c) > LEN_THRESHOLD]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Split comments with no duplicate data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "metadata": {},
   "outputs": [],
   "source": [
    "split_comments_nsp_no_duplicate = list(set(split_comments_nsp))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "len(split_comments_nsp_no_duplicate)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Write split comments"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "metadata": {},
   "outputs": [],
   "source": [
    "## High frequency used\n",
    "D.write_df_comments(df_comments)\n",
    "D.write_df_split_comments_no_duplicate(df_split_comments_no_duplicate)\n",
    "D.write_split_comments_no_duplicate(split_comments_no_duplicate)\n",
    "\n",
    "## Not used\n",
    "D.write_df_split_comments(df_split_comments)\n",
    "D.write_split_comments(split_comments)\n",
    "D.write_split_comments_nsp(split_comments_nsp)\n",
    "D.write_split_comments_nsp_no_duplicate(split_comments_nsp_no_duplicate)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "research_venv",
   "language": "python",
   "name": "research_venv"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
