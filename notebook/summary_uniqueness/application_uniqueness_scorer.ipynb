{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6f564250",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import os\n",
    "from collections import defaultdict\n",
    "import matplotlib.pyplot as plt\n",
    "from itertools import product, chain\n",
    "from joblib import Parallel, delayed\n",
    "\n",
    "from tqdm import tqdm\n",
    "tqdm.pandas(desc=\"progress: \")\n",
    "\n",
    "from importlib import reload\n",
    "import pickle\n",
    "\n",
    "# Utility variable\n",
    "import sys, getopt\n",
    "sys.path.insert(0, '../..')\n",
    "\n",
    "# var\n",
    "import var.var as V\n",
    "import var.path as P\n",
    "\n",
    "# utils\n",
    "import utils.data as D\n",
    "import utils.io as IO\n",
    "import utils.preprocess as PP\n",
    "import utils.torch as Tor"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "230c5a7b",
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch.utils.data import Dataset, DataLoader"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bc42cc7d",
   "metadata": {},
   "outputs": [],
   "source": [
    "os.environ['TOKENIZERS_PARALLELISM']= 'false'"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "af86f650",
   "metadata": {},
   "source": [
    "## Process Command Line Arguments"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ad61fe33",
   "metadata": {},
   "outputs": [],
   "source": [
    "opts, args = getopt.getopt(sys.argv[1:], \"ab:s:e:f:r:n:d\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ca8046fd",
   "metadata": {},
   "outputs": [],
   "source": [
    "TRAIN_OR_ALL = 'train'\n",
    "BATCH_SIZE = 70\n",
    "START_IDX = 2080\n",
    "END_IDX = 2091\n",
    "RADIUS = 7.5\n",
    "N_NEIGHBORS = 30\n",
    "debug = False\n",
    "\n",
    "for opt, arg in opts:\n",
    "    if opt == '-a':\n",
    "        TRAIN_OR_ALL = 'all'\n",
    "    elif opt == '-b':\n",
    "        BATCH_SIZE\n",
    "    elif opt == '-s':\n",
    "        START_IDX = int(arg)\n",
    "    elif opt == '-e':\n",
    "        END_IDX = int(arg)\n",
    "    elif opt == '-r':\n",
    "        RADIUS = float(arg)\n",
    "    elif opt == '-n':\n",
    "        N_NEIGHBORS = int(arg)\n",
    "    elif opt == '-f':\n",
    "        debug = True\n",
    "    elif opt == '-d':\n",
    "        debug = True"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4d5cd8f1",
   "metadata": {},
   "source": [
    "## Read data\n",
    "- need to know which comment chunk belong to which applicant"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f018d82a",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_applicants = D.read_df_applicants(TRAIN_OR_ALL)\n",
    "df_comments = D.read_df_comments()\n",
    "df_split_comments = D.read_df_split_comments_no_duplicate(TRAIN_OR_ALL)\n",
    "split_comments = D.read_split_comments_no_duplicate(TRAIN_OR_ALL)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0d83d666",
   "metadata": {},
   "outputs": [],
   "source": [
    "split_comment_to_id = {sc: idx for idx, sc in zip(df_split_comments['split_comment'].index, df_split_comments['split_comment'].values)}"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4aa006c7",
   "metadata": {},
   "source": [
    "## Find original applicant for each split comment"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d2beb355",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# %%time\n",
    "sc_applicant_lists = df_split_comments['applicants']\n",
    "sc_committee_lists = df_split_comments['committee']"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a03afca3",
   "metadata": {},
   "source": [
    "## Load the  embedding and the topics of each split comment"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d8df29a2",
   "metadata": {},
   "outputs": [],
   "source": [
    "from bertopic import BERTopic\n",
    "import utils.bertopic as BT"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "81f07c87",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.feature_extraction.text import CountVectorizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d3d46e27",
   "metadata": {},
   "outputs": [],
   "source": [
    "_pass = BT._pass\n",
    "topic_doc_tokenizer = BT.topic_doc_tokenizer\n",
    "vectorizer = CountVectorizer(tokenizer=topic_doc_tokenizer, lowercase=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f1f84d11",
   "metadata": {},
   "outputs": [],
   "source": [
    "SBERT_MODEL_NAME = 'ckiplab/bert-base-chinese'\n",
    "\n",
    "if TRAIN_OR_ALL == 'train':\n",
    "    BERTOPIC_MODEL_NAME = \"BERTopic_custom_mcs_100_ckip_diversified_low_train\"\n",
    "elif TRAIN_OR_ALL == 'all':\n",
    "    BERTOPIC_MODEL_NAME = \"BERTopic_custom_mcs_100_ckip_diversified_low_all\"\n",
    "    \n",
    "SPLITTER = '＄'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "84da21a5",
   "metadata": {},
   "outputs": [],
   "source": [
    "topic_model = BERTopic.load(os.path.join(P.FP_COMMENT_CLUSTERING_MODEL_DIR, BERTOPIC_MODEL_NAME))\n",
    "print(\"Load BERTopic model success.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0d922526",
   "metadata": {},
   "outputs": [],
   "source": [
    "sbert_model = topic_model.embedding_model.embedding_model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "08e38509",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_tokenization_database = df_split_comments"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "11265815",
   "metadata": {},
   "outputs": [],
   "source": [
    "sentence_bert = topic_model.embedding_model.embedding_model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0ee72740",
   "metadata": {},
   "outputs": [],
   "source": [
    "split_comments_embeds = sentence_bert.encode(split_comments, show_progress_bar=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2d320e6f",
   "metadata": {},
   "outputs": [],
   "source": [
    "reduced_split_comments_embeds = topic_model.umap_model['umap'].embedding_\n",
    "reduced_split_comments_embeds = topic_model.umap_model['norm'].transform(reduced_split_comments_embeds)\n",
    "reduced_split_comments_embeds.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c0274536",
   "metadata": {},
   "outputs": [],
   "source": [
    "import hdbscan"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3eb65523",
   "metadata": {},
   "outputs": [],
   "source": [
    "_, probs = hdbscan.approximate_predict(\n",
    "    topic_model.hdbscan_model, reduced_split_comments_embeds\n",
    ")\n",
    "topics = topic_model.hdbscan_model.labels_\n",
    "\n",
    "topics = topic_model._map_predictions(topics)\n",
    "probs = topic_model._map_probabilities(probs, original_topics=True)\n",
    "topic_labels = topics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b8ee2693",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_topic(s):\n",
    "    idx = split_comments.index(s)\n",
    "    return topics[idx]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fabb63c4",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "15850af7",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sentence_transformers.util import cos_sim"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "36ec3f40",
   "metadata": {},
   "outputs": [],
   "source": [
    "# %%time\n",
    "split_comments_sim_mat = cos_sim(split_comments_embeds, split_comments_embeds)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8b4e55f4",
   "metadata": {},
   "source": [
    "## Calculate chunk consensus for each applicant"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "17635632",
   "metadata": {},
   "outputs": [],
   "source": [
    "chunk_row_data_list = []\n",
    "\n",
    "for _, row in df_split_comments.iterrows():\n",
    "    sc = row['split_comment']\n",
    "    committee = row['committee']\n",
    "    \n",
    "#     print(sc, committee)\n",
    "    \n",
    "    for com in committee:\n",
    "        chunk_row_data = {\n",
    "            \"year\": com[0],\n",
    "            \"id\": com[1],\n",
    "            \"committee_number\": com[2],\n",
    "            \"split_comment\": sc\n",
    "        }\n",
    "        chunk_row_data_list.append(chunk_row_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a2868170",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_chunk = pd.DataFrame(chunk_row_data_list)\n",
    "df_chunk.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f50402da",
   "metadata": {},
   "source": [
    "## Find the committee that does not write comments"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "095cd14f",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_comment_committee_group = df_comments.groupby(['year', 'group', 'committee_number'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "531af294",
   "metadata": {},
   "outputs": [],
   "source": [
    "empty_comment_rate_threshold = 0.9"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8d2f4aee",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "committee_empty_comment_rate_dict = {}\n",
    "empty_comment_committee_list = []\n",
    "\n",
    "for committee, g in df_comment_committee_group:\n",
    "    comment_cnt = g.shape[0]\n",
    "    \n",
    "    empty_comment_cnt = 0\n",
    "    for comment in g['comment']:\n",
    "        if PP.is_empty_sent(comment):\n",
    "            empty_comment_cnt += 1\n",
    "    \n",
    "    empty_comment_rate = empty_comment_cnt / comment_cnt\n",
    "    print(committee, \"empty_comment_rate: {:.3f}\".format(empty_comment_rate))\n",
    "    committee_empty_comment_rate_dict[committee] = empty_comment_rate\n",
    "    \n",
    "    if empty_comment_rate > empty_comment_rate_threshold:\n",
    "        empty_comment_committee_list.append(committee)\n",
    "    \n",
    "#     print(g)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0acd7ab5",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "sorted(committee_empty_comment_rate_dict.items(), key=lambda item: -item[1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8935f337",
   "metadata": {},
   "outputs": [],
   "source": [
    "empty_comment_committee_list"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2dad97d4",
   "metadata": {},
   "source": [
    "## Calculate the number of committee per group"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "87546135",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_applicant_group = df_comments.groupby(['year', 'group'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0f4e0d35",
   "metadata": {},
   "outputs": [],
   "source": [
    "app_group_committee_count = {}\n",
    "\n",
    "for app_group, g in df_applicant_group:\n",
    "#     num_committee = g.groupby(['committee_number']).ngroups\n",
    "    group_committee = g.groupby(['committee_number']).groups.keys()\n",
    "    num_committee = sum([1 for com in group_committee if (*app_group, com) not in empty_comment_committee_list ])\n",
    "    \n",
    "    app_group_committee_count[app_group] = num_committee"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0b92c593",
   "metadata": {},
   "outputs": [],
   "source": [
    "app_group_committee_count"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "314a6d7c",
   "metadata": {},
   "source": [
    "## Find nearest neighbors "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ed7a9705",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.neighbors import NearestNeighbors"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "47c991c8",
   "metadata": {},
   "outputs": [],
   "source": [
    "neigh = NearestNeighbors(metric='minkowski') ## or 'cosine'\n",
    "neigh.fit(split_comments_embeds)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5bf4d282",
   "metadata": {},
   "outputs": [],
   "source": [
    "RADIUS = 7.5\n",
    "N_NEIGHBORS = 30"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "73b11a81",
   "metadata": {},
   "outputs": [],
   "source": [
    "# %%time\n",
    "r_neigh_dist, r_neighbor_ind = neigh.radius_neighbors(split_comments_embeds, RADIUS)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ec766477",
   "metadata": {},
   "outputs": [],
   "source": [
    "# # %%time\n",
    "k_neigh_dist, k_neighbor_ind = neigh.kneighbors(split_comments_embeds, N_NEIGHBORS)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d9299d92",
   "metadata": {},
   "outputs": [],
   "source": [
    "k_neighbor_ind"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "029a1258",
   "metadata": {},
   "outputs": [],
   "source": [
    "neigh_count_within_r = np.array([len(neighbor_ind) for neighbor_ind in r_neighbor_ind])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bd228498",
   "metadata": {},
   "outputs": [],
   "source": [
    "mean_neigh_count_within_r = np.mean(neigh_count_within_r)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c20f0399",
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.plot(np.sort(neigh_count_within_r))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "69fb0ab8",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# for i, idx in enumerate(np.argsort(neigh_count_within_r)):\n",
    "#     print(i, split_comments[idx])\n",
    "#     print(len(r_neighbor_ind[idx]))\n",
    "    \n",
    "#     for nidx in r_neighbor_ind[idx][-10:]:\n",
    "#         print('    ', split_comments[nidx])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cea76bbb",
   "metadata": {},
   "source": [
    "## Combine radius neighbors and k nearest neighbors"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "441f9622",
   "metadata": {},
   "outputs": [],
   "source": [
    "sc_neighbor_distance = []\n",
    "sc_neighbor_index = []\n",
    "\n",
    "for rnd, rni, knd, kni in zip(r_neigh_dist, r_neighbor_ind, k_neigh_dist, k_neighbor_ind):\n",
    "    ## apply k nearest neighbors\n",
    "    if len(rni) < N_NEIGHBORS:\n",
    "        sc_neighbor_distance.append(knd)\n",
    "        sc_neighbor_index.append(kni)\n",
    "    ## apply radius based neighbors\n",
    "    else:\n",
    "        sc_neighbor_distance.append(rnd)\n",
    "        sc_neighbor_index.append(rni)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a12dcda0",
   "metadata": {},
   "outputs": [],
   "source": [
    "neighbor_count = np.array([len(nind) for nind in sc_neighbor_index])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "82ba1791",
   "metadata": {},
   "source": [
    "## Aggregate the referred applicants of all neighbors"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "eca99de2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# %%time\n",
    "print(\"aggregate the referred applicants of all neighbors...\")\n",
    "\n",
    "applicants_of_neighbor = []\n",
    "committees_of_neighbor = []\n",
    "\n",
    "for nind in tqdm(sc_neighbor_index):\n",
    "    applicants = set()\n",
    "    committees = set()\n",
    "    \n",
    "    for nidx in nind:\n",
    "        sc_applicants = sc_applicant_lists.iloc[nidx]\n",
    "        sc_committees = sc_committee_lists.iloc[nidx]\n",
    "        \n",
    "        for app in sc_applicants:\n",
    "            applicants.add(app)\n",
    "            \n",
    "        for com in sc_committees:\n",
    "            committees.add(com)\n",
    "            \n",
    "    applicants_of_neighbor.append(applicants)\n",
    "    committees_of_neighbor.append(committees)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "53e8f790",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_neighbor(sc, debug=False):\n",
    "    idx = split_comments.index(sc)\n",
    "    print(idx)\n",
    "    row = df_chunk.query(\"`split_comment` == @sc\")\n",
    "    print(row)\n",
    "    \n",
    "    print(\"split comment:\", split_comments[idx])\n",
    "    print(\"Neighbors:\")\n",
    "    \n",
    "    for nidx in sc_neighbor_index[idx]:\n",
    "        if debug:\n",
    "            print(\"\\\"{}\\\",\".format(split_comments[nidx]))\n",
    "        else:\n",
    "#             print('  ', split_comments[nidx])\n",
    "            print('  ', nidx, sc_applicant_lists.iloc[nidx], split_comments[nidx])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b384b801",
   "metadata": {},
   "source": [
    "## Calculate uniqueness score\n",
    "- inverse applicant frequency\n",
    "- consensus rate"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cc7c8dc5",
   "metadata": {},
   "outputs": [],
   "source": [
    "num_applicants = df_applicants.shape[0]\n",
    "df_comment_applicant_group = df_comments.groupby(['year', 'id'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "05540995",
   "metadata": {},
   "outputs": [],
   "source": [
    "app_committee_count = {}\n",
    "\n",
    "for _, row in df_comments.iterrows():\n",
    "    app = (row['year'], row['id'])\n",
    "    committee_count = app_group_committee_count[(row['year'], row['group'])] \n",
    "    \n",
    "    app_committee_count[app] = committee_count"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "003c697e",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "print(\"calculate uniqueness score...\")\n",
    "\n",
    "split_comments_uniqueness = []\n",
    "split_comments_iaf = []\n",
    "split_comments_iccr = []\n",
    "split_comments_ccr = []\n",
    "\n",
    "for idx, (neigh_app, neigh_com) in tqdm(enumerate(zip(applicants_of_neighbor, committees_of_neighbor))):\n",
    "    ## inverse applicant frequency\n",
    "    iaf = np.log(num_applicants / len(neigh_app))\n",
    "    ## conmittee consensus rate\n",
    "    all_hit_applicant_committee = sum([\n",
    "        app_committee_count[app] for app in neigh_app\n",
    "    ])\n",
    "    mention_hit_applicant_committee = len(neigh_com)\n",
    "    ccr = mention_hit_applicant_committee / all_hit_applicant_committee\n",
    "    iccr = np.log(ccr) ** -1\n",
    "    \n",
    "    \n",
    "    uniqueness = iaf * iccr\n",
    "#     uniqueness = iaf * ccr\n",
    "    \n",
    "    split_comments_uniqueness.append(uniqueness)\n",
    "    split_comments_iaf.append(iaf)\n",
    "    split_comments_iccr.append(iccr)\n",
    "    split_comments_ccr.append(ccr)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4c498ef2",
   "metadata": {},
   "source": [
    "### Plot uniqueness"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "22e1191e",
   "metadata": {},
   "outputs": [],
   "source": [
    "if debug:\n",
    "    fig, axs = plt.subplots(3, 1, figsize=(4, 7), constrained_layout=True)\n",
    "\n",
    "    ## Uniqueness\n",
    "    _ = axs[0].plot(np.sort(split_comments_uniqueness)[::-1])\n",
    "    _ = axs[0].set_title(\"Sorted uniqueness\")\n",
    "\n",
    "    ## iaf\n",
    "    _ = axs[1].plot(np.sort(split_comments_iaf)[::-1])\n",
    "    _ = axs[1].set_title(\"Sorted inverse applicant frequency\")\n",
    "\n",
    "    ## icr\n",
    "    _ = axs[2].plot(np.sort(split_comments_ccr)[::-1])\n",
    "    _ = axs[2].set_title(\"Sorted comittee concensus rate\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "84d0e056",
   "metadata": {},
   "outputs": [],
   "source": [
    "if debug:\n",
    "    for i, idx in enumerate(np.argsort(split_comments_uniqueness)[::-1]):\n",
    "        print(\n",
    "            \"{} {}, uniqueness: {:.3f}, iaf: {:.3f}, cr: {:.3f}\".format(\n",
    "            i, \n",
    "            split_comments[idx], \n",
    "            split_comments_uniqueness[idx], \n",
    "            split_comments_iaf[idx], \n",
    "            split_comments_ccr[idx]\n",
    "        ))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "92bae2f6",
   "metadata": {},
   "outputs": [],
   "source": [
    "if debug:\n",
    "    for i, idx in enumerate(np.argsort(split_comments_iaf)[::-1]):\n",
    "        print(\n",
    "            \"{} {}, uniqueness: {:.3f}, iaf: {:.3f}, cr: {:.3f}\".format(\n",
    "            i, \n",
    "            split_comments[idx], \n",
    "            split_comments_uniqueness[idx], \n",
    "            split_comments_iaf[idx], \n",
    "            split_comments_ccr[idx]\n",
    "        ))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1157b799",
   "metadata": {},
   "source": [
    "## Calculate the uniqueness score of each sentence inside application"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b1825533",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.neighbors import KNeighborsClassifier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "aff00dd5",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"start calculate uniqueness score...\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e0c409af",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_applicants = D.read_df_applicants()\n",
    "df_applications = D.read_df_applications()\n",
    "test_df = pd.read_csv(\"112_F.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6cdf0b6e",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_applicants = pd.concat([df_applicants, test_df])\n",
    "df_applicants"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2332b990",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_applications = pd.merge(\n",
    "    df_applications, df_applicants[['year', 'id', 'name']], how='left', on=['year', 'id']\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7ae51bc1",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_applications.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d1b8fdb1",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_applications.name = df_applications.name.fillna('?')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7692fcc3",
   "metadata": {},
   "outputs": [],
   "source": [
    "tuples = df_applications.apply(lambda row: (row['year'], row['id'], row['name']), axis=1).to_list()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "20ee7a5a",
   "metadata": {},
   "outputs": [],
   "source": [
    "tuples = [{\n",
    "    'year': info[0],\n",
    "    'id': info[1],\n",
    "    'name': info[2],\n",
    "} for info in tuples]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cda2bb0f",
   "metadata": {},
   "outputs": [],
   "source": [
    "def dict_info_to_tuple_info(dict_info):\n",
    "    _year = dict_info['year']\n",
    "    _id = dict_info['id']\n",
    "    _name = dict_info['name']\n",
    "    tuple_info = (_year, _id, _name)\n",
    "    \n",
    "    return tuple_info"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "796eecc1",
   "metadata": {},
   "outputs": [],
   "source": [
    "tuples = tuples[START_IDX:END_IDX]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8af25c9c",
   "metadata": {},
   "outputs": [],
   "source": [
    "if debug:\n",
    "    tuples = [\n",
    "        \"# The content is removed due to confidential concerns.\"\n",
    "    ]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "16158f11",
   "metadata": {},
   "outputs": [],
   "source": [
    "len(tuples)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "aaab8884",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_achievements = D.read_df_achievements()\n",
    "df_recommendation_letters = D.read_df_recommendation_letters()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9e4d58de",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_chunks_and_sents_from_data_sheet(_year, _id):\n",
    "    row = df_achievements.query('`year` == {} and `id` == {}'.format(_year, _id))\n",
    "    \n",
    "    try:\n",
    "        chunks = row['achievement'].to_list()\n",
    "        ## [TODO] deal with nan achievement result\n",
    "        sents = [\"{}，{}\".format(a, r) for a, r in \n",
    "                 zip(row['achievement'].to_list(), row['achievement_result'].to_list())]\n",
    "    except:\n",
    "        chunks = []\n",
    "        sents = []\n",
    "        \n",
    "    if chunks == None:\n",
    "        chunks = []\n",
    "    if sents == None:\n",
    "        sents = []\n",
    "        \n",
    "    return chunks, sents"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1ddffb1c",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_chunks_and_sents_from_self_statement(_year, _id):\n",
    "    row = df_applications.query('`year` == {} and `id` == {}'.format(_year, _id))\n",
    "    \n",
    "    try:\n",
    "        chunks = row['self_statement_chunk'].to_list()[0]\n",
    "        sents = row['self_statement_sent'].to_list()[0]\n",
    "    except:\n",
    "        chunks = []\n",
    "        sents = []\n",
    "\n",
    "    if chunks == None:\n",
    "        chunks = []\n",
    "    if sents == None:\n",
    "        sents = []\n",
    "        \n",
    "    return chunks, sents"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "211fa201",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_chunks_and_sents_from_recommendation_letter(_year, _id):\n",
    "    rows = df_recommendation_letters.query('`year` == {} and `id` == {}'.format(_year, _id))\n",
    "    \n",
    "    try:\n",
    "        rls_chunks = rows['all_paragraph_chunk'].to_list()\n",
    "        rls_sents = rows['all_paragraph_sent'].to_list()\n",
    "    except:\n",
    "        rls_chunks = []\n",
    "        rls_sents = []\n",
    "        \n",
    "    if rls_chunks == None:\n",
    "        rls_chunks = []\n",
    "    if rls_sents == None:\n",
    "        rls_sents = []\n",
    "        \n",
    "    chunks = list(chain.from_iterable(rls_chunks))\n",
    "    sents = list(chain.from_iterable(rls_sents))\n",
    "            \n",
    "    return chunks, sents"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "43ad3396",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_topic_prediction(topic_model, chunks, n_neighbors, method=\"k\", radius=0.02):\n",
    "    if method == \"k\":\n",
    "        neigh = KNeighborsClassifier(n_neighbors=n_neighbors)\n",
    "        neigh.fit(reduced_split_comments_embeds, topic_labels)\n",
    "    elif method == \"r\":\n",
    "        neigh = KNeighborsClassifier(radius=radius, outlier_label=-1)\n",
    "        neigh.fit(reduced_split_comments_embeds, topic_labels)\n",
    "        \n",
    "    ## get reduce chunk embeddings\n",
    "    chunk_embeds = topic_model.embedding_model.embed(chunks)\n",
    "    \n",
    "#     dataset = Tor.ListDataset(chunk_embeds)\n",
    "#     dataloader = DataLoader(dataset, batch_size=32, num_workers=8)\n",
    "    \n",
    "#     chunk_reduced_embeds = np.vstack(Parallel(n_jobs=24)(delayed(topic_model.umap_model.transform)(batch) for batch in dataloader))\n",
    "    chunk_reduced_embeds = topic_model.umap_model.transform(chunk_embeds)\n",
    "    ## predict topic and confidence\n",
    "    predicted_topics = neigh.predict(chunk_reduced_embeds)\n",
    "    predicted_confs = neigh.predict_proba(chunk_reduced_embeds)\n",
    "    predicted_neighbors_idx = neigh.kneighbors(chunk_reduced_embeds, n_neighbors=n_neighbors, return_distance=False)\n",
    "    \n",
    "    return predicted_topics, predicted_confs, predicted_neighbors_idx"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7c4eebe4",
   "metadata": {},
   "outputs": [],
   "source": [
    "def calculate_uniqueness_score(chunks, predicted_neighbors_sc_idx):\n",
    "    chunks_embed = sbert_model.encode(chunks, batch_size=128, show_progress_bar=False)\n",
    "    uniqueness_score = []\n",
    "    iaf_score = []\n",
    "    ccr_score = []\n",
    "    iccr_score = []\n",
    "    \n",
    "    for chunk_embed, pred_neigh_idx in zip(chunks_embed, predicted_neighbors_sc_idx):\n",
    "#         ## filter out negative comments\n",
    "#         outliers_idx = [_idx for _idx in outliers_idx if split_comments_sentiment[_idx] == 1]\n",
    "#         outliers = [split_comments[_idx] for _idx in outliers_idx]\n",
    "        \n",
    "        ## find neighbors and their corresponding uniqueness score\n",
    "        neighbors = [split_comments[idx] for idx in pred_neigh_idx]\n",
    "        neighbors_uniquenss = [split_comments_uniqueness[idx] for idx in pred_neigh_idx]\n",
    "        neighbors_iaf = [split_comments_iaf[idx] for idx in pred_neigh_idx]\n",
    "        neighbors_ccr = [split_comments_ccr[idx] for idx in pred_neigh_idx]\n",
    "        neighbors_iccr = [split_comments_iccr[idx] for idx in pred_neigh_idx]\n",
    "        ## calculate the semantic similarity between chunk and neighbor as uniqueness weight\n",
    "        neighbors_embed = sbert_model.encode(neighbors, batch_size=128, show_progress_bar=False)\n",
    "        uniqueness_weight = cos_sim(chunk_embed, neighbors_embed).reshape(-1)\n",
    "        ## calculate chunk uniqueness with uniqueness weight\n",
    "        chunk_uniqueness = np.dot(neighbors_uniquenss, uniqueness_weight)\n",
    "        chunk_iaf = np.dot(neighbors_iaf, uniqueness_weight)\n",
    "        chunk_ccr = np.dot(neighbors_ccr, uniqueness_weight)\n",
    "        chunk_iccr = np.dot(neighbors_iccr, uniqueness_weight)\n",
    "        \n",
    "        ## [TODO] append iaf and cr\n",
    "        uniqueness_score.append(chunk_uniqueness)\n",
    "        iaf_score.append(chunk_iaf)\n",
    "        ccr_score.append(chunk_ccr)\n",
    "        iccr_score.append(chunk_iccr)\n",
    "        \n",
    "    return uniqueness_score, iaf_score, ccr_score, iccr_score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ee7552af",
   "metadata": {},
   "outputs": [],
   "source": [
    "def calculate_candidate_sents_score(\n",
    "    sents,\n",
    "    chunks,\n",
    "    topic_model,\n",
    "    n_neighbors = 25\n",
    "):\n",
    "    if chunks == None or len(chunks) == 0:\n",
    "        return {\n",
    "            \"sents\": [],\n",
    "            \"chunks\": [],\n",
    "            \"uniqueness_score\": [],\n",
    "            \"iaf_score\": [],\n",
    "            \"ccr_score\": [],\n",
    "            \"iccr_score\": [],\n",
    "        }\n",
    "\n",
    "    _, _, predicted_neighbors_sc_idx = get_topic_prediction(\n",
    "        topic_model, chunks, n_neighbors\n",
    "    )\n",
    "    \n",
    "    uniqueness_score, iaf_score, ccr_score, iccr_score = calculate_uniqueness_score(chunks, predicted_neighbors_sc_idx)\n",
    "    \n",
    "    return {\n",
    "        \"sents\": sents,\n",
    "        \"chunks\": chunks,\n",
    "        \"uniqueness_score\": uniqueness_score,\n",
    "        \"iaf_score\": iaf_score,\n",
    "        \"ccr_score\": ccr_score,\n",
    "        \"iccr_score\": iccr_score,\n",
    "    }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c5f86951",
   "metadata": {},
   "outputs": [],
   "source": [
    "def find_summary_candidate_pipe(info, get_chunks_and_sents_and_refs_func, debug=False):\n",
    "    ## get basic info\n",
    "    _year = info['year']\n",
    "    _id = info['id']\n",
    "    _name = info['name']\n",
    "    idx = (_year, _id, _name)\n",
    "    \n",
    "#     print(idx)\n",
    "    \n",
    "    ## get chunks and sents\n",
    "    chunks, sents = get_chunks_and_sents_and_refs_func(_year, _id)\n",
    "    ## [TODO] calculate importance score for each summary\n",
    "    chunk_debug_info = calculate_candidate_sents_score(\n",
    "        sents, chunks, topic_model\n",
    "    )\n",
    "    \n",
    "    return chunk_debug_info"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dc06117f",
   "metadata": {},
   "outputs": [],
   "source": [
    "def merge_chunk_debug_info(old_info, new_info):\n",
    "    ## if old info is empty, return new info\n",
    "    if old_info == {} or old_info['chunks'] == []:\n",
    "        return new_info\n",
    "    \n",
    "    ## if new info is empty, return old info\n",
    "    if new_info['chunks'] == []:\n",
    "        return old_info\n",
    "    \n",
    "    info = {}\n",
    "    \n",
    "    info['sents'] = np.concatenate((old_info['sents'], new_info['sents']))\n",
    "    info['chunks'] = old_info['chunks'] + new_info['chunks']\n",
    "    info['uniqueness_score'] = np.concatenate((old_info['uniqueness_score'], new_info['uniqueness_score']))\n",
    "    info['iaf_score'] = np.concatenate((old_info['iaf_score'], new_info['iaf_score']))\n",
    "    info['ccr_score'] = np.concatenate((old_info['ccr_score'], new_info['ccr_score']))\n",
    "    info['iccr_score'] = np.concatenate((old_info['iccr_score'], new_info['iccr_score']))\n",
    "    \n",
    "    return info"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "16c20dae",
   "metadata": {},
   "outputs": [],
   "source": [
    "chunk_debug_info_buffer = defaultdict(dict)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3473b9d4",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "IO.print_dividing_line()\n",
    "IO.print_dividing_line(\"Processing data sheet ...\")\n",
    "\n",
    "for dict_info in tqdm(tuples):\n",
    "    idx = dict_info_to_tuple_info(dict_info)\n",
    "    chunk_debug_info = find_summary_candidate_pipe(\n",
    "        dict_info, get_chunks_and_sents_from_data_sheet\n",
    "    )\n",
    "\n",
    "    chunk_debug_info_buffer[idx] = merge_chunk_debug_info(chunk_debug_info_buffer[idx], chunk_debug_info)\n",
    "#     IO.print_dividing_line()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "328e8c7d",
   "metadata": {},
   "outputs": [],
   "source": [
    "IO.print_dividing_line()\n",
    "IO.print_dividing_line(\"Processing self-statement ...\")\n",
    "\n",
    "for dict_info in tqdm(tuples):\n",
    "    idx = dict_info_to_tuple_info(dict_info)\n",
    "    chunk_debug_info = find_summary_candidate_pipe(\n",
    "        dict_info, get_chunks_and_sents_from_self_statement\n",
    "    )\n",
    "\n",
    "    chunk_debug_info_buffer[idx] = merge_chunk_debug_info(chunk_debug_info_buffer[idx], chunk_debug_info)\n",
    "#     IO.print_dividing_line()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e73ea9a4",
   "metadata": {},
   "outputs": [],
   "source": [
    "IO.print_dividing_line()\n",
    "IO.print_dividing_line(\"Processing recommendation letter ...\")\n",
    "\n",
    "for dict_info in tqdm(tuples):\n",
    "    idx = dict_info_to_tuple_info(dict_info)\n",
    "    chunk_debug_info = find_summary_candidate_pipe(\n",
    "        dict_info, get_chunks_and_sents_from_recommendation_letter\n",
    "    )\n",
    "\n",
    "    chunk_debug_info_buffer[idx] = merge_chunk_debug_info(chunk_debug_info_buffer[idx], chunk_debug_info)\n",
    "#     IO.print_dividing_line()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c568a564",
   "metadata": {},
   "outputs": [],
   "source": [
    "if debug:\n",
    "    chunk_debug_info_buffer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "59886c2c",
   "metadata": {},
   "outputs": [],
   "source": [
    "for _, _dict in chunk_debug_info_buffer.items():\n",
    "    assert len(_dict['chunks']) == len(_dict['uniqueness_score'])\n",
    "    assert len(_dict['uniqueness_score']) == len(_dict['iaf_score'])\n",
    "    assert len(_dict['iaf_score']) == len(_dict['ccr_score'])\n",
    "    assert len(_dict['ccr_score']) == len(_dict['iccr_score'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0242a7ea",
   "metadata": {},
   "outputs": [],
   "source": [
    "if debug:\n",
    "    for info, chunk_uni_dict in chunk_debug_info_buffer.items():\n",
    "        print(info)\n",
    "\n",
    "        chunks = chunk_uni_dict['chunks']\n",
    "        uniqueness_score = chunk_uni_dict['uniqueness_score']\n",
    "\n",
    "        for idx in np.argsort(uniqueness_score)[::-1]:\n",
    "            print(chunks[idx], uniqueness_score[idx])\n",
    "\n",
    "        IO.print_dividing_line()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "074ea0f2",
   "metadata": {},
   "outputs": [],
   "source": [
    "## store data\n",
    "if debug:\n",
    "    fn = \"{}_chunk_uniqueness_debug.pkl\".format(TRAIN_OR_ALL)\n",
    "    _dir = os.path.join(P.FP_UNIQUENESS_PSEUDO_SUMMARY_DIR, 'custom_bertopic', TRAIN_OR_ALL, 'all_data')\n",
    "else:\n",
    "    fn = \"{}_chunk_uniqueness_{:04d}_to_{:04d}.pkl\".format(TRAIN_OR_ALL, START_IDX, END_IDX)\n",
    "    _dir = os.path.join(P.FP_UNIQUENESS_PSEUDO_SUMMARY_DIR, 'custom_bertopic', TRAIN_OR_ALL, 'all_data')\n",
    "\n",
    "if not os.path.exists(_dir):\n",
    "    os.makedirs(_dir)\n",
    "\n",
    "all_data_fp = os.path.join(_dir, fn)\n",
    "\n",
    "with open(all_data_fp, 'wb') as f:\n",
    "    pickle.dump(chunk_debug_info_buffer, f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "88e52882",
   "metadata": {},
   "outputs": [],
   "source": [
    "# chunk load test\n",
    "if debug:\n",
    "    fn = \"{}_chunk_uniqueness_{:04d}_to_{:04d}.pkl\".format(TRAIN_OR_ALL, START_IDX, END_IDX)\n",
    "    _dir = os.path.join(P.FP_UNIQUENESS_PSEUDO_SUMMARY_DIR, 'custom_bertopic', TRAIN_OR_ALL, 'all_data')\n",
    "    \n",
    "    all_data_fp = os.path.join(_dir, fn)\n",
    "\n",
    "    with open(all_data_fp, 'rb') as f:\n",
    "        _dict = pickle.load(f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3ee1c13a",
   "metadata": {},
   "outputs": [],
   "source": [
    "fn = \"{}_chunk_uniqueness_{:04d}_to_{:04d}.pkl\".format(TRAIN_OR_ALL, 0, 100)\n",
    "_dir = os.path.join(P.FP_UNIQUENESS_PSEUDO_SUMMARY_DIR, 'custom_bertopic', TRAIN_OR_ALL, 'all_data')\n",
    "\n",
    "all_data_fp = os.path.join(_dir, fn)\n",
    "\n",
    "with open(all_data_fp, 'rb') as f:\n",
    "    _dict = pickle.load(f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "82369720",
   "metadata": {},
   "outputs": [],
   "source": [
    "_dict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "969cab32",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "research_venv",
   "language": "python",
   "name": "research_venv"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
