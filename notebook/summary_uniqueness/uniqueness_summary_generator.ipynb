{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "debug = False"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import os\n",
    "import torch\n",
    "from collections import defaultdict\n",
    "from itertools import combinations\n",
    "\n",
    "from tqdm import tqdm\n",
    "tqdm.pandas(desc=\"progress: \")\n",
    "\n",
    "from importlib import reload\n",
    "\n",
    "# Utility variable\n",
    "import sys\n",
    "sys.path.insert(0, '../..')\n",
    "\n",
    "# var\n",
    "import var.var as V\n",
    "import var.path as P\n",
    "\n",
    "# utils\n",
    "import utils.data as D\n",
    "import utils.io as IO\n",
    "import utils.mmr as MMR\n",
    "import utils.preprocess as PP"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Hyper-parameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "TRAIN_OR_ALL = 'train'\n",
    "VAL_OR_TEST = 'val'\n",
    "TEST_YEAR = 112\n",
    "TOP_K = 5\n",
    "SIM_THRESHOLD = 0.94"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def defaultdict_init_defaultdict_init_by_int():\n",
    "    return defaultdict(int)\n",
    "\n",
    "def defaultdict_init_defaultdict_init_by_float():\n",
    "    return defaultdict(float)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Load data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_applicants = D.read_df_applicants()\n",
    "df_applications = D.read_df_applications()\n",
    "test_df = pd.read_csv(\"112_F.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_applicants = pd.concat([df_applicants, test_df])\n",
    "df_applicants"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_applications_applicants = pd.merge(\n",
    "    df_applications, df_applicants[['year', 'id', 'name', 'train_or_test']], how='left', on=['year', 'id']\n",
    ")\n",
    "df_applications_applicants.name.fillna('?', inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_info_data = []\n",
    "\n",
    "if VAL_OR_TEST == 'val':\n",
    "    for _, row in df_applications_applicants.iterrows():\n",
    "        train_or_test = row['train_or_test']\n",
    "\n",
    "        if train_or_test != 'test':\n",
    "            continue\n",
    "\n",
    "        _year = row['year']\n",
    "        _id = row['id']\n",
    "        _name = row['name']\n",
    "\n",
    "        test_info_data.append((_year, _id, _name))\n",
    "        \n",
    "elif VAL_OR_TEST == 'test':\n",
    "    for _, row in df_applications_applicants.iterrows():\n",
    "        _year = row['year']\n",
    "        _id = row['id']\n",
    "        _name = row['name']\n",
    "        \n",
    "        if _year != TEST_YEAR:\n",
    "            continue\n",
    "\n",
    "        test_info_data.append((_year, _id, _name))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Generate pseudo summary"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "GPU_NUM = 0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "device = torch.device(GPU_NUM)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sentence_transformers import SentenceTransformer\n",
    "from sentence_transformers.util import cos_sim\n",
    "from sklearn.metrics.pairwise import cosine_similarity"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "SBERT_MODEL_NAME = 'ckiplab/bert-base-chinese'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sbert_model = SentenceTransformer(SBERT_MODEL_NAME).to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pickle\n",
    "from itertools import chain\n",
    "from collections import Counter, OrderedDict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def mmr_sorted(docs, q, lambda_=0.7):\n",
    "    def mmr_sim1(x, q):\n",
    "        \"\"\"\n",
    "            q is the pre-computed score dictionary for each x\n",
    "        \"\"\"\n",
    "        return q[x]\n",
    "\n",
    "    def mmr_sim2(x, y, sim_mat):\n",
    "        _idx_x = doc_to_idx[x]\n",
    "        _idx_y = doc_to_idx[y]\n",
    "        return sim_mat[_idx_x, _idx_y]\n",
    "    \n",
    "    def argmax(keys, f):\n",
    "        return max(keys, key=f)\n",
    "    \n",
    "    if len(docs) == 0:\n",
    "        return {}\n",
    "    \n",
    "    docs_embed = sbert_model.encode(docs, batch_size=512, show_progress_bar=False)\n",
    "    sim_mat = cosine_similarity(docs_embed, docs_embed)\n",
    "    doc_to_idx = {doc: i for i, doc in enumerate(docs)}\n",
    "    \n",
    "    docs = set(docs)\n",
    "    \n",
    "    selected = OrderedDict() \n",
    "    while set(selected) != docs: \n",
    "        remaining = docs - set(selected) \n",
    "        mmr_score = lambda x: lambda_*mmr_sim1(x, q) - (1-lambda_)*max([mmr_sim2(x, y, sim_mat) for y in set(selected)-{x}] or [0]) \n",
    "        next_selected = argmax(remaining, mmr_score) \n",
    "        selected[next_selected] = len(selected) \n",
    "    \n",
    "    return selected"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_info = [\n",
    "    \"# The content is removed due to confidential concerns.\"\n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## load uniqueness dictionary\n",
    "uniqueness_pseudo_summary_dir = os.path.join(P.FP_UNIQUENESS_PSEUDO_SUMMARY_DIR, 'custom_bertopic', TRAIN_OR_ALL)\n",
    "uniqueness_all_data_dir = os.path.join(uniqueness_pseudo_summary_dir, 'all_data')\n",
    "uniqueness_debug_buffer = {}\n",
    "\n",
    "for file in tqdm(os.listdir(uniqueness_all_data_dir)):\n",
    "    fn = os.path.join(uniqueness_all_data_dir, file)\n",
    "    \n",
    "    if os.path.isdir(fn):\n",
    "        continue\n",
    "    if 'uniqueness' not in fn:\n",
    "        continue\n",
    "        \n",
    "    with open(fn, 'rb') as f:\n",
    "        buffer = pickle.load(f)\n",
    "\n",
    "    uniqueness_debug_buffer |= buffer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "significance_pseudo_summary_dir = os.path.join(P.FP_SIGNIFICANCE_PSEUDO_SUMMARY_DIR, 'custom_bertopic', TRAIN_OR_ALL)\n",
    "significance_all_data_dir = os.path.join(significance_pseudo_summary_dir, 'all_data')\n",
    "significance_chunk_debug_buffer = {}\n",
    "significance_sents_info_buffer = {}\n",
    "\n",
    "for file in tqdm(os.listdir(significance_all_data_dir)):\n",
    "    fn = os.path.join(significance_all_data_dir, file)\n",
    "    \n",
    "    if os.path.isdir(fn):\n",
    "        continue\n",
    "        \n",
    "    with open(fn, 'rb') as f:\n",
    "        buffer = pickle.load(f)\n",
    "\n",
    "    significance_chunk_debug_buffer |= buffer['chunk_debug_info_buffer']\n",
    "    significance_sents_info_buffer |= buffer['candidate_sents_info_buffer']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "len(significance_chunk_debug_buffer)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "len(significance_sents_info_buffer)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "os.path.exists"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "uniqueness_summary_docx_dir = os.path.join(\n",
    "    P.FP_UNIQUENESS_SUMMARY_DIR, TRAIN_OR_ALL,\n",
    ")\n",
    "\n",
    "if not os.path.exists(uniqueness_summary_docx_dir):\n",
    "    os.mkdir(uniqueness_summary_docx_dir)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.preprocessing import StandardScaler\n",
    "\n",
    "scaler = StandardScaler()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "uniqueness_summary_dict = {}\n",
    "\n",
    "for info in tqdm(test_info_data):\n",
    "    if debug and info not in test_info:\n",
    "        continue\n",
    "    \n",
    "    try:\n",
    "        debug_dict = uniqueness_debug_buffer[info]\n",
    "    except:\n",
    "        print(\"no test applicant\")\n",
    "        continue\n",
    "    \n",
    "    _year = info[0]\n",
    "    _id = info[1]\n",
    "    _name = info[2]\n",
    "    \n",
    "    if debug:\n",
    "        print(info)\n",
    "        \n",
    "    # [TODO] load sents after fix the bug\n",
    "#     sents = debug_dict['sents']\n",
    "    sents = significance_sents_info_buffer[info]['sents']\n",
    "\n",
    "    chunks = debug_dict['chunks']\n",
    "    uniqueness_score = debug_dict['uniqueness_score']\n",
    "    # [TODO] load iaf and ccr score after fix the bug\n",
    "#     iaf_score = debug_dict['iaf_score']\n",
    "#     ccr_score = debug_dict['ccr_score']\n",
    "    \n",
    "    if debug and info not in test_info:\n",
    "        continue\n",
    "\n",
    "    if len(uniqueness_score) == 0:\n",
    "        uniqueness_summary_dict[info] = []\n",
    "        continue\n",
    "        \n",
    "    ## Normalize uniqueness score\n",
    "    _min = np.min(uniqueness_score)\n",
    "    _max = np.max(uniqueness_score)\n",
    "    uniqueness_score = (uniqueness_score - _min) / (_max - _min)\n",
    "        \n",
    "    ## Uniqueness: select top-k sentence from outliers with MMR\n",
    "    summary = []\n",
    "\n",
    "    sent_unique_dict = defaultdict(float)\n",
    "    ## Aggregate sentence uniqueness score over chunk uniqueness score\n",
    "    ## Method 1: [MAX Pool]\n",
    "    for chunk, uniq in zip(chunks, uniqueness_score):\n",
    "        if uniq == 0:\n",
    "            continue\n",
    "            \n",
    "        ## find the sentence cotaining the chunk\n",
    "        for sent in sents:\n",
    "            ## aggregate chunk uniqueness\n",
    "            if chunk in sent:\n",
    "                sent_unique_dict[sent] = max(uniq, sent_unique_dict[sent]) ## use max chunk as uniqueness score\n",
    "                break\n",
    "        \n",
    "    ## Method 2: [Mean Pool]\n",
    "    ## find the sentence cotaining the chunk\n",
    "#     for sent in sents:\n",
    "#         chunk_cnt = 0\n",
    "#         agg_unique = 0\n",
    "        \n",
    "#         for chunk, uniq in zip(chunks, uniqueness_score):\n",
    "#             if chunk in sent:\n",
    "#                 agg_unique += uniq\n",
    "#                 chunk_cnt += 1\n",
    "        \n",
    "#         try:\n",
    "#             ## aggregate max pool and mean pool\n",
    "#             sent_unique_dict[sent] = (sent_unique_dict[sent] + agg_unique / chunk_cnt) / 2\n",
    "#         except:\n",
    "#             sent_unique_dict[sent] = 0\n",
    "\n",
    "    sent_mmr_sorted = mmr_sorted(sents, sent_unique_dict)\n",
    "\n",
    "#     if debug:\n",
    "#         print(\"unique sentences: \")\n",
    "#         pass\n",
    "\n",
    "    for sent in sent_mmr_sorted.keys():\n",
    "        if len(summary) == TOP_K:\n",
    "            break\n",
    "        ## [TODO] remove sentence with too low uniqueness and iaf value\n",
    "        if sent_unique_dict[sent] < 0.9:\n",
    "            continue\n",
    "        summary.append(sent)\n",
    "        \n",
    "        \n",
    "        if debug:\n",
    "            print(sent, sent_unique_dict[sent])\n",
    "#             print(sent)\n",
    "            pass\n",
    "\n",
    "    while len(summary) < TOP_K:\n",
    "        summary.append('') ## pad empty sentence\n",
    "\n",
    "    if debug:\n",
    "        IO.print_dividing_line()\n",
    "\n",
    "    if debug:\n",
    "        print(\"before remove similar sentences\")\n",
    "        print(summary)\n",
    "        IO.print_dividing_line()\n",
    "\n",
    "    ## within each perspective, remove sentence with too similar semantic meaning (> 0.95)\n",
    "    summary_sent_embeds = sbert_model.encode(summary, show_progress_bar=False)\n",
    "    sim_mat = cos_sim(summary_sent_embeds, summary_sent_embeds)\n",
    "\n",
    "    similar_pair = []\n",
    "\n",
    "    for i, j in combinations(range(TOP_K), 2):\n",
    "        if sim_mat[i, j] > SIM_THRESHOLD:\n",
    "            similar_pair.append((i, j))\n",
    "\n",
    "    remove_sent_id_buf = []\n",
    "    ## remove the shorter sentence\n",
    "    for i, j in similar_pair:\n",
    "        if summary[i] == '' and summary[j] == '':\n",
    "            continue\n",
    "\n",
    "#         if debug:\n",
    "#             print(summary[i])\n",
    "#             print(summary[j])\n",
    "#             IO.print_dividing_line()\n",
    "        len_i = PP.get_sent_len(summary[i])\n",
    "        len_j = PP.get_sent_len(summary[j])\n",
    "\n",
    "        if len_j > len_i:\n",
    "            remove_sent_id_buf.append(i)\n",
    "        else:\n",
    "            remove_sent_id_buf.append(j)\n",
    "\n",
    "    for i in remove_sent_id_buf:\n",
    "        summary[i] = ''\n",
    "\n",
    "    if debug:\n",
    "        print(\"after remove similar sentences\")\n",
    "        print(summary)\n",
    "        IO.print_dividing_line()\n",
    "\n",
    "    uniqueness_summary_dict[info] = summary"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "len(uniqueness_summary_dict)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "uniqueness_summary_docx_dir"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if not debug:\n",
    "    fp = os.path.join(uniqueness_summary_docx_dir, 'uniqueness_summary_dict.pkl')\n",
    "\n",
    "    with open(fp, \"wb\") as f:\n",
    "        pickle.dump(uniqueness_summary_dict, f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "research_venv",
   "language": "python",
   "name": "research_venv"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
