{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# README\n",
    "\n",
    "### Purpose of this notebook\n",
    "- Predict the topic of the application sentences with respect to the comment cluster using K-Nearest Neighbor algorithm (KNN).\n",
    "- Form the summary based on the above prediction.\n",
    "\n",
    "### Steps\n",
    "1. Get the reduced embeddings and cluster label from the comment sentences.\n",
    "2. Apply KNN algorithm to predict the label and the confidence of application sentences.\n",
    "3. Generate summary according to the results from step 2."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import os\n",
    "import torch\n",
    "from collections import defaultdict\n",
    "from itertools import combinations, chain\n",
    "import pickle\n",
    "import copy\n",
    "from docx import Document\n",
    "from zhon import hanzi\n",
    "\n",
    "import re\n",
    "from zhon import hanzi\n",
    "\n",
    "from tqdm import tqdm\n",
    "tqdm.pandas(desc=\"progress: \")\n",
    "\n",
    "from importlib import reload\n",
    "\n",
    "# Utility variable\n",
    "import sys, getopt\n",
    "sys.path.insert(0, '../..')\n",
    "\n",
    "# var\n",
    "import var.var as V\n",
    "import var.path as P\n",
    "\n",
    "# utils\n",
    "import utils.data as D\n",
    "import utils.io as IO\n",
    "import utils.preprocess as PP"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from torch import Tensor"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Process Command Line Arguments"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "opts, args = getopt.getopt(sys.argv[1:], \"d:e:m:l:s:f:t\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "SIGNIFICANCE_MODEL_SAVE_DIR_NAME = \"significance_pHAN_cmt_cos_dist_w_cmt_aug_train_2022-12-12_mixed_1\"\n",
    "EPOCH = 20\n",
    "MAX_SENT = 8\n",
    "UNI_MAX_SENT = 2\n",
    "MAX_SENT_LEN = 60\n",
    "LAMBDA = 0.3\n",
    "NORM_RATIO = 4\n",
    "VAL_OR_TEST = 'val'\n",
    "debug = False\n",
    "EVIDENCE_SCORE_THRESHOLD = 0.75\n",
    "\n",
    "for opt, arg in opts:\n",
    "    if opt == '-d':\n",
    "        SIGNIFICANCE_MODEL_SAVE_DIR_NAME = arg\n",
    "    elif opt == '-e':\n",
    "        EPOCH = int(arg)\n",
    "    elif opt == '-s':\n",
    "        MAX_SENT = int(arg)\n",
    "    elif opt == '-m':\n",
    "        MAX_SENT_LEN = int(arg)\n",
    "    elif opt == '-l':\n",
    "        LAMBDA = float(arg)\n",
    "    elif opt == '-t':\n",
    "        VAL_OR_TEST = 'test'\n",
    "    elif opt == '-f':\n",
    "        debug = True"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if 'train' in SIGNIFICANCE_MODEL_SAVE_DIR_NAME:\n",
    "    TRAIN_OR_ALL = 'train'\n",
    "elif 'all' in SIGNIFICANCE_MODEL_SAVE_DIR_NAME:\n",
    "    TRAIN_OR_ALL = 'all'\n",
    "    \n",
    "if 'wo' in SIGNIFICANCE_MODEL_SAVE_DIR_NAME:\n",
    "    COMMENT_AUGMENTATION = False\n",
    "else:\n",
    "    COMMENT_AUGMENTATION = True"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### BERT NSP Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "GPU_NUM = 0\n",
    "device = torch.device(GPU_NUM)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "BERT_NSP_MODEL_NAME = 'bert-base-chinese'\n",
    "BERT_NSP_TOKENIZER_NAME = 'bert-base-chinese'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import BertTokenizerFast, BertForNextSentencePrediction"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "bert_nsp_tokenizer = BertTokenizerFast.from_pretrained(BERT_NSP_TOKENIZER_NAME)\n",
    "bert_nsp_model = BertForNextSentencePrediction.from_pretrained(BERT_NSP_MODEL_NAME).to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "SBERT_MODEL_NAME = 'ckiplab/bert-base-chinese'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "from sentence_transformers import SentenceTransformer\n",
    "\n",
    "sbert_model = SentenceTransformer(SBERT_MODEL_NAME).to(device)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Utils"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def defaultdict_init_defaultdict_init_by_int():\n",
    "    return defaultdict(int)\n",
    "\n",
    "def defaultdict_init_defaultdict_init_by_float():\n",
    "    return defaultdict(float)\n",
    "\n",
    "def defaultdict_init_defaultdict_init_by_str():\n",
    "    return defaultdict(str)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Build the reference citation dictionary"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_recommendation_letters = D.read_df_recommendation_letters()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "df_recommendation_letters.tail()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "rl_info_dict = defaultdict(defaultdict_init_defaultdict_init_by_str)\n",
    "\n",
    "for _, row in df_recommendation_letters.iterrows():\n",
    "    _year = int(row['year'])\n",
    "    _id = int(row['id'])\n",
    "\n",
    "    rl_sent = row['all_paragraph_sent']\n",
    "    info = \"，\".join(row['info'])\n",
    "    \n",
    "    if info == \"\":\n",
    "        continue\n",
    "        \n",
    "    sent_info_dict = defaultdict_init_defaultdict_init_by_str()\n",
    "    \n",
    "    for sent in rl_sent:\n",
    "        sent_info_dict[sent] = info\n",
    "        \n",
    "    rl_info_dict[(_year, _id)] = rl_info_dict[(_year, _id)] | sent_info_dict    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Calculate the evidence score for each sentence"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "significance_pseudo_summary_dir = os.path.join(P.FP_SIGNIFICANCE_PSEUDO_SUMMARY_DIR, 'custom_bertopic', TRAIN_OR_ALL)\n",
    "significance_all_data_dir = os.path.join(significance_pseudo_summary_dir, 'all_data')\n",
    "\n",
    "uniqueness_pseudo_summary_dir = os.path.join(P.FP_UNIQUENESS_PSEUDO_SUMMARY_DIR, 'custom_bertopic', TRAIN_OR_ALL)\n",
    "uniqueness_all_data_dir = os.path.join(uniqueness_pseudo_summary_dir, 'all_data')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "i = 0\n",
    "sent_evidence_score_dict = defaultdict(defaultdict_init_defaultdict_init_by_float)\n",
    "\n",
    "for file in tqdm(os.listdir(significance_all_data_dir)):\n",
    "    fn = os.path.join(significance_all_data_dir, file)\n",
    "    \n",
    "#     IO.print_dividing_line()\n",
    "#     if i >= 1:\n",
    "#         break\n",
    "    \n",
    "    if os.path.isdir(fn):\n",
    "        continue\n",
    "\n",
    "#     print(fn)\n",
    "        \n",
    "    with open(fn, \"rb\") as f:\n",
    "        group_data = pickle.load(f)\n",
    "\n",
    "    ## process group data\n",
    "    candidate_sents_info_buffer = group_data[\"candidate_sents_info_buffer\"]\n",
    "    chunk_debug_info_buffer = group_data[\"chunk_debug_info_buffer\"]\n",
    "    \n",
    "#     print(candidate_sents_info_buffer)\n",
    "#     print(chunk_debug_info_buffer)\n",
    "\n",
    "    for info, debug_info in chunk_debug_info_buffer.items():\n",
    "        buffer_dict = defaultdict_init_defaultdict_init_by_float()\n",
    "        \n",
    "#         print(info, debug_info)\n",
    "        sents = candidate_sents_info_buffer[info]['sents']\n",
    "        chunks = debug_info['chunks']\n",
    "        chunk_evidence_scores = debug_info['evidence_score']\n",
    "    \n",
    "#         print(info)\n",
    "#         print(sents)\n",
    "#         print(chunks)\n",
    "#         print(chunk_evidence_scores)\n",
    "        \n",
    "        for chunk, chunk_evidence_score in zip(chunks, chunk_evidence_scores):\n",
    "            # find corresponding sentences\n",
    "            for sent in sents:\n",
    "                ## aggregate sent evidence score\n",
    "                if chunk in sent:\n",
    "                    buffer_dict[sent] = max(\n",
    "                        buffer_dict[sent], chunk_evidence_score\n",
    "                    )\n",
    "                    \n",
    "        sent_evidence_score_dict[info] = buffer_dict\n",
    "    \n",
    "    i += 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "len(sent_evidence_score_dict)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## All data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "all_candidate_sents_info_buffer = {}\n",
    "all_chunk_debug_info_buffer = {}\n",
    "\n",
    "for file in tqdm(os.listdir(significance_all_data_dir)):\n",
    "    fn = os.path.join(significance_all_data_dir, file)\n",
    "    \n",
    "    if os.path.isdir(fn):\n",
    "        continue\n",
    "        \n",
    "    with open(fn, \"rb\") as f:\n",
    "        group_data = pickle.load(f)\n",
    "        \n",
    "    candidate_sents_info_buffer = group_data[\"candidate_sents_info_buffer\"]\n",
    "    chunk_debug_info_buffer = group_data[\"chunk_debug_info_buffer\"]\n",
    "    \n",
    "    all_candidate_sents_info_buffer |= candidate_sents_info_buffer\n",
    "    all_chunk_debug_info_buffer |= chunk_debug_info_buffer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "len(all_candidate_sents_info_buffer)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "len(all_chunk_debug_info_buffer)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Long sentence post process utils"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sentence_transformers.util import cos_sim"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_sent_len(s):\n",
    "    re_alphanumeric = '[a-zA-Z0-9_]+'\n",
    "    re_ch_p = '[{}]'.format(hanzi.characters + hanzi.punctuation)\n",
    "    \n",
    "    l = 0\n",
    "    \n",
    "    ## find all english and number token\n",
    "    l += len(re.findall(re_alphanumeric, s))\n",
    "    s = re.sub(re_alphanumeric, '', s)\n",
    "    \n",
    "    ## remove whitespace\n",
    "    s = re.sub('\\s', '', s)\n",
    "    \n",
    "    ## count chinese character\n",
    "    l += len(re.findall(re_ch_p, s))\n",
    "    \n",
    "    return l"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def find_longest_consecutive_sequence(seq):\n",
    "    assert len(seq) >= 1\n",
    "    \n",
    "    cand_seqs = []\n",
    "    \n",
    "    if len(seq) == 1:\n",
    "        return seq\n",
    "\n",
    "    cur_num = seq[0]\n",
    "    seq_buf = seq[:1]\n",
    "\n",
    "    for num in seq[1:]:\n",
    "        if num == cur_num + 1:\n",
    "            seq_buf.append(num)\n",
    "        else:\n",
    "            cand_seqs.append(seq_buf)\n",
    "            seq_buf = [num]\n",
    "\n",
    "        cur_num = num\n",
    "\n",
    "    cand_seqs.append(seq_buf)\n",
    "    sorted_cand_seqs = sorted(cand_seqs, key=lambda l: -len(l))\n",
    "    \n",
    "    return sorted_cand_seqs[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def is_reasonable_sent(chunks, tokenizer, model, debug=False):\n",
    "    ## get all sublist with length >= 2\n",
    "    sublist_idx = []\n",
    "    for start_idx, end_idx in combinations(range(len(chunks)+1), 2):\n",
    "        if end_idx - start_idx > 1:\n",
    "            sublist_idx.append((start_idx, end_idx))\n",
    "    \n",
    "    ## split sublist into two list with len >= 1\n",
    "    text_pair = []\n",
    "    for start_idx, end_idx in sublist_idx:\n",
    "        sublist = chunks[start_idx:end_idx]\n",
    "        for pivot in range(1, len(sublist)):\n",
    "            l_sublist = sublist[0:pivot]\n",
    "            r_sublist = sublist[pivot:len(sublist)]\n",
    "            text_pair.append(('，'.join(l_sublist)+'。', '，'.join(r_sublist)+'。'))\n",
    "    \n",
    "    inputs = tokenizer(text_pair, return_tensors='pt', padding=True)\n",
    "\n",
    "    for key in inputs:\n",
    "        if isinstance(inputs[key], Tensor):\n",
    "            inputs[key] = inputs[key].to(device)\n",
    "    \n",
    "    with torch.no_grad():\n",
    "        outputs = model(**inputs)\n",
    "        results = torch.argmax(outputs.logits, dim=-1)\n",
    "        \n",
    "    if debug:\n",
    "        print(outputs)\n",
    "        for p in text_pair:\n",
    "            print(p)\n",
    "        print(results)\n",
    "    \n",
    "    return bool(sum(results) == 0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "hanzi.punctuation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "split_punc = '\\n＂＃＄％＆＇＊＋，－／：；＜＝＞＠＼＾＿｀｜～､\\u3000、〃〜〝〞〟〰〾〿–—‘’‛“”„‟…‧﹏﹑﹔·！？｡。'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "parenthesis_punc = '＂（）［］｛｝｟｠｢｣〈〉《》「」『』【】〔〕〖〗〘〙〚〛'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def long_sentence_post_process(s, debug_info, ext_summary, debug=False):\n",
    "    ## get debug chunk info\n",
    "    debug_chunks_idx = []\n",
    "    for i, chunk in enumerate(debug_info['chunks']):\n",
    "        if chunk in s:\n",
    "            debug_chunks_idx.append(i)\n",
    "\n",
    "    ## find the longest consecutive sequence\n",
    "    debug_chunks_idx = find_longest_consecutive_sequence(debug_chunks_idx)\n",
    "    \n",
    "    if debug:\n",
    "        for _idx in debug_chunks_idx:\n",
    "            print(debug_info['chunks'][_idx])\n",
    "    \n",
    "    ## directly split s to get all chunks\n",
    "    cand_chunks = re.split('[{}]'.format(split_punc), s)\n",
    "    cand_chunks = [c for c in cand_chunks if c]\n",
    "    if debug:\n",
    "        print(\"real chunks:\", cand_chunks)\n",
    "    \n",
    "    ## compute the len of each chunk\n",
    "    chunks_len = [get_sent_len(cc) for cc in cand_chunks]\n",
    "    ## brutal force to find text span that is below length limit\n",
    "    span_tuple = combinations(range(len(cand_chunks)+1), 2)\n",
    "    f_span_tuple = []\n",
    "\n",
    "    if debug:\n",
    "        print(\"cand chunks:\", cand_chunks)\n",
    "    \n",
    "    for start_idx, end_idx in span_tuple:\n",
    "        spans = cand_chunks[start_idx:end_idx]\n",
    "    #     print(spans)\n",
    "        ## check if satisfy length limit\n",
    "        if sum(chunks_len[start_idx:end_idx]) <= MAX_SENT_LEN:\n",
    "            f_span_tuple.append((start_idx, end_idx))\n",
    "    \n",
    "    ## remove text span that is the subset of other text span\n",
    "    filtered_span_tuple = []\n",
    "    for i, (start_idx, end_idx) in enumerate(f_span_tuple):\n",
    "        spans = cand_chunks[start_idx:end_idx]\n",
    "        unique = True\n",
    "        for j, (_start_idx, _end_idx) in enumerate(f_span_tuple):\n",
    "            if i == j:\n",
    "                continue\n",
    "            if _start_idx <= start_idx and end_idx <= _end_idx:\n",
    "                unique = False\n",
    "\n",
    "        if unique:\n",
    "#             print('，'.join(spans))\n",
    "            filtered_span_tuple.append((start_idx, end_idx))\n",
    "\n",
    "    if debug:\n",
    "        print(\"spans after brutal force search\")\n",
    "        print(filtered_span_tuple)\n",
    "    \n",
    "    ## remove unreasonable sentence\n",
    "    buf = []\n",
    "    for start_idx, end_idx in filtered_span_tuple:\n",
    "        chunks = cand_chunks[start_idx:end_idx]\n",
    "        if end_idx - start_idx == 1:\n",
    "            buf.append((start_idx, end_idx))\n",
    "        elif is_reasonable_sent(chunks, bert_nsp_tokenizer, bert_nsp_model):\n",
    "            buf.append((start_idx, end_idx))\n",
    "            \n",
    "    filtered_span_tuple = buf\n",
    "\n",
    "    if debug:\n",
    "        print(\"spans after nsp filter\")\n",
    "        print(filtered_span_tuple)\n",
    "    \n",
    "    if len(filtered_span_tuple) == 0:\n",
    "        return ''\n",
    "    \n",
    "    cand_score = []\n",
    "    ## calculate final score\n",
    "    for start_idx, end_idx in filtered_span_tuple:\n",
    "        cc = cand_chunks[start_idx:end_idx]\n",
    "        if debug:\n",
    "#             print(cc)\n",
    "            pass\n",
    "        \n",
    "        ## aggregate imp score from real chunks\n",
    "        ## should contain inportant info (high importance)\n",
    "        cand_importance = 0\n",
    "        for _c in cc:\n",
    "            for _idx in debug_chunks_idx:\n",
    "                if _c in debug_info['chunks'][_idx]:\n",
    "                    cand_importance += debug_info['importance'][_idx]\n",
    "                    break\n",
    "        \n",
    "        cand_importance /= len(cc)\n",
    "        \n",
    "#         ## info should be novel (dissimilar with existing summary)\n",
    "        cc_embed = sbert_model.encode('，'.join(cc), show_progress_bar=False)\n",
    "        ext_summary_embed = sbert_model.encode(ext_summary, show_progress_bar=False)\n",
    "        cand_novel = 1 - cos_sim(cc_embed, ext_summary_embed)[0][0]\n",
    "        \n",
    "        score = LAMBDA * cand_importance + (1 - LAMBDA) * cand_novel * NORM_RATIO\n",
    "        cand_score.append(score)\n",
    "        if debug:\n",
    "            print('imp', float(cand_importance))\n",
    "            print('novel', float(cand_novel))\n",
    "            print(score)\n",
    "        \n",
    "    final_span_idx = np.argmax(cand_score)\n",
    "    final_start_idx, final_end_idx = filtered_span_tuple[final_span_idx]\n",
    "    final_chunks = cand_chunks[final_start_idx:final_end_idx]\n",
    "    final_sent = '，'.join(final_chunks) + '。'\n",
    "    \n",
    "    return final_sent\n",
    "    \n",
    "    # ## find the desire chunk to extract (max topic match, claim score, evidence score, etc.)\n",
    "    # for idx in cand_chunks_idx:\n",
    "    #     print(idx)\n",
    "    #     print(\"chunk: \", debug_info['chunks'][idx])\n",
    "    #     print(\"predicted_topics: \", debug_info['predicted_topics'][idx])\n",
    "    #     print(\"knn_confidence: \", debug_info['knn_confidence'][idx])\n",
    "    #     print(\"topic_match_score: \", debug_info['topic_match_score'][idx])\n",
    "    #     print(\"claim_score: \", debug_info['claim_score'][idx])\n",
    "    #     print(\"future_plan_score: \", debug_info['future_plan_score'][idx])\n",
    "    #     print(\"evidence_score: \", debug_info['evidence_score'][idx])\n",
    "    #     print(\"uniqueness_score: \", debug_info['uniqueness_score'][idx])\n",
    "    #     print(\"importance: \", debug_info['importance'][idx])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import string\n",
    "from zhon import hanzi"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def sentence_post_process(s):\n",
    "    s = s.strip()\n",
    "    try:\n",
    "        ## remove trailing number\n",
    "        while s[0] in string.punctuation:\n",
    "            s = s[1:]\n",
    "    except:\n",
    "        return ''\n",
    "    s = s.strip()\n",
    "    \n",
    "    ## remove mojibake\n",
    "    s = s.replace('\\uf06c', '')\n",
    "    s = s.replace('。，', '，')\n",
    "    s = s.replace('，。', '，')\n",
    "    s = s.replace('，nan', '')\n",
    "    s = s.replace('&lt；', '')\n",
    "    \n",
    "    ## remove zh number bullet\n",
    "    ch_number = \"一二三四五六七八九十\"\n",
    "    p = '[{}\\d]、'.format(ch_number)\n",
    "    s = re.sub(p, '', s)\n",
    "    \n",
    "    ## remove number bullet\n",
    "    p = '((?<!\\d)\\d+\\.(?!\\d)|★)'\n",
    "    s = re.sub(p, '',s)\n",
    "    \n",
    "    s = s.strip()\n",
    "    try:\n",
    "        ## remove trailing number\n",
    "        while s[-1] in string.digits:\n",
    "            s = s[:-1]\n",
    "    except:\n",
    "        return ''\n",
    "    s = s.strip()\n",
    "    \n",
    "    s = s.strip()\n",
    "    try:\n",
    "        ## remove trailing non stop punctuation\n",
    "        while s[-1] in hanzi.non_stops:\n",
    "            s = s[:-1]\n",
    "    except:\n",
    "        return ''\n",
    "    s = s.strip()\n",
    "    \n",
    "    return s"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# def sentence_post_process(s):\n",
    "#     s = s.strip()\n",
    "#     ## remove trailing number\n",
    "#     while s[0] in string.punctuation:\n",
    "#         s = s[1:]\n",
    "#     s = s.strip()\n",
    "    \n",
    "#     ## remove mojibake\n",
    "#     s = s.replace('\\uf06c', '')\n",
    "#     s = s.replace('。，', '，')\n",
    "#     s = s.replace('，。', '，')\n",
    "    \n",
    "#     ## remove zh number bullet\n",
    "#     ch_number = \"一二三四五六七八九十\"\n",
    "#     p = '[{}\\d]、'.format(ch_number)\n",
    "#     s = re.sub(p, '', s)\n",
    "    \n",
    "#     ## remove number bullet\n",
    "#     p = '((?<!\\d)\\d+\\.(?!\\d)|★)'\n",
    "#     s = re.sub(p, '',s)\n",
    "    \n",
    "#     s = s.strip()\n",
    "#     try:\n",
    "#         ## remove trailing number\n",
    "#         while s[-1] in string.digits:\n",
    "#             s = s[:-1]\n",
    "#     except:\n",
    "#         return ''\n",
    "#     s = s.strip()\n",
    "    \n",
    "#     s = s.strip()\n",
    "#     try:\n",
    "#         ## remove trailing non stop punctuation\n",
    "#         while s[-1] in hanzi.non_stops:\n",
    "#             s = s[:-1]\n",
    "#     except:\n",
    "#         return ''\n",
    "#     s = s.strip()\n",
    "    \n",
    "#     return s"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Post process the summary"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Load summary dict"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Utils"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "default_talents = [\"# The content is removed due to confidential concerns.\"]\n",
    "\n",
    "def generate_docx(doc, sum_sent, talents=default_talents, debug=False):    \n",
    "    for talent in talents:\n",
    "        doc.add_heading(talent, level=2)\n",
    "        sents = sum_sent[talent]\n",
    "        \n",
    "        no_duplicate_sents = []\n",
    "        sum_sents_buffer = []\n",
    "        for sent in sents:\n",
    "            if sent not in no_duplicate_sents:\n",
    "                no_duplicate_sents.append(sent)\n",
    "                sum_sents_buffer.append(sent)\n",
    "        \n",
    "        for sent in sum_sents_buffer:\n",
    "            doc.add_paragraph(sent, style='List Bullet')\n",
    "    \n",
    "        if len(sents) == 0:\n",
    "            doc.add_paragraph(\"無\")\n",
    "    \n",
    "    for talent in default_talents:\n",
    "        if talent not in talents:\n",
    "            doc.add_heading(talent, level=2)\n",
    "            doc.add_paragraph(\"無\")\n",
    "    \n",
    "    return doc"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "debug_tuple = [\n",
    "    \"# The content is removed due to confidential concerns.\"\n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## STEP 0: load the summary data\n",
    "if COMMENT_AUGMENTATION:\n",
    "    significance_summary_docx_dir = os.path.join(\n",
    "        P.FP_SIGNIFICANCE_SUMMARY_DIR, TRAIN_OR_ALL, 'w', SIGNIFICANCE_MODEL_SAVE_DIR_NAME, \"epoch_{}_max_sent_{}\".format(EPOCH, MAX_SENT)\n",
    "    )\n",
    "    summary_docx_dir = os.path.join(\n",
    "        P.FP_SUMMARY_GENERATION_DIR, TRAIN_OR_ALL, 'w', SIGNIFICANCE_MODEL_SAVE_DIR_NAME, \"epoch_{}_max_sent_{}\".format(EPOCH, MAX_SENT)\n",
    "    )\n",
    "else:\n",
    "    significance_summary_docx_dir = os.path.join(\n",
    "        P.FP_SIGNIFICANCE_SUMMARY_DIR, TRAIN_OR_ALL, 'wo', SIGNIFICANCE_MODEL_SAVE_DIR_NAME, \"epoch_{}_max_sent_{}\".format(EPOCH, MAX_SENT)\n",
    "    )\n",
    "    summary_docx_dir = os.path.join(\n",
    "        P.FP_SUMMARY_GENERATION_DIR, TRAIN_OR_ALL, 'wo', SIGNIFICANCE_MODEL_SAVE_DIR_NAME, \"epoch_{}_max_sent_{}\".format(EPOCH, MAX_SENT)\n",
    "    )\n",
    "    \n",
    "uniqueness_summary_docx_dir = os.path.join(\n",
    "    P.FP_UNIQUENESS_SUMMARY_DIR, TRAIN_OR_ALL\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if not os.path.exists(summary_docx_dir):\n",
    "    os.makedirs(summary_docx_dir)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "significance_summary_docx_dir"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "uniqueness_summary_docx_dir"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "summary_docx_dir"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## load significance summary data\n",
    "fn_significance_summary_dict = os.path.join(significance_summary_docx_dir, \"summary_dict.pkl\")\n",
    "with open(fn_significance_summary_dict, 'rb') as f:\n",
    "    significance_summary_dict = pickle.load(f)\n",
    "    \n",
    "## load uniqueness summary data\n",
    "fn_uniqueness_summary_dict = os.path.join(uniqueness_summary_docx_dir, \"uniqueness_summary_dict.pkl\")\n",
    "with open(fn_uniqueness_summary_dict, 'rb') as f:\n",
    "    uniqueness_summary_dict = pickle.load(f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "len(significance_summary_dict), len(uniqueness_summary_dict)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "pp_summary_dict = {}\n",
    "pp_summary_no_tag_dict = {}\n",
    "\n",
    "for info, sig_summary_info in tqdm(significance_summary_dict.items()):\n",
    "    ## STEP 1: Post process summary\n",
    "    _year = info[0]\n",
    "    _id = info[1]\n",
    "    _name = info[2]\n",
    "\n",
    "    if debug:\n",
    "        if info not in debug_tuple:\n",
    "            continue\n",
    "\n",
    "    uni_summary = uniqueness_summary_dict[info]\n",
    "            \n",
    "    sig_summary = sig_summary_info['summary']\n",
    "    title_weight = sig_summary_info['title_weight']\n",
    "\n",
    "    if debug:\n",
    "        print(info)\n",
    "        print(\"significance summary before post process\")\n",
    "        print(sig_summary)\n",
    "        print(\"=\"*10)\n",
    "\n",
    "    pp_summary = defaultdict(list)\n",
    "    pp_summary_no_tag = defaultdict(list)\n",
    "    buf_summary = copy.deepcopy(sig_summary)\n",
    "    \n",
    "    ## append uniqueness summary\n",
    "    cnt = 0\n",
    "    while cnt < UNI_MAX_SENT and uni_summary != []:\n",
    "        uni_summary = [sent for sent in uni_summary if not PP.is_empty_sent(sent)]\n",
    "        uni_summary_sent_embed = sbert_model.encode(uni_summary, show_progress_bar=False)\n",
    "        \n",
    "        buf_summary_sent = list(chain.from_iterable(buf_summary.values()))\n",
    "        buf_summary_sent = [sent for sent in buf_summary_sent if not PP.is_empty_sent(sent)]\n",
    "        \n",
    "        buf_summary_sent_embed = sbert_model.encode(buf_summary_sent, show_progress_bar=False)\n",
    "        \n",
    "        sim_mat = np.array(cos_sim(uni_summary_sent_embed, buf_summary_sent_embed))\n",
    "        uni_sent_disimilarity = 1 - np.mean(sim_mat, axis=-1)\n",
    "        \n",
    "        idx = np.argmax(uni_sent_disimilarity)\n",
    "        \n",
    "        if debug:\n",
    "            print(\"disimilarity\", uni_sent_disimilarity[idx])\n",
    "            print(\"sent\", uni_summary[idx])\n",
    "            print(\"max sim\", np.max(sim_mat[idx]))\n",
    "        \n",
    "        ## append unique sentence that is most disimilar to significance summary\n",
    "        if np.max(sim_mat[idx]) < 0.94:\n",
    "            buf_summary['獨特表現'].append(uni_summary[idx])\n",
    "            sig_summary['獨特表現'].append(uni_summary[idx])\n",
    "            cnt += 1\n",
    "        uni_summary[idx] = ''\n",
    "        uni_summary = [sent for sent in uni_summary if not PP.is_empty_sent(sent)]\n",
    "    \n",
    "    for talent, sum_sents in sig_summary.items():\n",
    "        for sum_sent in sum_sents:\n",
    "            _bidx = buf_summary[talent].index(sum_sent)\n",
    "            rl_infos = rl_info_dict[(_year, _id)]\n",
    "            sent_evidence = sent_evidence_score_dict[info]\n",
    "\n",
    "            tag = \"\"\n",
    "\n",
    "            if sum_sent in rl_infos.keys():\n",
    "                ## 1. add citations (only for rl)\n",
    "                rl_info = rl_infos[sum_sent]\n",
    "\n",
    "                if info != '':\n",
    "                    tag = rl_info\n",
    "            else:\n",
    "                ## 2. add verified mark (only for not rl)\n",
    "                evidence = sent_evidence[sum_sent]\n",
    "\n",
    "                if evidence > EVIDENCE_SCORE_THRESHOLD:\n",
    "                    tag = \"已驗證\"\n",
    "\n",
    "            ## trim sentence that is too long\n",
    "            ## get the length of the sentence\n",
    "            sum_sent_len = get_sent_len(sum_sent)\n",
    "\n",
    "            if sum_sent_len > MAX_SENT_LEN:                \n",
    "                if debug:\n",
    "                    print('sum_sent_to_trim:', sum_sent)\n",
    "\n",
    "                debug_info = all_chunk_debug_info_buffer[info]\n",
    "\n",
    "                ext_summary = list(chain.from_iterable(buf_summary.values()))\n",
    "                ext_summary = [sent for sent in ext_summary if sent != sum_sent]\n",
    "                ext_summary = '。'.join(ext_summary)\n",
    "\n",
    "                if debug:\n",
    "                    print('ext_summary:', ext_summary)\n",
    "                sum_sent = long_sentence_post_process(sum_sent, debug_info, ext_summary)\n",
    "\n",
    "                if debug:\n",
    "                    print('sum_sent:', sum_sent, \"len:\", get_sent_len(sum_sent))\n",
    "                    print('-'*10)\n",
    "\n",
    "            sum_sent = sentence_post_process(sum_sent)\n",
    "            ## replace sum sent in original summary\n",
    "            buf_summary[talent][_bidx] = sum_sent\n",
    "\n",
    "            ## [TODO] process sentence with unclosed parenthesis (remove unreasonable sentence)\n",
    "            if sum_sent == '':\n",
    "                continue\n",
    "\n",
    "            pp_summary_no_tag[talent].append(sum_sent)\n",
    "                \n",
    "            if tag:\n",
    "                sum_sent = \"{}（{}）\".format(sum_sent, tag)\n",
    "\n",
    "            pp_summary[talent].append(sum_sent)\n",
    "\n",
    "    talent_list = sorted(title_weight.items(), key=lambda i: -i[1])\n",
    "    talent_list = [t[0] for t in talent_list]\n",
    "    talent_list.append('獨特表現')\n",
    "    \n",
    "    if debug:\n",
    "        print('='*10)\n",
    "        print(\"summary after post process\")\n",
    "        print(pp_summary)\n",
    "        IO.print_dividing_line()\n",
    "\n",
    "    pp_summary_dict[info] = pp_summary\n",
    "    pp_summary_no_tag_dict[info] = pp_summary_no_tag\n",
    "    \n",
    "    if not debug:\n",
    "        ## STEP 2: Write post processed summary to docx file\n",
    "        doc = Document()\n",
    "        doc = generate_docx(doc, pp_summary, talent_list)\n",
    "\n",
    "        fn = \"{}.docx\".format(\"_\".join(map(str, info)))\n",
    "        if debug:\n",
    "            print(fn)\n",
    "        doc.save(os.path.join(summary_docx_dir, fn))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Prepare data for calculating BERTScore"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_comments = D.read_df_comments()\n",
    "df_applicants = D.read_df_applicants()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "individual_data = []\n",
    "if VAL_OR_TEST == 'val':\n",
    "    year_dir = V.YEAR_DIRS[:-1]\n",
    "elif VAL_OR_TEST == 'test':\n",
    "    year_dir = V.YEAR_DIRS[-1:]\n",
    "\n",
    "for year in year_dir:\n",
    "    _dir = os.path.join(P.FP_SIGNIFICANCE_PSEUDO_SUMMARY_DIR, 'custom_bertopic', TRAIN_OR_ALL, year)\n",
    "    \n",
    "    for file in os.listdir(_dir):\n",
    "        if file == '.ipynb_checkpoints':\n",
    "            continue\n",
    "\n",
    "        fn = os.path.join(_dir, file)\n",
    "\n",
    "        with open(fn, \"rb\") as f:\n",
    "            data = pickle.load(f)\n",
    "            individual_data.append(data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "len(individual_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_info_data = []\n",
    "test_pseudo_summary_data = []\n",
    "test_comment_data = []\n",
    "test_grade_data = []\n",
    "\n",
    "# pseudo_summary_to_info_dict = {}\n",
    "\n",
    "if VAL_OR_TEST == 'val':\n",
    "    for data in tqdm(individual_data):\n",
    "        _year = data['year']\n",
    "        _id = data['id']\n",
    "        _name = data['name']\n",
    "        pseudo_summary = data['pseudo_summary']\n",
    "\n",
    "        ## check train or test data\n",
    "        row = df_applicants.query('`year` == {} and `id` == {}'.format(_year, _id))\n",
    "        try:\n",
    "            train_or_test = row['train_or_test'].to_list()[0]\n",
    "        except:\n",
    "            train_or_test = 'train'\n",
    "\n",
    "        if train_or_test == 'train':\n",
    "            continue\n",
    "            \n",
    "        ## get corresponding comments\n",
    "        row = df_comments.query('`year` == {} and `id` == {}'.format(_year, _id))\n",
    "        comments = row['comment'].to_list()\n",
    "        grades = row['grade'].to_list()\n",
    "\n",
    "        ## append test data set\n",
    "        for comment, grade in zip(comments, grades):\n",
    "            ## remove empty comment\n",
    "            if PP.is_empty_sent(comment):\n",
    "                continue\n",
    "\n",
    "            test_info_data.append((_year, _id, _name))\n",
    "            test_comment_data.append(comment)\n",
    "            test_pseudo_summary_data.append(pseudo_summary)\n",
    "            test_grade_data.append(grade)\n",
    "                \n",
    "elif VAL_OR_TEST == 'test':\n",
    "    for data in tqdm(individual_data):\n",
    "        _year = data['year']\n",
    "        _id = data['id']\n",
    "        _name = data['name']\n",
    "        pseudo_summary = data['pseudo_summary']\n",
    "\n",
    "        ## append data to test data set\n",
    "        test_info_data.append((_year, _id, _name))\n",
    "        test_comment_data.append('') ## stuff empty comment\n",
    "        test_pseudo_summary_data.append(pseudo_summary)\n",
    "        test_grade_data.append('F') ## stuff empty comment"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "len(pp_summary_no_tag_dict)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ## find summary and its corresponding comment\n",
    "# if VAL_OR_TEST == 'val':\n",
    "#     test_summary_result = []\n",
    "\n",
    "#     for applicant_info, pseudo_summary, comment in zip(test_info_data, test_pseudo_summary_data, test_comment_data):\n",
    "#         applicant_info = tuple(applicant_info)\n",
    "#         summary = pp_summary_no_tag_dict[applicant_info]\n",
    "#         buffer = []\n",
    "\n",
    "#         for sents in summary.values():\n",
    "#             buffer.append(sents)\n",
    "\n",
    "#         summary = ''.join(list(chain.from_iterable(buffer)))\n",
    "\n",
    "#         # concat summary together [TODO] different concata method may result in differenet bertscore?\n",
    "#         test_summary_result.append(summary)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# if VAL_OR_TEST == 'val':\n",
    "#     from bert_score import score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# def calculate_bert_score(cands, refs, rescale=False, verbose=False):\n",
    "#     return score(\n",
    "#         cands,\n",
    "#         refs,\n",
    "#         lang=\"zh\",\n",
    "#     #     model_type=MODEL_TYPE,\n",
    "#     #     num_layers=LAYER,\n",
    "#         verbose=verbose,\n",
    "#         device=0,\n",
    "#         batch_size=32,\n",
    "#     #     idf=False,\n",
    "#         rescale_with_baseline=rescale\n",
    "#     )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# if VAL_OR_TEST == 'val':\n",
    "#     _P, _R, _F1 = calculate_bert_score(test_summary_result, test_comment_data, rescale=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# import logging\n",
    "# from datetime import datetime\n",
    "\n",
    "# for handler in logging.root.handlers[:]:\n",
    "#     logging.root.removeHandler(handler)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# if COMMENT_AUGMENTATION:\n",
    "#     log_file = os.path.join(P.FP_SUMMARY_GENERATION_DIR, TRAIN_OR_ALL, 'w', SIGNIFICANCE_MODEL_SAVE_DIR_NAME, 'summary_post_process_log.log')\n",
    "# else:\n",
    "#     log_file = os.path.join(P.FP_SUMMARY_GENERATION_DIR, TRAIN_OR_ALL, 'wo', SIGNIFICANCE_MODEL_SAVE_DIR_NAME, 'summary_post_process_log.log')\n",
    "    \n",
    "# logger = logging.getLogger()\n",
    "# logger.setLevel(logging.INFO)\n",
    "# # create file handler which logs even debug messages\n",
    "# fh = logging.FileHandler(log_file)\n",
    "# fh.setLevel(logging.INFO)\n",
    "# # create console handler with a higher log level\n",
    "# ch = logging.StreamHandler()\n",
    "# ch.setLevel(logging.INFO)\n",
    "# # add the handlers to logger\n",
    "# logger.addHandler(ch)\n",
    "# logger.addHandler(fh)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# if VAL_OR_TEST == 'val':\n",
    "#     IO.log_dividing_line(logger)\n",
    "#     logger.info(\"model dir: {}\".format(SIGNIFICANCE_MODEL_SAVE_DIR_NAME))\n",
    "#     logger.info(\"time: {}\".format(str(datetime.now())))\n",
    "#     logger.info(\"epoch: {}\".format(EPOCH))\n",
    "#     logger.info(\"max sent: {}\".format(MAX_SENT))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# if VAL_OR_TEST == 'val':\n",
    "#     rouge_precision = torch.mean(_P)\n",
    "#     rouge_recall = torch.mean(_R)\n",
    "#     rouge_f1 = torch.mean(_F1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# if VAL_OR_TEST == 'val':\n",
    "#     logger.info(\"p: {:4f}\".format(rouge_precision))\n",
    "#     logger.info(\"r: {:4f}\".format(rouge_recall))\n",
    "#     logger.info(\"f: {:4f}\".format(rouge_f1))\n",
    "#     IO.log_dividing_line(logger)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "research_venv",
   "language": "python",
   "name": "research_venv"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
