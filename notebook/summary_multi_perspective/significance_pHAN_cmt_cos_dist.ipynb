{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2badb093",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import os\n",
    "import pickle\n",
    "from itertools import chain\n",
    "from tqdm import tqdm\n",
    "\n",
    "from importlib import reload\n",
    "\n",
    "# Utility variable\n",
    "import sys, getopt\n",
    "sys.path.insert(0, '../..')\n",
    "\n",
    "# var\n",
    "import var.var as V\n",
    "import var.path as P\n",
    "\n",
    "# utils\n",
    "import utils.data as D\n",
    "import utils.io as IO\n",
    "import utils.preprocess as PP\n",
    "import utils.torch as Tor"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "81bc5f8d",
   "metadata": {},
   "source": [
    "## Process Command Line Options"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6dcddcff",
   "metadata": {},
   "outputs": [],
   "source": [
    "## parse arguments\n",
    "opts, args = getopt.getopt(sys.argv[1:], \"d:acns:p:f:\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dca7ca18",
   "metadata": {},
   "outputs": [],
   "source": [
    "DATE = \"2022-12-09_mixed_3\"\n",
    "\n",
    "TRAIN_OR_ALL = 'train'\n",
    "COMMENT_AUGMENTATION = False\n",
    "NEG_SAMPLE = False\n",
    "\n",
    "SENT_TEMPERATURE = 1\n",
    "PERS_TEMPERATURE = 1\n",
    "\n",
    "for opt, arg in opts:\n",
    "    if opt == '-d':\n",
    "        DATE = arg\n",
    "    elif opt == '-a':\n",
    "        TRAIN_OR_ALL = 'all'\n",
    "    elif opt == '-c':\n",
    "        COMMENT_AUGMENTATION = True\n",
    "    elif opt == '-n':\n",
    "        NEG_SAMPLE = True\n",
    "    elif opt == '-s':\n",
    "        SENT_TEMPERATURE = float(arg)\n",
    "    elif opt == '-p':\n",
    "        PERS_TEMPERATURE = float(arg)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7946dc4d",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"date:\", DATE)\n",
    "print(\"train or all:\", TRAIN_OR_ALL)\n",
    "print(\"comment augmentation:\", COMMENT_AUGMENTATION)\n",
    "print(\"negative comment sample:\", NEG_SAMPLE)\n",
    "print(\"sentence temperature:\", SENT_TEMPERATURE)\n",
    "print(\"persepctive temperature:\", PERS_TEMPERATURE)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9ebbad99",
   "metadata": {},
   "source": [
    "## Params"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fed0b197",
   "metadata": {},
   "outputs": [],
   "source": [
    "if NEG_SAMPLE:\n",
    "    if COMMENT_AUGMENTATION:\n",
    "        MODEL_SAVE_DIR = \"significance_pHAN_cmt_cos_dist_w_cmt_aug_{}_{}_neg\".format(TRAIN_OR_ALL, DATE)\n",
    "    else:\n",
    "        MODEL_SAVE_DIR = \"significance_pHAN_cmt_cos_dist_wo_cmt_aug_{}_{}_neg\".format(TRAIN_OR_ALL, DATE)\n",
    "else:\n",
    "    if COMMENT_AUGMENTATION:\n",
    "        MODEL_SAVE_DIR = \"significance_pHAN_cmt_cos_dist_w_cmt_aug_{}_{}\".format(TRAIN_OR_ALL, DATE)\n",
    "    else:\n",
    "        MODEL_SAVE_DIR = \"significance_pHAN_cmt_cos_dist_wo_cmt_aug_{}_{}\".format(TRAIN_OR_ALL, DATE)\n",
    "    \n",
    "BERTOPIC_MODEL_NAME = \"BERTopic_custom_mcs_100_ckip_diversified_low_{}\".format(TRAIN_OR_ALL)\n",
    "\n",
    "BERT_MODEL_NAME = 'ckiplab/bert-base-chinese'\n",
    "BERT_TOKENIZER_NAME = 'bert-base-chinese'\n",
    "\n",
    "if COMMENT_AUGMENTATION:\n",
    "    MODEL_SAVE_DIR_PATH = os.path.join(P.FP_SIGNIFICANCE_PHAN_DIR, TRAIN_OR_ALL, 'w', MODEL_SAVE_DIR)\n",
    "else:\n",
    "    MODEL_SAVE_DIR_PATH = os.path.join(P.FP_SIGNIFICANCE_PHAN_DIR, TRAIN_OR_ALL, 'wo', MODEL_SAVE_DIR)\n",
    "\n",
    "if not os.path.exists(MODEL_SAVE_DIR_PATH):\n",
    "    os.makedirs(MODEL_SAVE_DIR_PATH)\n",
    "\n",
    "USE_SBERT_EMBED = True\n",
    "USE_BERT_EMBED = False\n",
    "assert (USE_SBERT_EMBED or USE_BERT_EMBED) == True\n",
    "\n",
    "GPU_NUM = 0\n",
    "\n",
    "TOP_K = V.TOP_K\n",
    "NUM_PERSPECTIVE = V.MAX_NUM_PERSPECTIVE\n",
    "\n",
    "BATCH_SIZE = 128\n",
    "## pHAN params\n",
    "BERT_DIM = 768\n",
    "SENT_DIM = 768\n",
    "CXT_DIM = 128\n",
    "PRJ_DIM = 768\n",
    "COMPRESSION = False\n",
    "PROJECTION = False\n",
    "ATTENTION_EMPTY_MASK = True\n",
    "FREEZE_BERT = True\n",
    "DROPOUT_RATE = 0.1\n",
    "LEAKY_RELU_NEG_SLOPE = 0.1\n",
    "ENC_BZ = 128\n",
    "\n",
    "## gradient params\n",
    "GRADIENT_MAX_NORM = 0.5\n",
    "GRADIENT_CLIP_VALUE = 0.5\n",
    "\n",
    "RANDOM_STATE = 42\n",
    "np.random.seed(RANDOM_STATE)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6e75abb9",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"model save dir:\", MODEL_SAVE_DIR)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "584826ee",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from torch import Tensor\n",
    "from torch.utils.data import Dataset\n",
    "from torch.utils.data import DataLoader"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ff2f409c",
   "metadata": {},
   "outputs": [],
   "source": [
    "device = torch.device(GPU_NUM)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e8ccf5ac",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pytz\n",
    "timezone = pytz.timezone('Asia/Taipei')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8eae33bc",
   "metadata": {},
   "outputs": [],
   "source": [
    "import logging\n",
    "\n",
    "for handler in logging.root.handlers[:]:\n",
    "    logging.root.removeHandler(handler)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f1793ab8",
   "metadata": {},
   "outputs": [],
   "source": [
    "log_file = os.path.join(MODEL_SAVE_DIR_PATH, 'training_log.log')\n",
    "\n",
    "logger = logging.getLogger()\n",
    "logger.setLevel(logging.INFO)\n",
    "# create file handler which logs even debug messages\n",
    "fh = logging.FileHandler(log_file)\n",
    "fh.setLevel(logging.INFO)\n",
    "# create console handler with a higher log level\n",
    "ch = logging.StreamHandler()\n",
    "ch.setLevel(logging.INFO)\n",
    "# add the handlers to logger\n",
    "logger.addHandler(ch)\n",
    "logger.addHandler(fh)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e24a8ce6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Disable hugging face tokenizer parallelism\n",
    "os.environ[\"TOKENIZERS_PARALLELISM\"] = \"false\""
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f1f440a9",
   "metadata": {},
   "source": [
    "### Utils"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9ef75d5c",
   "metadata": {},
   "outputs": [],
   "source": [
    "def defaultdict_init_defaultdict_init_by_int():\n",
    "    return defaultdict(int)\n",
    "\n",
    "def defaultdict_init_defaultdict_init_by_float():\n",
    "    return defaultdict(float)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0cb5069f",
   "metadata": {},
   "source": [
    "## Read raw data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "62b458a0",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "df_comments = D.read_df_comments()\n",
    "df_comments"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a50d7aa8",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "df_applicants = D.read_df_applicants()\n",
    "df_applicants"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b8fd1f6e",
   "metadata": {},
   "outputs": [],
   "source": [
    "individual_data = []\n",
    "\n",
    "for year in V.YEAR_DIRS[:-1]:\n",
    "    _dir = os.path.join(P.FP_SIGNIFICANCE_PSEUDO_SUMMARY_DIR, 'custom_bertopic', TRAIN_OR_ALL, year)\n",
    "    \n",
    "    for file in os.listdir(_dir):\n",
    "        if file == '.ipynb_checkpoints':\n",
    "            continue\n",
    "\n",
    "        fn = os.path.join(_dir, file)\n",
    "\n",
    "        with open(fn, \"rb\") as f:\n",
    "            data = pickle.load(f)\n",
    "            individual_data.append(data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0885b3b2",
   "metadata": {},
   "outputs": [],
   "source": [
    "len(individual_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "70f79a77",
   "metadata": {},
   "outputs": [],
   "source": [
    "fn = os.path.join(\n",
    "    P.FP_COMMENT_CLUSTERING_TOPIC_HIERARCHY_DIR, \n",
    "    \"{}_topic_aggregate_info.pkl\".format(BERTOPIC_MODEL_NAME)\n",
    ")\n",
    "\n",
    "with open(fn, \"rb\") as f:\n",
    "    topic_aggregate_info = pickle.load(f)\n",
    "    perspective_mean_embed_dict = topic_aggregate_info['topic_aggregate_embed_mean_dict']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c847da94",
   "metadata": {},
   "outputs": [],
   "source": [
    "perspective_mean_embed = []\n",
    "\n",
    "for i, embed in perspective_mean_embed_dict.items():\n",
    "    perspective_mean_embed.append(embed)\n",
    "    \n",
    "perspective_mean_embed = torch.tensor(np.stack(perspective_mean_embed))\n",
    "perspective_mean_embed.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8b8d5d4e",
   "metadata": {},
   "source": [
    "## Prepare training data and testing data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4e790411",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_pseudo_summary_data = []\n",
    "train_comment_data = []\n",
    "train_aug_comment_data = []\n",
    "train_grade_data = []\n",
    "\n",
    "test_pseudo_summary_data = []\n",
    "test_comment_data = []\n",
    "test_aug_comment_data = []\n",
    "test_grade_data = []\n",
    "\n",
    "for data in tqdm(individual_data):\n",
    "    _year = data['year']\n",
    "    _id = data['id']\n",
    "    _name = data['name']\n",
    "    pseudo_summary = data['pseudo_summary']\n",
    "    \n",
    "    ## check if pseudo summary is empty\n",
    "    ps = ''.join(chain.from_iterable(pseudo_summary))\n",
    "    if ps == '':\n",
    "        continue\n",
    "    \n",
    "    ## check train or test data\n",
    "    row = df_applicants.query('`year` == {} and `id` == {}'.format(_year, _id))\n",
    "    try:\n",
    "        train_or_test = row['train_or_test'].to_list()[0]\n",
    "    except:\n",
    "        train_or_test = 'train'\n",
    "    \n",
    "    ## get corresponding comments\n",
    "    row = df_comments.query('`year` == {} and `id` == {}'.format(_year, _id))\n",
    "    comments = row['comment'].to_list()\n",
    "    augmented_comments = row['augmented_comments'].to_list()\n",
    "    grades = row['grade'].to_list()\n",
    "    \n",
    "    ## append data to train data set or test data set\n",
    "    if train_or_test == 'train' or TRAIN_OR_ALL == 'all':\n",
    "        for comment, augmented_comment, grade in zip(comments, augmented_comments, grades):\n",
    "            ## remove empty comment\n",
    "            if PP.is_empty_sent(comment):\n",
    "                continue\n",
    "            \n",
    "            train_aug_comment_data.append(augmented_comment)\n",
    "            train_comment_data.append(comment)\n",
    "            train_pseudo_summary_data.append(pseudo_summary)\n",
    "            train_grade_data.append(grade)\n",
    "                \n",
    "    elif train_or_test == 'test':\n",
    "        for comment, augmented_comment, grade in zip(comments, augmented_comments, grades):\n",
    "            ## remove empty comment\n",
    "            if PP.is_empty_sent(comment):\n",
    "                continue\n",
    "                \n",
    "            test_aug_comment_data.append(augmented_comment)\n",
    "            test_comment_data.append(comment)\n",
    "            test_pseudo_summary_data.append(pseudo_summary)\n",
    "            test_grade_data.append(grade)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7c1b882b",
   "metadata": {},
   "source": [
    "### Sentiment analysis to assign positive and negative sample for computing cosine embedding loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "96722035",
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import BertForSequenceClassification\n",
    "from transformers import BertTokenizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cef90718",
   "metadata": {},
   "outputs": [],
   "source": [
    "sentiment_analysis_model_name = 'IDEA-CCNL/Erlangshen-Roberta-110M-Sentiment'\n",
    "\n",
    "sentiment_analysis_tokenizer = BertTokenizer.from_pretrained(sentiment_analysis_model_name)\n",
    "sentiment_analysis_model = BertForSequenceClassification.from_pretrained(sentiment_analysis_model_name).to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bf52ee76",
   "metadata": {},
   "outputs": [],
   "source": [
    "def sentiment_analysis_inference(text):\n",
    "    dataset = Tor.BatchSentenceDataset(text)\n",
    "    dataloader = DataLoader(dataset, batch_size=16, shuffle=False)\n",
    "    \n",
    "    prob_batch = []\n",
    "    with torch.no_grad():\n",
    "        for batch in dataloader:\n",
    "            encoding = sentiment_analysis_tokenizer(batch, padding=True, return_tensors='pt', truncation='longest_first', max_length=510)\n",
    "\n",
    "            for key in encoding:\n",
    "                if isinstance(encoding[key], Tensor):\n",
    "                    encoding[key] = encoding[key].to(device)\n",
    "\n",
    "            output = sentiment_analysis_model(**encoding)\n",
    "            postive_prob = torch.nn.functional.softmax(output.logits, dim=-1)[:, 1]\n",
    "            prob_batch.append(postive_prob)\n",
    "            \n",
    "    postive_probs = torch.cat(prob_batch)\n",
    "     ## -1 represent negative, 1 represent neutral or positive\n",
    "    sentiment_label = [1 if p > 0.3 else -1 for p in postive_probs]\n",
    "    \n",
    "    return sentiment_label"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2b304936",
   "metadata": {},
   "outputs": [],
   "source": [
    "# %%time\n",
    "if not COMMENT_AUGMENTATION:\n",
    "    train_comment_sentiment_data = sentiment_analysis_inference(train_comment_data)\n",
    "    if TRAIN_OR_ALL == 'train':\n",
    "        test_comment_sentiment_data = sentiment_analysis_inference(test_comment_data)\n",
    "else:\n",
    "    train_comment_sentiment_data = sentiment_analysis_inference(train_aug_comment_data)\n",
    "    if TRAIN_OR_ALL == 'train':\n",
    "        test_comment_sentiment_data = sentiment_analysis_inference(test_aug_comment_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ee4e338b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# if not COMMENT_AUGMENTATION:\n",
    "#     D.write_comment_sentiment(train_comment_sentiment_data, 'train')\n",
    "#     D.write_comment_sentiment(test_comment_sentiment_data, 'test')\n",
    "# else:\n",
    "#     D.write_aug_comment_sentiment(train_comment_sentiment_data, 'train')\n",
    "#     D.write_aug_comment_sentiment(test_comment_sentiment_data, 'test')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0158a72e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# if TRAIN_OR_ALL == 'train':\n",
    "#     if not COMMENT_AUGMENTATION:\n",
    "#         train_comment_sentiment_data = D.read_comment_sentiment('train')\n",
    "#         test_comment_sentiment_data = D.read_comment_sentiment('test')\n",
    "#     else:\n",
    "#         train_comment_sentiment_data = D.read_aug_comment_sentiment('train')\n",
    "#         test_comment_sentiment_data = D.read_aug_comment_sentiment('test')\n",
    "# elif TRAIN_OR_ALL == 'all':\n",
    "#     if not COMMENT_AUGMENTATION:\n",
    "#         train_comment_sentiment_data = D.read_comment_sentiment('all')\n",
    "#     else:\n",
    "#         train_comment_sentiment_data = D.read_aug_comment_sentiment('all')\n",
    "        \n",
    "#     test_comment_sentiment_data = []"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "58e26d82",
   "metadata": {},
   "outputs": [],
   "source": [
    "len(train_pseudo_summary_data), len(train_comment_data), len(train_aug_comment_data), len(train_grade_data), len(train_comment_sentiment_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f7d5f945",
   "metadata": {},
   "outputs": [],
   "source": [
    "from collections import Counter\n",
    "for g, c in Counter(train_grade_data).items():\n",
    "    print(g, c / len(train_grade_data) * 100)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d224bba5",
   "metadata": {},
   "outputs": [],
   "source": [
    "if TRAIN_OR_ALL == 'train':\n",
    "    len(test_pseudo_summary_data), len(test_comment_data), len(test_aug_comment_data), len(test_grade_data), len(test_comment_sentiment_data)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "55856254",
   "metadata": {},
   "source": [
    "### class weight"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f5eb5c51",
   "metadata": {},
   "outputs": [],
   "source": [
    "# from sklearn.utils import class_weight"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e6ca8353",
   "metadata": {},
   "outputs": [],
   "source": [
    "# class_weights = class_weight.compute_class_weight(\n",
    "#     'balanced', classes=np.unique(test_grade_data), y=test_grade_data\n",
    "# )\n",
    "# class_weights"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "690f191a",
   "metadata": {},
   "source": [
    "# Train DNN"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e369a406",
   "metadata": {},
   "source": [
    "### Apply one hot encoder to grade label"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1362d424",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.preprocessing import OneHotEncoder"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2344ac38",
   "metadata": {},
   "outputs": [],
   "source": [
    "V.TRAIN_GRADE_LABELS"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4ee1ebd3",
   "metadata": {},
   "outputs": [],
   "source": [
    "enc = OneHotEncoder()\n",
    "one_hot_vector = enc.fit_transform(V.TRAIN_GRADE_LABELS).toarray()\n",
    "one_hot_vector"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a5764c7e",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_ext_grade_data = np.array(train_grade_data).reshape(-1, 1)\n",
    "train_ext_grade_data = enc.transform(train_ext_grade_data).toarray()\n",
    "\n",
    "if TRAIN_OR_ALL == 'train':\n",
    "    test_ext_grade_data = np.array(test_grade_data).reshape(-1, 1)\n",
    "    test_ext_grade_data = enc.transform(test_ext_grade_data).toarray()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1b4ebdf4",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_ext_grade_data.shape\n",
    "\n",
    "if TRAIN_OR_ALL == 'train':\n",
    "    test_ext_grade_data.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "014964ef",
   "metadata": {},
   "source": [
    "### Create dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a0ccab77",
   "metadata": {},
   "outputs": [],
   "source": [
    "class PseudoSummaryEvaluationDataset(Dataset):\n",
    "    def __init__(self, pseudo_summaries, comments, aug_comments, grades, comments_sentiment):\n",
    "        ## list of sentences\n",
    "        self.pseudo_summaries = pseudo_summaries\n",
    "        self.comments = comments\n",
    "        self.aug_comments = aug_comments\n",
    "        self.grades = grades\n",
    "        self.comments_sentiment = comments_sentiment\n",
    "        \n",
    "    def __len__(self):\n",
    "        return len(self.grades)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        pseudo_summary = self.pseudo_summaries[idx]\n",
    "        comment = self.comments[idx]\n",
    "        aug_comment = self.aug_comments[idx]\n",
    "        grade = self.grades[idx]\n",
    "        comment_sentiment = self.comments_sentiment[idx]\n",
    "        \n",
    "        return pseudo_summary, comment, aug_comment, grade, comment_sentiment"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "41866db9",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_dataset = PseudoSummaryEvaluationDataset(\n",
    "    train_pseudo_summary_data, train_comment_data, train_aug_comment_data, train_ext_grade_data, train_comment_sentiment_data\n",
    ")\n",
    "train_dataloader = DataLoader(\n",
    "    train_dataset, batch_size=BATCH_SIZE, shuffle=True, collate_fn=lambda batch: batch,\n",
    "    num_workers=8, pin_memory=True\n",
    ")\n",
    "\n",
    "if TRAIN_OR_ALL == 'train':\n",
    "    test_dataset = PseudoSummaryEvaluationDataset(\n",
    "        test_pseudo_summary_data, test_comment_data, test_aug_comment_data, test_ext_grade_data, test_comment_sentiment_data\n",
    "    )\n",
    "    test_dataloader = DataLoader(\n",
    "        test_dataset, batch_size=BATCH_SIZE, collate_fn=lambda batch: batch,\n",
    "        num_workers=8, pin_memory=True\n",
    "    )"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "207aea69",
   "metadata": {},
   "source": [
    "## Model Building"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2edcf057",
   "metadata": {},
   "source": [
    "### Load BERT model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4cadd987",
   "metadata": {},
   "outputs": [],
   "source": [
    "if USE_SBERT_EMBED:\n",
    "    from sentence_transformers import SentenceTransformer\n",
    "    \n",
    "    bert_tokenizer = None\n",
    "    bert_model = SentenceTransformer(BERT_MODEL_NAME).to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4c964dd2",
   "metadata": {},
   "outputs": [],
   "source": [
    "if USE_BERT_EMBED:\n",
    "    from transformers import BertTokenizerFast, AutoModel\n",
    "\n",
    "    bert_tokenizer = BertTokenizerFast.from_pretrained(BERT_TOKENIZER_NAME)\n",
    "    bert_model = AutoModel.from_pretrained(BERT_MODEL_NAME).to(device)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1c2fe5ec",
   "metadata": {},
   "source": [
    "### Attention Network"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "46e59a96",
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch import nn"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b6e36aa4",
   "metadata": {},
   "source": [
    "### Perspective HAN"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e7be791b",
   "metadata": {},
   "outputs": [],
   "source": [
    "import utils.pHAN as PHAN"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fbd5fbb0",
   "metadata": {},
   "source": [
    "## Training loop & testing loop"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5c303987",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import classification_report\n",
    "from sklearn.metrics import confusion_matrix"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "81e17ef9",
   "metadata": {},
   "outputs": [],
   "source": [
    "## utils\n",
    "def get_data(batch):\n",
    "    batch_pseudo_summaries = [p[0] for p in batch]\n",
    "    batch_comments = [p[1] for p in batch]\n",
    "    batch_aug_comments = [p[2] for p in batch]\n",
    "    batch_grades = np.array([p[3] for p in batch])\n",
    "    batch_comments_sentiment = np.array([p[4] for p in batch])\n",
    "    \n",
    "    return batch_pseudo_summaries, batch_comments, batch_aug_comments, batch_grades, batch_comments_sentiment\n",
    "\n",
    "def logit_to_label(logits, return_numpy=False):\n",
    "    ## convert logits to labels\n",
    "    labels_idx = torch.argmax(logits, 1).cpu().detach().numpy()\n",
    "    labels = [V.GRADE_INDEX_TO_LABEL[idx] for idx in labels_idx]\n",
    "    \n",
    "    if return_numpy:\n",
    "        return np.array(labels)\n",
    "    \n",
    "    return labels\n",
    "\n",
    "def log_gradients(layer, layer_name, mean=True):\n",
    "    IO.log_dividing_line(logger, \"Gradient of {}\".format(layer_name))\n",
    "    for name, i in layer.named_parameters():\n",
    "        try:\n",
    "            logger.info(\"{} gradient mean: {}\".format(name, torch.mean(i.grad)))\n",
    "        except:\n",
    "            logger.info(\"{} : no gradient\".format(name))\n",
    "\n",
    "def decouple_loss_fn_dict(d):\n",
    "    cls_loss_fn = d[\"cls_loss_fn\"]\n",
    "    cos_dis_loss_fn = d[\"cos_dis_loss_fn\"]\n",
    "    con_loss_fn = d[\"con_loss_fn\"]\n",
    "    \n",
    "    return cls_loss_fn, cos_dis_loss_fn, con_loss_fn\n",
    "\n",
    "def decouple_loss_weight_dict(d):\n",
    "    cls_loss_weight = d[\"cls_loss_weight\"]\n",
    "    cos_dis_loss_weight = d[\"cos_dis_loss_weight\"]\n",
    "    con_loss_weight = d[\"con_loss_weight\"]\n",
    "\n",
    "    return cls_loss_weight, cos_dis_loss_weight, con_loss_weight"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7a7b9c32",
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_loop(\n",
    "    dataloader, model, loss_fn_dict, optimizer, eta, loss_weight_dict, epoch\n",
    "):\n",
    "    cls_loss_fn, cos_dis_loss_fn, con_loss_fn = decouple_loss_fn_dict(loss_fn_dict)\n",
    "    cls_loss_weight, cos_dis_loss_weight, con_loss_weight = decouple_loss_weight_dict(loss_weight_dict)\n",
    "    \n",
    "    size = len(dataloader.dataset)\n",
    "    \n",
    "    for _id, batch in enumerate(dataloader):\n",
    "        model.train()\n",
    "        IO.log_dividing_line(logger, \"Epoch {}, Batch {}\".format(epoch, _id))\n",
    "        batch_pseudo_summaries, batch_comments, batch_aug_comments, batch_grades, batch_comments_sentiment = get_data(batch)\n",
    "        batch_comments_sentiment = torch.tensor(batch_comments_sentiment).to(device)\n",
    "#         batch_grade_logits = torch.tensor(batch_grades).type(torch.float).to(device)\n",
    "        \n",
    "        # Compute prediction\n",
    "        if COMMENT_AUGMENTATION:\n",
    "            projected_embed, logits, _, _ = model(batch_pseudo_summaries, batch_aug_comments, eta)\n",
    "        else:\n",
    "            projected_embed, logits, _, _ = model(batch_pseudo_summaries, batch_comments, eta)\n",
    "        comments_embed = model.encode(batch_comments)\n",
    "        \n",
    "        # Compute loss\n",
    "#         cls_loss = cls_loss_fn(logits, batch_grade_logits)\n",
    "        \n",
    "        ## compute cosine embedding loss normally\n",
    "        if NEG_SAMPLE:\n",
    "            cos_dis_loss = cos_dis_loss_fn(projected_embed, comments_embed, batch_comments_sentiment.to(device))\n",
    "        else:\n",
    "            cos_dis_loss = cos_dis_loss_fn(\n",
    "                projected_embed, comments_embed, torch.ones(comments_embed.shape[0]).to(device)\n",
    "            )\n",
    "\n",
    "#         con_loss = con_loss_fn(projected_embed, labels=batch_grade_logits.argmax(1))\n",
    "#         loss = cls_loss_weight * cls_loss + cos_dis_loss_weight * cos_dis_loss + con_loss_weight * con_loss\n",
    "        loss = cos_dis_loss\n",
    "        \n",
    "#         pred_labels = logit_to_label(logits)\n",
    "#         true_labels = logit_to_label(batch_grade_logits)\n",
    "        \n",
    "#         logger.info(\"pred logits: {}\".format(logits))\n",
    "#         logger.info(\"pred label: \".format(pred_labels))\n",
    "#         logger.info(\"grades label: \".format(true_labels))\n",
    "\n",
    "        # Backpropagation\n",
    "        optimizer.zero_grad()\n",
    "        loss.backward()\n",
    "        \n",
    "        ## apply gradient clipping\n",
    "#         nn.utils.clip_grad_norm_(model.parameters(), GRADIENT_MAX_NORM)\n",
    "#         nn.utils.clip_grad_value_(model.parameters(), clip_value=GRADIENT_CLIP_VALUE)\n",
    "        optimizer.step()\n",
    "\n",
    "        ## log gradient\n",
    "#         if COMPRESSION:\n",
    "#             log_gradients(model.compression_layer, \"compression_layer\")\n",
    "#         log_gradients(model.sentence_att_net, \"sentence_att_net\")\n",
    "#         log_gradients(model.perspective_att_net, \"perspective_att_net\")\n",
    "# #         log_gradients(model.grade_classifier, \"grade_classifier\")\n",
    "#         log_gradients(model.project_head, \"project_head\")\n",
    "        \n",
    "        ## log training loss\n",
    "#         IO.log_dividing_line(logger, \"Loss\")\n",
    "#         logger.info(\"Total loss: \".format(loss))\n",
    "#         logger.info(\"cls loss: \".format(cls_loss))\n",
    "#         logger.info(\"cos dis loss: \".format(cos_dis_loss))\n",
    "#         logger.info(\"con loss: \".format(con_loss))\n",
    "        \n",
    "        if _id % 10 == 0:\n",
    "            loss, current = loss.item(), _id * (len(batch_pseudo_summaries)+1)\n",
    "            logger.info(\"loss: {:>7f}  [{:>5d}/{:>5d}]\".format(loss, current, size))\n",
    "\n",
    "            \n",
    "def test_loop(\n",
    "    dataloader, model, loss_fn_dict, eta, loss_weight_dict\n",
    "):\n",
    "    cls_loss_fn, cos_dis_loss_fn, con_loss_fn = decouple_loss_fn_dict(loss_fn_dict)\n",
    "    cls_loss_weight, cos_dis_loss_weight, con_loss_weight = decouple_loss_weight_dict(loss_weight_dict)\n",
    "    \n",
    "    IO.log_dividing_line(logger)\n",
    "    IO.log_dividing_line(logger, \"Testing\")\n",
    "    IO.log_dividing_line(logger)\n",
    "    \n",
    "    size = len(dataloader.dataset)\n",
    "    num_batches = len(dataloader)\n",
    "    test_loss, correct = 0, 0\n",
    "\n",
    "#     pred_label_list = []\n",
    "#     true_label_list = []\n",
    "    \n",
    "    with torch.no_grad():\n",
    "        for batch in dataloader:\n",
    "            model.eval()\n",
    "            batch_pseudo_summaries, batch_comments, batch_aug_comments, batch_grades, batch_comments_sentiment = get_data(batch)\n",
    "            batch_comments_sentiment = torch.tensor(batch_comments_sentiment).to(device)\n",
    "#             batch_grade_logits = torch.tensor(batch_grades).type(torch.float).to(device)\n",
    "            comments_embed = model.encode(batch_comments)\n",
    "    \n",
    "            # Compute prediction\n",
    "            if COMMENT_AUGMENTATION:\n",
    "                projected_embed, logits, _, _ = model(batch_pseudo_summaries, batch_aug_comments, eta)\n",
    "            else:\n",
    "                projected_embed, logits, _, _ = model(batch_pseudo_summaries, batch_comments, eta)\n",
    "        \n",
    "            # Compute loss\n",
    "    #         cls_loss = cls_loss_fn(logits, batch_grade_logits)\n",
    "            if NEG_SAMPLE:\n",
    "                cos_dis_loss = cos_dis_loss_fn(projected_embed, comments_embed, batch_comments_sentiment.to(device))\n",
    "            else:\n",
    "                cos_dis_loss = cos_dis_loss_fn(\n",
    "                    projected_embed, comments_embed, torch.ones(comments_embed.shape[0]).to(device)\n",
    "                )\n",
    "                \n",
    "    #         con_loss = con_loss_fn(projected_embed, labels=batch_grade_logits.argmax(1))\n",
    "    #         loss = cls_loss_weight * cls_loss + cos_dis_loss_weight * cos_dis_loss + con_loss_weight * con_loss\n",
    "            loss = cos_dis_loss\n",
    "            test_loss += loss\n",
    "\n",
    "            # Compute accuracy\n",
    "#             pred_labels = logit_to_label(logits, return_numpy=True)\n",
    "#             true_labels = logit_to_label(batch_grade_logits, return_numpy=True)\n",
    "            \n",
    "#             pred_label_list.append(pred_labels)\n",
    "#             true_label_list.append(true_labels)\n",
    "            \n",
    "#             correct += (pred_labels == true_labels).sum()\n",
    "\n",
    "#     y_test = list(chain.from_iterable(true_label_list))\n",
    "#     y_test_pred = list(chain.from_iterable(pred_label_list))\n",
    "    \n",
    "#     logger.info(\"Test Classification Report\")\n",
    "#     logger.info(classification_report(y_test, y_test_pred))\n",
    "#     logger.info(\"Confusion Matrix\")\n",
    "#     logger.info(confusion_matrix(y_test, y_test_pred))\n",
    "        \n",
    "    test_loss /= num_batches\n",
    "    correct /= size\n",
    "    logger.info(\"Test Error: \\nAccuracy: {:>0.1f}%, Avg loss: {:>8f} \\n\".format(100*correct, test_loss))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9283493b",
   "metadata": {},
   "source": [
    "## Model initialization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7ba47e46",
   "metadata": {},
   "outputs": [],
   "source": [
    "pHAN = PHAN.PerspectiveHierarchicalAttentionNetwork(\n",
    "    bert_model, bert_tokenizer, perspective_mean_embed, \n",
    "    NUM_PERSPECTIVE, TOP_K, BERT_DIM, SENT_DIM, CXT_DIM, PRJ_DIM, \n",
    "    sent_temperature=SENT_TEMPERATURE, pers_temperature=PERS_TEMPERATURE, \n",
    "    dropout_rate=DROPOUT_RATE, leaky_relu_negative_slope=LEAKY_RELU_NEG_SLOPE, \n",
    "    encode_batch_size=ENC_BZ, compression=COMPRESSION, projection=PROJECTION,\n",
    "    attention_empty_mask=ATTENTION_EMPTY_MASK, freeze_bert=FREEZE_BERT\n",
    ").to(device)\n",
    "pHAN"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "be49ca19",
   "metadata": {},
   "source": [
    "### Number of parameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d2c30a23",
   "metadata": {},
   "outputs": [],
   "source": [
    "num_params = torch.tensor(0)\n",
    "\n",
    "for parameter in pHAN.parameters():\n",
    "    if parameter.requires_grad:\n",
    "        num_params += torch.prod(torch.tensor(parameter.shape))\n",
    "\n",
    "num_params"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3e741331",
   "metadata": {},
   "source": [
    "## Train with whole dataset"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fdac3210",
   "metadata": {},
   "source": [
    "### Loss functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5bd349bd",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pytorch_metric_learning import losses"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5573f1b6",
   "metadata": {},
   "outputs": [],
   "source": [
    "## classification loss\n",
    "weight = torch.tensor([3, 1.5, 1, 1]).to(device)\n",
    "# weight = torch.tensor(class_weights).to(device)\n",
    "cls_loss_fn = nn.BCELoss(weight)\n",
    "# class_weights"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "589dcc8d",
   "metadata": {},
   "outputs": [],
   "source": [
    "## cosine similarity loss w.r.t comment\n",
    "cos_dis_loss_fn = nn.CosineEmbeddingLoss()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "660d2c08",
   "metadata": {},
   "outputs": [],
   "source": [
    "## contrastive loss\n",
    "con_loss_fn = losses.SupConLoss(temperature=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "92b8bff0",
   "metadata": {},
   "outputs": [],
   "source": [
    "loss_fn_dict = {\n",
    "    \"cls_loss_fn\": cls_loss_fn,\n",
    "    \"cos_dis_loss_fn\": cos_dis_loss_fn,\n",
    "    \"con_loss_fn\": con_loss_fn\n",
    "}"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "41e720be",
   "metadata": {},
   "source": [
    "### Learning rate scheduler"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9bc53cb4",
   "metadata": {},
   "outputs": [],
   "source": [
    "low_lr_param_list = ['grade_classifier']\n",
    "\n",
    "low_lr_params = list(filter(\n",
    "    lambda kv: sum([_name in kv[0] for _name in low_lr_param_list]),\n",
    "    pHAN.named_parameters()\n",
    "))\n",
    "low_lr_params = [params[1] for params in low_lr_params]\n",
    "\n",
    "base_lr_params = list(filter(\n",
    "    lambda kv: sum([_name not in kv[0] for _name in low_lr_param_list]),\n",
    "    pHAN.named_parameters()\n",
    "))\n",
    "base_lr_params = [params[1] for params in base_lr_params]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "93f92f19",
   "metadata": {},
   "outputs": [],
   "source": [
    "base_learning_rate = 5 * 1e-4\n",
    "\n",
    "optimizer = torch.optim.AdamW(\n",
    "    [\n",
    "        {\"params\": base_lr_params, \"lr\": 5 * 1e-4},\n",
    "        {\"params\": low_lr_params, \"lr\": 5 * 1e-4},\n",
    "        {\"params\": torch.tensor([1]), \"lr\": 1}, ## eta\n",
    "    ],\n",
    "    lr=base_learning_rate\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3e37d463",
   "metadata": {},
   "outputs": [],
   "source": [
    "lr_gamma = 0.8\n",
    "step_size = 2\n",
    "sch_lambda_params = lambda epoch: lr_gamma ** (epoch // step_size)\n",
    "sch_lambda_eta = lambda epoch: max(0, 1 - 0.25 * (epoch // 1 // step_size))\n",
    "\n",
    "scheduler = torch.optim.lr_scheduler.LambdaLR(\n",
    "    optimizer, lr_lambda=[sch_lambda_params, sch_lambda_params, sch_lambda_eta]\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b82df842",
   "metadata": {},
   "outputs": [],
   "source": [
    "epochs = 20\n",
    "\n",
    "# for epoch in range(epochs):\n",
    "#     print(epoch, scheduler.get_last_lr())\n",
    "#     scheduler.step()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5b18e7e1",
   "metadata": {},
   "outputs": [],
   "source": [
    "from datetime import datetime"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "faee71bb",
   "metadata": {},
   "outputs": [],
   "source": [
    "def save_model(t):\n",
    "    model_name = \"epoch_{:04d}.pt\".format(t)\n",
    "    fn = os.path.join(MODEL_SAVE_DIR_PATH, model_name)\n",
    "    \n",
    "    torch.save({\n",
    "        'epoch': t,\n",
    "        'use_sbert_embed': USE_SBERT_EMBED,\n",
    "        'use_bert_embed': USE_BERT_EMBED,\n",
    "        'num_perspective': NUM_PERSPECTIVE,\n",
    "        'train_or_all': TRAIN_OR_ALL,\n",
    "        'model_state_dict': pHAN.state_dict(),\n",
    "        'optimizer_state_dict': optimizer.state_dict(),\n",
    "        'scheduler_state_dict': scheduler.state_dict(),\n",
    "        'top_k': TOP_K,\n",
    "        'bert_model': BERT_MODEL_NAME,\n",
    "        'bert_tokenizer': BERT_TOKENIZER_NAME,\n",
    "        'bert_topic_model': BERTOPIC_MODEL_NAME,\n",
    "        'model_save_dir': MODEL_SAVE_DIR,\n",
    "        'comment_augmentation': COMMENT_AUGMENTATION,\n",
    "        'perspective_mean_embed': perspective_mean_embed,\n",
    "        'bert_dim': BERT_DIM,\n",
    "        'sent_dim': SENT_DIM,\n",
    "        'cxt_dim': CXT_DIM,\n",
    "        'prj_dim': PRJ_DIM,\n",
    "        'sent_temperature': SENT_TEMPERATURE,\n",
    "        'pers_temperature': PERS_TEMPERATURE,\n",
    "        'dropout_rate': DROPOUT_RATE,\n",
    "        'leaky_relu_negative_slope': LEAKY_RELU_NEG_SLOPE,\n",
    "        'encode_batch_size': ENC_BZ,\n",
    "        'compression': COMPRESSION,\n",
    "        'projection': PROJECTION,\n",
    "        'attention_empty_mask': ATTENTION_EMPTY_MASK,\n",
    "        'freeze_bert': FREEZE_BERT,\n",
    "        'gradient_max_norm': GRADIENT_MAX_NORM,\n",
    "        'gradient_clip_value': GRADIENT_CLIP_VALUE,\n",
    "        'random_state': RANDOM_STATE,\n",
    "    }, fn)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "26267ecf",
   "metadata": {},
   "source": [
    "## Training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2e5afded",
   "metadata": {},
   "outputs": [],
   "source": [
    "cls_loss_weight = 0\n",
    "cos_dis_loss_weight = 1\n",
    "con_loss_weight = 0\n",
    "\n",
    "loss_weight_dict = {\n",
    "    \"cls_loss_weight\": cls_loss_weight,\n",
    "    \"cos_dis_loss_weight\": cos_dis_loss_weight,\n",
    "    \"con_loss_weight\": con_loss_weight\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d7315600",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# %%time\n",
    "\n",
    "for t in tqdm(range(1, epochs+1)):\n",
    "    now = datetime.now(timezone)\n",
    "    ts = datetime.strftime(now,'%Y-%m-%d_%H:%M:%S')\n",
    "    IO.log_dividing_line(logger, \"Epoch {}, Timestamp: {}\".format(t, ts))\n",
    "    \n",
    "    ## get current eta\n",
    "    eta = scheduler.get_last_lr()[-1]\n",
    "    \n",
    "    train_loop(train_dataloader, pHAN, loss_fn_dict, optimizer, eta, loss_weight_dict, t)\n",
    "    \n",
    "    if TRAIN_OR_ALL == 'train':\n",
    "        test_loop(test_dataloader, pHAN, loss_fn_dict, eta, loss_weight_dict)\n",
    "\n",
    "    scheduler.step()\n",
    "    \n",
    "    ## [TODO] log the results\n",
    "    if t % (step_size*5) == 0:\n",
    "        ## Save the model\n",
    "        IO.log_dividing_line(logger, \"Saving model from epoch {:04d}...\".format(t))\n",
    "        save_model(t)\n",
    "    \n",
    "logger.info(\"Done!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c7a10b42",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "research_venv",
   "language": "python",
   "name": "research_venv"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
