{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import os\n",
    "import torch\n",
    "from collections import defaultdict\n",
    "\n",
    "from tqdm import tqdm\n",
    "tqdm.pandas(desc=\"progress: \")\n",
    "\n",
    "from importlib import reload\n",
    "from docx import Document\n",
    "from bertopic import BERTopic\n",
    "\n",
    "# Utility variable\n",
    "import sys, getopt\n",
    "sys.path.insert(0, '../..')\n",
    "\n",
    "# var\n",
    "import var.path as P\n",
    "\n",
    "# utils\n",
    "import utils.bertopic as BT\n",
    "import utils.data as D\n",
    "import utils.io as IO\n",
    "import utils.torch as Tor"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Process Command Line Arguments"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "opts, args = getopt.getopt(sys.argv[1:], \"ab:s:e:f:\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "TRAIN_OR_ALL = 'train'\n",
    "BATCH_SIZE = 70\n",
    "START_IDX = 0\n",
    "END_IDX = 2\n",
    "\n",
    "for opt, arg in opts:\n",
    "    if opt == '-a':\n",
    "        TRAIN_OR_ALL = 'all'\n",
    "    elif opt == '-b':\n",
    "        BATCH_SIZE\n",
    "    elif opt == '-s':\n",
    "        START_IDX = int(arg)\n",
    "    elif opt == '-e':\n",
    "        END_IDX = int(arg)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Hyper-parameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "SBERT_MODEL_NAME = 'ckiplab/bert-base-chinese'\n",
    "\n",
    "GPU_NUM = 0\n",
    "\n",
    "START_YEAR = 106\n",
    "END_YEAR = 112\n",
    "\n",
    "if TRAIN_OR_ALL == 'train':\n",
    "    BERTOPIC_MODEL_NAME = \"BERTopic_custom_mcs_100_ckip_diversified_low_train\"\n",
    "elif TRAIN_OR_ALL == 'all':\n",
    "    BERTOPIC_MODEL_NAME = \"BERTopic_custom_mcs_100_ckip_diversified_low_all\"\n",
    "\n",
    "SPLITTER = 'ï¼„'\n",
    "\n",
    "DEBUG = False\n",
    "\n",
    "RANDOM_STATE = 42\n",
    "np.random.seed(RANDOM_STATE)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Import modules"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import utils.sentence_scorer as S"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Disable hugging face tokenizer parallelism\n",
    "os.environ[\"TOKENIZERS_PARALLELISM\"] = \"false\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## GPU setup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "device = torch.device(GPU_NUM)\n",
    "\n",
    "S.GPU_NUM = GPU_NUM\n",
    "S.device = device\n",
    "\n",
    "S.gpu_setting()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Test tuple setup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# random pick test data\n",
    "df_applicants = D.read_df_applicants()\n",
    "df_applications = D.read_df_applications()\n",
    "# test_df = pd.read_csv(\"112_F.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# df_applicants = pd.concat([df_applicants, test_df])\n",
    "# df_applicants"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_applications = pd.merge(\n",
    "    df_applications, df_applicants[['year', 'id', 'name']], how='left', on=['year', 'id']\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_applications.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_applications.name = df_applications.name.fillna('?')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tuples = df_applications.apply(lambda row: (row['year'], row['id'], row['name']), axis=1).to_list()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tuples = [{\n",
    "    'year': info[0],\n",
    "    'id': info[1],\n",
    "    'name': info[2],\n",
    "} for info in tuples]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "len(tuples)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tuples = tuples[START_IDX:END_IDX]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "len(tuples)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def dict_info_to_tuple_info(dict_info):\n",
    "    _year = dict_info['year']\n",
    "    _id = dict_info['id']\n",
    "    _name = dict_info['name']\n",
    "    tuple_info = (_year, _id, _name)\n",
    "    \n",
    "    return tuple_info"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "DATA_SHEET_SUMMARY_WEIGHT = {\n",
    "    'knn_conf': 0,\n",
    "    'topic_match': 1,\n",
    "    'claim': 0,\n",
    "    'future_plan': 0,\n",
    "    'evidence': 2,\n",
    "    'uniqueness': 0,\n",
    "}\n",
    "\n",
    "SELF_STATEMENT_SUMMARY_WEIGHT = {\n",
    "    'knn_conf': 0,\n",
    "    'topic_match': 1,\n",
    "    'claim': 2,\n",
    "    'future_plan': 0.25,\n",
    "    'evidence': 2,\n",
    "    'uniqueness': 0,\n",
    "}\n",
    "\n",
    "RECOMMENDATION_LETTER_SUMMARY_WEIGHT = {\n",
    "    'knn_conf': 0,\n",
    "    'topic_match': 1,\n",
    "    'claim': 2,\n",
    "    'future_plan': 0,\n",
    "    'evidence': 0,\n",
    "    'uniqueness': 0,\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def defaultdict_init_defaultdict_init_by_int():\n",
    "    return defaultdict(int)\n",
    "\n",
    "def defaultdict_init_defaultdict_init_by_float():\n",
    "    return defaultdict(float)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "empty_candidate_sents_info = {\n",
    "    \"sents\": [],\n",
    "    \"sents_avg_importance_dict\": {},\n",
    "    \"sents_topic_importance_dict\": {},\n",
    "    \"sents_topic_id_dict\": {},\n",
    "    \"topic_sent_dict\": {},\n",
    "    \"refs\": {},\n",
    "}\n",
    "\n",
    "empty_chunk_debug_info = {\n",
    "    \"chunks\": [],\n",
    "    \"predicted_topics\": [],\n",
    "    \"predicted_knn_confs\": [],\n",
    "    \"predicted_neighbors_sc_idx\": [],\n",
    "    \"knn_confidence\": [],\n",
    "    \"topic_match_score\": [],\n",
    "    \"claim_score\": [],\n",
    "    \"future_plan_score\": [],\n",
    "    \"evidence_score\": [],\n",
    "    \"uniqueness_score\": [],\n",
    "    \"importance\": [],\n",
    "    \"refs\": {},\n",
    "}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Get the reduced embeddings and labels from comment sentences"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Load BERTopic Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "topic_doc_tokenizer = BT.topic_doc_tokenizer\n",
    "custom_update_topics = BT.custom_update_topics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "topic_model = BERTopic.load(\n",
    "    os.path.join(P.FP_COMMENT_CLUSTERING_MODEL_DIR, BERTOPIC_MODEL_NAME),\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sbert_model = topic_model.embedding_model.embedding_model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Fetch class label and representatives for each topic"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "topic_rep_dict = topic_model.get_representative_docs()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "topic_rep_dict[-1] = ['0', '0', '0']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "topic_rep_dict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "topic_class_label = {}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "topic_info = topic_model.get_topic_info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def extract_topic_class_label(topic_rep):\n",
    "    chunks = topic_rep.split('_')\n",
    "    tid = int(chunks[0])\n",
    "    topic_class_label[tid] = chunks[1:]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "_ = topic_info['Name'].apply(extract_topic_class_label)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "topic_class_label"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Load comment sentences"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_split_comments = D.read_df_split_comments_no_duplicate(TRAIN_OR_ALL)\n",
    "split_comments = D.read_split_comments_no_duplicate(TRAIN_OR_ALL)\n",
    "df_tokenization_database = df_split_comments"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "len(df_split_comments)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "len(split_comments)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Get reduced embeddings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "reduced_split_comments_embeds = topic_model.umap_model['umap'].embedding_\n",
    "reduced_split_comments_embeds = topic_model.umap_model['norm'].transform(reduced_split_comments_embeds)\n",
    "reduced_split_comments_embeds.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Get the topic labels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import hdbscan"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# %%time\n",
    "topic_labels = topic_model.hdbscan_model.labels_\n",
    "topic_labels = topic_model._map_predictions(topic_labels)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Get the sentiment of the comment"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# from transformers import BertForSequenceClassification\n",
    "# from transformers import BertTokenizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# from torch.utils.data import Dataset\n",
    "# from torch.utils.data import DataLoader\n",
    "# from torch import Tensor"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# sentiment_analysis_model_name = 'IDEA-CCNL/Erlangshen-Roberta-110M-Sentiment'\n",
    "\n",
    "# sentiment_analysis_tokenizer = BertTokenizer.from_pretrained(sentiment_analysis_model_name)\n",
    "# sentiment_analysis_model = BertForSequenceClassification.from_pretrained(sentiment_analysis_model_name).to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# def sentiment_analysis_inference(text):\n",
    "#     dataset = Tor.BatchSentenceDataset(text)\n",
    "#     dataloader = DataLoader(dataset, batch_size=16, shuffle=False)\n",
    "    \n",
    "#     prob_batch = []\n",
    "#     with torch.no_grad():\n",
    "#         for batch in dataloader:\n",
    "#             encoding = sentiment_analysis_tokenizer(batch, padding=True, return_tensors='pt', truncation='longest_first', max_length=510)\n",
    "\n",
    "#             for key in encoding:\n",
    "#                 if isinstance(encoding[key], Tensor):\n",
    "#                     encoding[key] = encoding[key].to(device)\n",
    "\n",
    "#             output = sentiment_analysis_model(**encoding)\n",
    "#             postive_prob = torch.nn.functional.softmax(output.logits, dim=-1)[:, 1]\n",
    "#             prob_batch.append(postive_prob)\n",
    "            \n",
    "#     postive_probs = torch.cat(prob_batch)\n",
    "#      ## -1 represent negative, 1 represent neutral or positive\n",
    "#     sentiment_label = [1 if p > 0.3 else -1 for p in postive_probs]\n",
    "    \n",
    "#     return sentiment_label"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# import utils.torch as Tor"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # %%time\n",
    "# split_comments_sentiment = sentiment_analysis_inference(split_comments)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# import pickle\n",
    "\n",
    "# with open('./train_split_comments_sentiment.pkl', 'wb') as f:\n",
    "#     pickle.dump(split_comments_sentiment, f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pickle"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "split_comments_sentiment = D.read_split_comments_sentiment(TRAIN_OR_ALL)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "len(split_comments_sentiment)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Application Inference\n",
    "Predict the topic of the application sentences with respect to the comment cluster using K-Nearest Neighbor algorithm (KNN)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Read Dataframe"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Read achievements"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_achievements = D.read_df_achievements()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_achievements.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Read self-statement"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_applications.tail()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Read recommendation-letter "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_recommendation_letters = D.read_df_recommendation_letters()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "df_recommendation_letters.tail()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Read Summaries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_summary = D.read_df_summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_summary.tail()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Summary Generation Utility"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.neighbors import KNeighborsClassifier, RadiusNeighborsClassifier, NearestCentroid"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from itertools import chain"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_evidences(_year, _id):\n",
    "    rows = df_recommendation_letters.query('`year` == {} and `id` == {}'.format(_year, _id))\n",
    "    \n",
    "    try:\n",
    "        rls_chunks = rows['all_paragraph_chunk'].to_list()\n",
    "    except:\n",
    "        rls_chunks = []\n",
    "        \n",
    "    if rls_chunks == None:\n",
    "        rls_chunks = []\n",
    "        \n",
    "    chunks = list(chain.from_iterable(rls_chunks))\n",
    "            \n",
    "    return chunks"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_recommendation_letter_uniqueness_ref_sents(_year, _id):\n",
    "    row = df_achievements.query('`year` == {} and `id` == {}'.format(_year, _id))\n",
    "    \n",
    "    try:\n",
    "        ref_sents = row['achievement'].to_list() + row['self_statement_sent'].to_list()[0]\n",
    "    except:\n",
    "        ref_sents = []\n",
    "        \n",
    "    if ref_sents == None:\n",
    "        ref_sents = []\n",
    "        \n",
    "    return ref_sents"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_topic_prediction(topic_model, chunks, n_neighbors, method=\"k\", radius=0.02):\n",
    "    if method == \"k\":\n",
    "        neigh = KNeighborsClassifier(n_neighbors=n_neighbors)\n",
    "        neigh.fit(reduced_split_comments_embeds, topic_labels)\n",
    "    elif method == \"r\":\n",
    "        neigh = KNeighborsClassifier(radius=radius, outlier_label=-1)\n",
    "        neigh.fit(reduced_split_comments_embeds, topic_labels)\n",
    "        \n",
    "    ## get reduce chunk embeddings\n",
    "    chunk_embeds = topic_model.embedding_model.embed(chunks)\n",
    "    chunk_reduced_embeds = topic_model.umap_model.transform(chunk_embeds)\n",
    "    ## predict topic and confidence\n",
    "    predicted_topics = neigh.predict(chunk_reduced_embeds)\n",
    "    predicted_confs = neigh.predict_proba(chunk_reduced_embeds)\n",
    "    predicted_neighbors_idx = neigh.kneighbors(chunk_reduced_embeds, n_neighbors=n_neighbors, return_distance=False)\n",
    "    \n",
    "    return predicted_topics, predicted_confs, predicted_neighbors_idx"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics.pairwise import cosine_similarity"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def calculate_uniqueness_score(chunks, predicted_neighbors_sc_idx):\n",
    "    chunks_embed = sbert_model.encode(chunks, batch_size=128, show_progress_bar=False)\n",
    "    scores = []\n",
    "    \n",
    "    for chunk_embed, pred_neigh_idx in zip(chunks_embed, predicted_neighbors_sc_idx):\n",
    "        ## find the outlier comments from the neighbors\n",
    "        outliers_idx = [_idx for _idx in pred_neigh_idx if topic_labels[_idx] == -1]\n",
    "        ## filter out negative comments\n",
    "        outliers_idx = [_idx for _idx in outliers_idx if split_comments_sentiment[_idx] == 1]\n",
    "        outliers = [split_comments[_idx] for _idx in outliers_idx]\n",
    "        \n",
    "        if len(outliers) == 0:\n",
    "            scores.append(0)\n",
    "            continue\n",
    "        \n",
    "        ## compute the average cosine similarity among chunk and neighbors\n",
    "        outliers_embed = sbert_model.encode(outliers, batch_size=128, show_progress_bar=False)\n",
    "        avg_sim = cosine_similarity([chunk_embed], outliers_embed).mean()\n",
    "        \n",
    "        scores.append(avg_sim)\n",
    "        \n",
    "    return scores"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def calculate_candidate_sents_score(\n",
    "    chunks,\n",
    "    sents,\n",
    "    evidences,\n",
    "    rl_uniqueness_refs,\n",
    "    topic_model,\n",
    "    imp_weights,\n",
    "    debug,\n",
    "    n_neighbors=25,\n",
    "):\n",
    "    ## Deal with empty corpus\n",
    "    if chunks == None or sents == None or len(chunks) == 0 or len(sents) == 0:\n",
    "        return empty_candidate_sents_info, empty_chunk_debug_info\n",
    "    \n",
    "    ## Predict topics on text chunks\n",
    "    predicted_topics, predicted_knn_confs, predicted_neighbors_sc_idx = get_topic_prediction(\n",
    "        topic_model, chunks, n_neighbors\n",
    "    )\n",
    "    \n",
    "    ## get topic reps for each chunks\n",
    "    topic_class_reps = [topic_rep_dict[tid] for tid in predicted_topics]\n",
    "    \n",
    "    skip_calculate_score = np.zeros(len(chunks))\n",
    "    ## Calculate chunk candidates importance\n",
    "    ## knn confidence\n",
    "    knn_confidence = np.max(predicted_knn_confs, axis=1)\n",
    "    ## topic matching score\n",
    "    if imp_weights['topic_match'] > 0:\n",
    "        topic_match_score = S.topic_match_score(chunks, topic_class_reps, batch_size=BATCH_SIZE)\n",
    "    else:\n",
    "        topic_match_score = skip_calculate_score\n",
    "    ## claim score\n",
    "    if imp_weights['claim'] > 0:\n",
    "        claim_score = np.array([S.claim_score(c, chunks, batch_size=BATCH_SIZE) for c in chunks])\n",
    "    else:\n",
    "        claim_score = skip_calculate_score\n",
    "    ## future plan score\n",
    "    if imp_weights['future_plan'] > 0:\n",
    "        future_plan_score = S.future_plan_score(chunks, batch_size=BATCH_SIZE)\n",
    "    else:\n",
    "        future_plan_score = skip_calculate_score\n",
    "    ## evidence score\n",
    "    if imp_weights['evidence'] > 0:\n",
    "        evidence_score = S.evidence_score(chunks, evidences)\n",
    "    else:\n",
    "        evidence_score = skip_calculate_score\n",
    "    ## [TODO] uniqueness score\n",
    "    if imp_weights['uniqueness'] > 0:\n",
    "        uniqueness_score = calculate_uniqueness_score(chunks, predicted_neighbors_sc_idx)\n",
    "    else:\n",
    "        uniqueness_score = skip_calculate_score\n",
    "    \n",
    "    ## [TODO] recommendation letter uniqueness score\n",
    "#     if imp_weights['rl_uniqueness'] > 0:\n",
    "#         uniqueness_score = S.uniqueness_score(chunks, rl_uniqueness_refs, predicted_topics, predicted_neighbors_sc_idx)\n",
    "#     else:\n",
    "#         uniqueness_score = skip_calculate_score\n",
    "    \n",
    "    ## importance\n",
    "    importance = (\\\n",
    "        imp_weights['knn_conf'] * knn_confidence + \\\n",
    "        imp_weights['topic_match'] * topic_match_score + \\\n",
    "        imp_weights['claim'] * (1 - claim_score) + \\\n",
    "        imp_weights['future_plan'] * (1 - future_plan_score) + \\\n",
    "        imp_weights['evidence'] * evidence_score + \\\n",
    "        imp_weights['uniqueness'] * uniqueness_score \\\n",
    "     ) / sum(imp_weights.values())\n",
    "    \n",
    "    sents_avg_importance_dict = defaultdict(list)\n",
    "    sents_topic_importance_dict = defaultdict(defaultdict_init_defaultdict_init_by_float)\n",
    "    topic_sent_dict = defaultdict(set)\n",
    "    sents_topic_id_dict = defaultdict(defaultdict_init_defaultdict_init_by_int)\n",
    "    ## Aggregate sentence importance score over chunk importance score\n",
    "    for chunk, imp, topic in zip(chunks, importance, predicted_topics):\n",
    "        ## find the sentence cotaining the chunk\n",
    "        for sent in sents:\n",
    "            ## aggregate chunk importance\n",
    "            ## [TODO] use df_comment to refine the result (because split chunk may not be exact match to sent)\n",
    "            if chunk in sent:\n",
    "                sents_avg_importance_dict[sent].append(imp)\n",
    "                sents_topic_importance_dict[sent][topic] += imp\n",
    "                topic_sent_dict[topic].add(sent)\n",
    "                sents_topic_id_dict[sent][topic] += 1\n",
    "                \n",
    "    ## Calculate the importance score of the sentence\n",
    "    sents_avg_importance_dict = {\n",
    "        sent: np.mean(imp_list) for sent, imp_list in sents_avg_importance_dict.items()\n",
    "    }\n",
    "    \n",
    "    if debug:\n",
    "        for sent, imp in sents_avg_importance_dict.items():\n",
    "            print(sent, imp, sents_topic_id_dict[sent]) \n",
    "    \n",
    "    ## remember to update the following variable and function if the key values are changed\n",
    "    ## empty_candidate_sents_info\n",
    "    ## merge_candidate_sents_info\n",
    "    candidate_sents_info = {\n",
    "        \"sents\": sents,\n",
    "        \"sents_avg_importance_dict\": sents_avg_importance_dict,\n",
    "        \"sents_topic_importance_dict\": sents_topic_importance_dict,\n",
    "        \"sents_topic_id_dict\": sents_topic_id_dict,\n",
    "        \"topic_sent_dict\": topic_sent_dict,\n",
    "    }\n",
    "    \n",
    "    ## remember to update the following variable and function if the key values are changed\n",
    "    ## empty_chunk_debug_info\n",
    "    ## merge_chunk_debug_info\n",
    "    chunk_debug_info = {\n",
    "        \"chunks\": chunks,\n",
    "        \"predicted_topics\": predicted_topics,\n",
    "        \"predicted_knn_confs\": predicted_knn_confs,\n",
    "        \"predicted_neighbors_sc_idx\": predicted_neighbors_sc_idx,\n",
    "        \"knn_confidence\": knn_confidence,\n",
    "        \"topic_match_score\": topic_match_score,\n",
    "        \"claim_score\": claim_score,\n",
    "        \"future_plan_score\": future_plan_score,\n",
    "        \"evidence_score\": evidence_score,\n",
    "        \"uniqueness_score\": uniqueness_score,\n",
    "        \"importance\": importance,\n",
    "    }\n",
    "    \n",
    "    return candidate_sents_info, chunk_debug_info"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def find_summary_candidate_pipe(info, get_chunks_and_sents_and_refs_func, imp_weights, doc_source, debug=False):\n",
    "    ## get basic info\n",
    "    _year = info['year']\n",
    "    _id = info['id']\n",
    "    _name = info['name']\n",
    "    idx = (_year, _id, _name)\n",
    "    \n",
    "#     print(idx)\n",
    "    \n",
    "    ## get chunks and sents\n",
    "    chunks, sents, ref_dict = get_chunks_and_sents_and_refs_func(_year, _id)\n",
    "    ## get evidences, currently only return chunks from recommendation letter\n",
    "    evidences = get_evidences(_year, _id)\n",
    "    ## get recommendation letter uniqueness references, \n",
    "    ## i.e. sentences from data sheet and self-statement\n",
    "    rl_uniqueness_refs = get_recommendation_letter_uniqueness_ref_sents(_year, _id)\n",
    "    ## [TODO] calculate importance score for each summary\n",
    "    candidate_sents_info, chunk_debug_info = calculate_candidate_sents_score(\n",
    "        chunks, sents, evidences, rl_uniqueness_refs, topic_model, imp_weights, debug\n",
    "    )\n",
    "    candidate_sents_info['refs'] = ref_dict\n",
    "    chunk_debug_info['refs'] = ref_dict\n",
    "    \n",
    "    return candidate_sents_info, chunk_debug_info"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def merge_candidate_sents_info(old_info, new_info):\n",
    "    ## if old info is empty, return new info\n",
    "    if old_info == {} or old_info['sents'] == []:\n",
    "        return new_info\n",
    "    \n",
    "    ## if new info is empty, return old info\n",
    "    if new_info['sents'] == []:\n",
    "        return old_info\n",
    "    \n",
    "    info = {}\n",
    "    \n",
    "    info['sents'] = old_info['sents'] + new_info['sents']\n",
    "    info['sents_avg_importance_dict'] = old_info['sents_avg_importance_dict'] | new_info['sents_avg_importance_dict']\n",
    "    info['sents_topic_id_dict'] = old_info['sents_topic_id_dict'] | new_info['sents_topic_id_dict']\n",
    "    info['sents_topic_importance_dict'] = old_info['sents_topic_importance_dict'] | new_info['sents_topic_importance_dict']\n",
    "    info['refs'] = old_info['refs'] | new_info['refs']\n",
    "\n",
    "    info['topic_sent_dict'] = old_info['topic_sent_dict']\n",
    "    for topic, sents in new_info['topic_sent_dict'].items():\n",
    "        old_topic_set = old_info['topic_sent_dict'][topic]\n",
    "        new_topic_set = new_info['topic_sent_dict'][topic]\n",
    "        info['topic_sent_dict'][topic] = old_topic_set.union(new_topic_set)\n",
    "    \n",
    "    return info"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def merge_chunk_debug_info(old_info, new_info):\n",
    "    ## if old info is empty, return new info\n",
    "    if old_info == {} or old_info['chunks'] == []:\n",
    "        return new_info\n",
    "    \n",
    "    ## if new info is empty, return old info\n",
    "    if new_info['chunks'] == []:\n",
    "        return old_info\n",
    "    \n",
    "    info = {}\n",
    "    \n",
    "    info['chunks'] = old_info['chunks'] + new_info['chunks']\n",
    "    info['predicted_topics'] = np.concatenate((old_info['predicted_topics'], new_info['predicted_topics']))\n",
    "    info['predicted_knn_confs'] = np.concatenate((old_info['predicted_knn_confs'], new_info['predicted_knn_confs']))\n",
    "    info['predicted_neighbors_sc_idx'] = np.concatenate((old_info['predicted_neighbors_sc_idx'], new_info['predicted_neighbors_sc_idx']))\n",
    "    info['knn_confidence'] = np.concatenate((old_info['knn_confidence'], new_info['knn_confidence']))\n",
    "    info['topic_match_score'] = np.concatenate((old_info['topic_match_score'], new_info['topic_match_score']))\n",
    "    info['claim_score'] = np.concatenate((old_info['claim_score'], new_info['claim_score']))\n",
    "    info['future_plan_score'] = np.concatenate((old_info['future_plan_score'], new_info['future_plan_score']))\n",
    "    info['evidence_score'] = np.concatenate((old_info['evidence_score'], new_info['evidence_score']))\n",
    "    info['uniqueness_score'] = np.concatenate((old_info['uniqueness_score'], new_info['uniqueness_score']))\n",
    "    info['importance'] = np.concatenate((old_info['importance'], new_info['importance']))\n",
    "    info['refs'] = old_info['refs'] | new_info['refs']\n",
    "    \n",
    "    return info"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Find summaries sentence candidates"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Find candidates from data sheet"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "candidate_sents_info_buffer = defaultdict(dict)\n",
    "chunk_debug_info_buffer = defaultdict(dict)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_chunks_and_sents_and_refs_from_data_sheet(_year, _id):\n",
    "    row = df_achievements.query('`year` == {} and `id` == {}'.format(_year, _id))\n",
    "    \n",
    "    try:\n",
    "        chunks = row['achievement'].to_list()\n",
    "        ## [TODO] deal with nan achievement result\n",
    "        sents = [\"{}ï¼Œ{}\".format(a, r) for a, r in \n",
    "                 zip(row['achievement'].to_list(), row['achievement_result'].to_list())]\n",
    "    except:\n",
    "        chunks = []\n",
    "        sents = []\n",
    "        \n",
    "    if chunks == None:\n",
    "        chunks = []\n",
    "    if sents == None:\n",
    "        sents = []\n",
    "        \n",
    "    ref_dict = defaultdict(str)\n",
    "        \n",
    "    return chunks, sents, ref_dict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# %%time\n",
    "## claim score + future plan score\n",
    "\n",
    "IO.print_dividing_line()\n",
    "IO.print_dividing_line(\"Processing data sheet ...\")\n",
    "\n",
    "for dict_info in tqdm(tuples):\n",
    "    idx = dict_info_to_tuple_info(dict_info)\n",
    "    candidate_sents_info, chunk_debug_info = find_summary_candidate_pipe(\n",
    "        dict_info, get_chunks_and_sents_and_refs_from_data_sheet, \n",
    "        DATA_SHEET_SUMMARY_WEIGHT, 'å€‹äººè³‡æ–™è¡¨', debug=DEBUG\n",
    "    )\n",
    "\n",
    "    candidate_sents_info_buffer[idx] = merge_candidate_sents_info(candidate_sents_info_buffer[idx], candidate_sents_info)\n",
    "    chunk_debug_info_buffer[idx] = merge_chunk_debug_info(chunk_debug_info_buffer[idx], chunk_debug_info)\n",
    "\n",
    "#     IO.print_dividing_line()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Find candidates from self-statement"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_chunks_and_sents_and_refs_from_self_statement(_year, _id):\n",
    "    row = df_applications.query('`year` == {} and `id` == {}'.format(_year, _id))\n",
    "    \n",
    "    try:\n",
    "        chunks = row['self_statement_chunk'].to_list()[0]\n",
    "        sents = row['self_statement_sent'].to_list()[0]\n",
    "    except:\n",
    "        chunks = []\n",
    "        sents = []\n",
    "\n",
    "    if chunks == None:\n",
    "        chunks = []\n",
    "    if sents == None:\n",
    "        sents = []\n",
    "        \n",
    "    ref_dict = defaultdict(str)\n",
    "        \n",
    "    return chunks, sents, ref_dict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# %%time\n",
    "## claim score + future plan score\n",
    "\n",
    "IO.print_dividing_line()\n",
    "IO.print_dividing_line(\"Processing self-statement ...\")\n",
    "\n",
    "for dict_info in tqdm(tuples):\n",
    "    idx = dict_info_to_tuple_info(dict_info)\n",
    "    candidate_sents_info, chunk_debug_info = find_summary_candidate_pipe(\n",
    "        dict_info, get_chunks_and_sents_and_refs_from_self_statement, \n",
    "        SELF_STATEMENT_SUMMARY_WEIGHT, 'è‡ªå‚³', debug=DEBUG\n",
    "    )\n",
    "\n",
    "    candidate_sents_info_buffer[idx] = merge_candidate_sents_info(candidate_sents_info_buffer[idx], candidate_sents_info)\n",
    "    chunk_debug_info_buffer[idx] = merge_chunk_debug_info(chunk_debug_info_buffer[idx], chunk_debug_info)\n",
    "\n",
    "#     IO.print_dividing_line()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Find candidates from recommendation letter"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_chunks_and_sents_and_refs_from_recommendation_letter(_year, _id):\n",
    "    rows = df_recommendation_letters.query('`year` == {} and `id` == {}'.format(_year, _id))\n",
    "    \n",
    "    try:\n",
    "        rls_chunks = rows['all_paragraph_chunk'].to_list()\n",
    "        rls_sents = rows['all_paragraph_sent'].to_list()\n",
    "        rls_info = rows['info'].to_list()\n",
    "    except:\n",
    "        rls_chunks = []\n",
    "        rls_sents = []\n",
    "        rls_info = []\n",
    "        \n",
    "    if rls_chunks == None:\n",
    "        rls_chunks = []\n",
    "    if rls_sents == None:\n",
    "        rls_sents = []\n",
    "    if rls_info == None:\n",
    "        rls_info = []\n",
    "        \n",
    "        \n",
    "    chunks = []\n",
    "    sents = []\n",
    "    ref_dict = defaultdict(str)\n",
    "    ## concat several recommendation letter into one document\n",
    "    ## however, it is possible to process the recommendation letter individually\n",
    "    for rl_chunks, rl_sents, rl_info in zip(rls_chunks, rls_sents, rls_info):\n",
    "        for chunk in rl_chunks:\n",
    "            ## [TODO] replace with info string from the dataframe after improving preprocess\n",
    "            ref_dict[(\"chunk\", len(chunks))] = \"ï¼Œ\".join(rl_info) \n",
    "            chunks.append(chunk)\n",
    "        for sent in rl_sents:\n",
    "            ## [TODO] replace with info string from the dataframe after improving preprocess\n",
    "            ref_dict[(\"sent\", len(sents))] = \"ï¼Œ\".join(rl_info)\n",
    "            sents.append(sent)\n",
    "            \n",
    "    return chunks, sents, ref_dict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# %%time\n",
    "## claim score + future plan score\n",
    "\n",
    "IO.print_dividing_line()\n",
    "IO.print_dividing_line(\"Processing recommendation letter ...\")\n",
    "\n",
    "for dict_info in tqdm(tuples):\n",
    "    idx = dict_info_to_tuple_info(dict_info)\n",
    "    candidate_sents_info, chunk_debug_info = find_summary_candidate_pipe(\n",
    "        dict_info, get_chunks_and_sents_and_refs_from_recommendation_letter, \n",
    "        RECOMMENDATION_LETTER_SUMMARY_WEIGHT, 'æŽ¨è–¦ä¿¡', debug=DEBUG\n",
    "    )\n",
    "\n",
    "    candidate_sents_info_buffer[idx] = merge_candidate_sents_info(candidate_sents_info_buffer[idx], candidate_sents_info)\n",
    "    chunk_debug_info_buffer[idx] = merge_chunk_debug_info(chunk_debug_info_buffer[idx], chunk_debug_info)\n",
    "\n",
    "#     IO.print_dividing_line()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Generate pseudo summary"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## [TODO] top-k sentence selection for each perspective"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pickle"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# for _idx, info in candidate_sents_info_buffer.items():\n",
    "#     _year = _idx[0]\n",
    "#     _id = _idx[1]\n",
    "#     _name = _idx[2]\n",
    "    \n",
    "#     pseudo_summary = info['topic_sent_dict']\n",
    "    \n",
    "#     write_buffer = {\n",
    "#         \"year\": _year,\n",
    "#         \"id\": _id,\n",
    "#         \"name\": _name,\n",
    "#         \"pseudo_summary\": pseudo_summary\n",
    "#     }\n",
    "    \n",
    "#     fn = \"{}.pkl\".format(\"_\".join(map(str, _idx)))\n",
    "#     _dir = os.path.join(P.FP_SIGNIFICANCE_PSEUDO_SUMMARY_DIR, 'custom_bertopic', TRAIN_OR_ALL, str(_year))\n",
    "    \n",
    "#     if not os.path.exists(_dir):\n",
    "#         os.makedirs(_dir)\n",
    "        \n",
    "#     fp = os.path.join(_dir, fn)\n",
    "    \n",
    "#     with open(fp, \"wb\") as f:\n",
    "#         pickle.dump(write_buffer, f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## save all data\n",
    "fn = \"{}_to_{}_all_data_{:04d}_to_{:04d}.pkl\".format(START_YEAR, END_YEAR, START_IDX, END_IDX)\n",
    "_dir = os.path.join(P.FP_SIGNIFICANCE_PSEUDO_SUMMARY_DIR, 'custom_bertopic', TRAIN_OR_ALL, 'all_data')\n",
    "\n",
    "if not os.path.exists(_dir):\n",
    "    os.makedirs(_dir)\n",
    "\n",
    "all_data_fp = os.path.join(_dir, fn)\n",
    "\n",
    "with open(all_data_fp, \"wb\") as f:\n",
    "    pickle.dump({\n",
    "        \"candidate_sents_info_buffer\": candidate_sents_info_buffer,\n",
    "        \"chunk_debug_info_buffer\": chunk_debug_info_buffer,\n",
    "    }, f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"Finish generating pseudo summary from {} to {}\".format(START_IDX, END_IDX))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "research_venv",
   "language": "python",
   "name": "research_venv"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
