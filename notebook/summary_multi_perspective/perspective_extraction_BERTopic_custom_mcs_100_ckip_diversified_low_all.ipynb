{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# README\n",
    "\n",
    "### Purpose of this notebook\n",
    "- Cluster comments with `BERTopic` library, which also apply UMAP and HDBSCAN to cluster comments.\n",
    "\n",
    "### Steps\n",
    "1. Read the raw comment text.\n",
    "2. Perform BERTopic to form topics (clusters).\n",
    "3. Visualization.\n",
    "    - Intertopic distance map\n",
    "    - Cluster and scatter plot\n",
    "    - Topic hierarchy\n",
    "    - Keywords for each topic\n",
    "    - Topic similarity matrix\n",
    "    - Term score decline per Topic"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import os\n",
    "from tqdm import tqdm\n",
    "\n",
    "from importlib import reload\n",
    "\n",
    "# Utility variable\n",
    "import sys\n",
    "sys.path.insert(0, '../..')\n",
    "\n",
    "# var\n",
    "import var.var as V\n",
    "import var.path as P\n",
    "\n",
    "# utils\n",
    "import utils.articut as A\n",
    "import utils.bertopic as BT\n",
    "import utils.data as D\n",
    "import utils.io as IO\n",
    "# import utils.visualize_cluster as VC"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from bertopic import BERTopic"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Set up chinese font for matplotlib\n",
    "plt.rcParams['font.family'] = 'sans-serif'\n",
    "plt.rcParams['font.sans-serif'] = ['Noto Sans CJK TC']  \n",
    "plt.rcParams['axes.unicode_minus'] = False\n",
    "\n",
    "# Disable hugging face tokenizer parallelism\n",
    "os.environ[\"TOKENIZERS_PARALLELISM\"] = \"false\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Hyper-parameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "SBERT_MODEL_NAME = 'ckiplab/bert-base-chinese'\n",
    "BERTOPIC_MODEL_NAME = \"BERTopic_custom_mcs_100_ckip_diversified_low_all\"\n",
    "DIVERSITY = 0.3\n",
    "NR_TOPICS = None\n",
    "\n",
    "MIN_CLUSTER_SIZE = 100\n",
    "\n",
    "TRAIN_MODEL = False\n",
    "LOAD_MODEL = not TRAIN_MODEL\n",
    "\n",
    "TRAIN_OR_ALL = 'all'\n",
    "\n",
    "SPLITTER = 'ï¼„'\n",
    "\n",
    "RANDOM_STATE = 42\n",
    "np.random.seed(RANDOM_STATE)\n",
    "\n",
    "VIS_WIDTH=800\n",
    "VIS_HEIGHT=600"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Load Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.feature_extraction.text import CountVectorizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "_pass = BT._pass\n",
    "topic_doc_tokenizer = BT.topic_doc_tokenizer\n",
    "vectorizer = CountVectorizer(tokenizer=topic_doc_tokenizer, lowercase=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if LOAD_MODEL:\n",
    "    try:\n",
    "        topic_model = BERTopic.load(os.path.join(P.FP_COMMENT_CLUSTERING_MODEL_DIR, BERTOPIC_MODEL_NAME))\n",
    "        print(\"Load BERTopic model success.\")\n",
    "    except:\n",
    "        print(\"BERTopic model does not exist.\")\n",
    "        TRAIN_MODEL = True\n",
    "        LOAD_MODEL = not TRAIN_MODEL"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Read comment sentences"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_split_comments = D.read_df_split_comments_no_duplicate(TRAIN_OR_ALL)\n",
    "split_comments = D.read_split_comments_no_duplicate(TRAIN_OR_ALL)\n",
    "df_tokenization_database = df_split_comments"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "len(split_comments)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_split_comments.grade.value_counts() / len(df_split_comments) * 100"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Prepare custom models for BERTopic"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Construct a Dimension Reduction Pipeline\n",
    "- Original: High-dimensional embedding from SBERT (300+)\n",
    "- Use PCA to reduce to 50 dimensions\n",
    "- Use UMAP to reduce to 20 dimensions\n",
    "- Normalize the embeddings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.decomposition import PCA\n",
    "from umap import UMAP\n",
    "from sklearn.preprocessing import Normalizer"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### UMAP Parameters\n",
    "\n",
    "- `n_neighbors`: the size of the local neighborhood UMAP will look at when attempting to learn the manifold structure of the data.\n",
    "    - Low value: focus on the local sturcture\n",
    "    - High value: focus on the global sturcture\n",
    "- `min_dist`: the minimum distance apart that points are allowed to be in the low dimensional representation. \n",
    "    - Low value: clumpier embeddings, good for clustering\n",
    "    - High value: focus on the topological structure\n",
    "- `n_components`: Dimensions of the reduced dimension space.\n",
    "    - For visualization: 2 or 3\n",
    "    - For clustering: Larger value is acceptable (10 or 50)\n",
    "- `metric`: euclidean, minkowski, cosine, etc."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## PCA params\n",
    "PCA_COMPONENTS = 300\n",
    "## UMAP params\n",
    "UMAP_NEIGHBORS = 100\n",
    "UMAP_COMPONENTS = 50\n",
    "UMAP_MIN_DIST = 0.01\n",
    "UMAP_METRIC = 'cosine'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dimension_reduction_pipe = Pipeline([\n",
    "    ('pca', PCA(\n",
    "        n_components=PCA_COMPONENTS\n",
    "    )),\n",
    "    ('umap', UMAP(\n",
    "        n_neighbors=UMAP_NEIGHBORS,\n",
    "        n_components=UMAP_COMPONENTS,\n",
    "        min_dist=UMAP_MIN_DIST,\n",
    "        metric=UMAP_METRIC,\n",
    "        random_state=RANDOM_STATE,\n",
    "    )),\n",
    "    ('norm', Normalizer(\n",
    "        norm='l2'\n",
    "    )),\n",
    "])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Prepare HDBSCAN model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from hdbscan import HDBSCAN"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## HDBSCAN parmas\n",
    "MIN_SAMPLES = 10 # fixed\n",
    "CLUSTER_SELECTION_METHOD = 'eom' # 'eom' or 'leaf'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cluster = HDBSCAN(\n",
    "    min_samples=MIN_SAMPLES,\n",
    "    min_cluster_size=MIN_CLUSTER_SIZE,\n",
    "    cluster_selection_method=CLUSTER_SELECTION_METHOD,\n",
    "#     metric='minkowski', p=10,\n",
    "    prediction_data=True,\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Generate SBERT embeddings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sentence_transformers import SentenceTransformer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "try:\n",
    "    sentence_bert = topic_model.embedding_model.embedding_model\n",
    "except:\n",
    "    sentence_bert = SentenceTransformer(SBERT_MODEL_NAME)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "split_comments_embeds = sentence_bert.encode(split_comments, show_progress_bar=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Embedding Visualization pipeline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "visualization_pipe = Pipeline([\n",
    "    ('umap', UMAP(\n",
    "        n_neighbors=UMAP_NEIGHBORS,\n",
    "        n_components=2,\n",
    "        min_dist=UMAP_MIN_DIST,\n",
    "        metric=UMAP_METRIC,\n",
    "        random_state=RANDOM_STATE\n",
    "    )),\n",
    "])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "split_comments_plot_data = visualization_pipe.fit_transform(split_comments_embeds)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## BERTopic integration\n",
    "\n",
    "### Parameters\n",
    "- `calculate_probabilities`: Whether to calculate the probabilities of all topics per document instead of the probability of the assigned topic per document. \n",
    "\n",
    "- `diversity`: Whether to use MMR to diversify the top n words inside the topic. The value is ranged between 0 to 1.\n",
    "    - 0: not diverse\n",
    "    - 1: completely diverse"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Apply BERTopic"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if TRAIN_MODEL:\n",
    "    topic_model = BERTopic(\n",
    "    #     top_n_words=10,\n",
    "    #     n_gram_range=(1, 1),\n",
    "    #     min_topic_size=10,\n",
    "        nr_topics=NR_TOPICS,\n",
    "        calculate_probabilities=True,\n",
    "        diversity=DIVERSITY, \n",
    "    #     seed_topic_list=None,\n",
    "        embedding_model=sentence_bert,\n",
    "        umap_model=dimension_reduction_pipe,\n",
    "        hdbscan_model=cluster,\n",
    "        vectorizer_model=vectorizer,\n",
    "        verbose=True\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "%%time\n",
    "if TRAIN_MODEL:\n",
    "    topics, probs = BT.custom_fit_transform(topic_model, split_comments)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Save BERTopic model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if TRAIN_MODEL:\n",
    "    topic_model.verbose = False\n",
    "    topic_model.save(os.path.join(P.FP_COMMENT_CLUSTERING_MODEL_DIR, BERTOPIC_MODEL_NAME))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Improve Topic Representation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Get reduced embeddings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if not TRAIN_MODEL:\n",
    "    reduced_split_comments_embeds = topic_model.umap_model['umap'].embedding_\n",
    "    reduced_split_comments_embeds = topic_model.umap_model['norm'].transform(reduced_split_comments_embeds)\n",
    "    reduced_split_comments_embeds.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Get the topic labels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import hdbscan"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "if not TRAIN_MODEL:\n",
    "    _, probs = hdbscan.approximate_predict(\n",
    "        topic_model.hdbscan_model, reduced_split_comments_embeds\n",
    "    )\n",
    "    topics = topic_model.hdbscan_model.labels_\n",
    "    \n",
    "\n",
    "    topics = topic_model._map_predictions(topics)\n",
    "    probs = topic_model._map_probabilities(probs, original_topics=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# %%time\n",
    "# _ = BT.custom_update_topics(topic_model, split_comments, topics, vectorizer_model=vectorizer)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "topic_model.get_topics()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "num_topics = len(set(topics)) - 1\n",
    "num_topics"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## BERTopic Visualization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "topic_model.visualize_topics()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "topic_model.visualize_topics()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "topic_model.visualize_documents(\n",
    "    split_comments, reduced_embeddings=split_comments_plot_data,\n",
    "    width=VIS_WIDTH, height=VIS_HEIGHT\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "topic_model.visualize_documents(\n",
    "    split_comments, reduced_embeddings=split_comments_plot_data,\n",
    "    width=VIS_WIDTH, height=VIS_HEIGHT\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import scipy.cluster.hierarchy as sch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "linkage_function = lambda x: sch.linkage(x, 'ward', optimal_ordering=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "hierarchical_topics = BT.custom_hierarchical_topics(\n",
    "    topic_model, split_comments, topics, linkage_function=linkage_function\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "{0: [9, 12, 14, 19, 24, 27, 31, 32, 36, 42],\n",
    " 1: [0, 1, 3, 6, 8, 11, 13, 15, 17, 18, 22, 23, 25, 28, 33, 35, 38],\n",
    " 2: [20, 41],\n",
    " 3: [2, 4, 5, 10, 16, 26, 29, 30, 37, 39],\n",
    " 4: [7, 21, 34, 40, 43]}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "topic_model.visualize_hierarchy(\n",
    "    hierarchical_topics=hierarchical_topics, linkage_function=linkage_function,\n",
    "    width=VIS_WIDTH, height=VIS_HEIGHT, color_threshold=1.75\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "hierarchical_topics.to_csv(os.path.join(\n",
    "    P.FP_COMMENT_CLUSTERING_TOPIC_HIERARCHY_DIR, \"{}_hierarchical_topics.csv\".format(BERTOPIC_MODEL_NAME)\n",
    "))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(topic_model.get_topic_tree(hierarchical_topics))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "topic_model.visualize_hierarchical_documents(\n",
    "    split_comments, hierarchical_topics, reduced_embeddings=split_comments_plot_data,\n",
    "    width=VIS_WIDTH, height=VIS_HEIGHT\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "topic_model.visualize_barchart(\n",
    "    top_n_topics=num_topics, n_words=5, width=VIS_WIDTH, height=VIS_HEIGHT\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "topic_model.visualize_heatmap()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "topic_model.visualize_term_rank()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "%%time\n",
    "topics_per_class, _ = BT.custom_topics_per_class(\n",
    "    topic_model, split_comments, topics=topics, classes=df_split_comments['grade']\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "topics_per_class"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_split_comments.grade.value_counts() / len(df_split_comments) * 100"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "df_split_comments[df_split_comments['grade'] == \"P\"]['split_comment'].to_list()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "topics_per_class"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "topic_model.visualize_topics_per_class(\n",
    "    topics_per_class, top_n_topics=num_topics , width=VIS_WIDTH, height=VIS_HEIGHT\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Representative sentences for each topic"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "num_topics = len(topic_model.get_topic_info()) - 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for tid in range(num_topics):\n",
    "    IO.print_dividing_line(\"Topic: {}\".format(tid))\n",
    "    for rep in topic_model.get_representative_docs(tid):\n",
    "        print(rep)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for tid in range(num_topics):\n",
    "    IO.print_dividing_line(\"Topic: {}\".format(tid))\n",
    "    for rep in topic_model.get_representative_docs(tid):\n",
    "        print(rep)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Aggregate topics\n",
    "- Aggregate the topics together with hierarchical topics (down to five topics)\n",
    "- Find the mean of the aggregated topics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "MAX_NUM_PERSPECTIVE = V.MAX_NUM_PERSPECTIVE"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "hierarchical_topics.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "topic_parents_ids = set()\n",
    "topic_leaf_ids = set()\n",
    "\n",
    "for _, row in hierarchical_topics.iterrows():\n",
    "    if len(topic_leaf_ids) == MAX_NUM_PERSPECTIVE:\n",
    "        break\n",
    "        \n",
    "    parents_id = row['Parent_ID']\n",
    "    left_child_id = row['Child_Left_ID']\n",
    "    right_child_id = row['Child_Right_ID']\n",
    "    \n",
    "    topic_parents_ids.add(parents_id)\n",
    "    topic_leaf_ids.discard(parents_id)\n",
    "    \n",
    "    topic_leaf_ids.add(left_child_id)\n",
    "    topic_leaf_ids.add(right_child_id)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "topic_parents_ids"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "topic_leaf_ids"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import ast"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "hierarchical_topics.query(\"`Parent_ID` == '66'\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "topic_aggregate_dict = {}\n",
    "all_topic = set()\n",
    "\n",
    "for i, tid in enumerate(topic_leaf_ids):\n",
    "    children_topics = hierarchical_topics.query(\"`Parent_ID` == '{}'\".format(tid))['Topics'].to_list()[0]\n",
    "    if type(children_topics) == str:\n",
    "        children_topics = ast.literal_eval(children_topics)\n",
    "    \n",
    "    topic_aggregate_dict[i] = children_topics\n",
    "    \n",
    "    for ctid in children_topics:\n",
    "        all_topic.add(ctid)\n",
    "        \n",
    "len(all_topic)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# topic_aggregate_dict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "topic_aggregate_dict = {0: [9, 12, 14, 19, 24, 27, 31, 32, 36, 42],\n",
    " 1: [0, 1, 3, 6, 8, 11, 13, 15, 17, 18, 22, 23, 25, 28, 33, 35, 38],\n",
    " 2: [20, 41],\n",
    " 3: [2, 4, 5, 10, 16, 26, 29, 30, 37, 39],\n",
    " 4: [7, 21, 34, 40, 43]}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Find the mean of each condensed perspective"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "topic_aggregate_embed_mean_dict = {}\n",
    "\n",
    "for i, pids in topic_aggregate_dict.items():\n",
    "    _filter = [_idx for _idx in range(len(topics)) if topics[_idx] in pids]\n",
    "    mean_embed = np.take(split_comments_embeds, _filter, axis=0)\n",
    "    mean_embed = np.mean(mean_embed, axis=0)\n",
    "    \n",
    "    topic_aggregate_embed_mean_dict[i] = mean_embed"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "topic_aggregate_embed_mean_dict"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Compute the similarity of each condensed perspective"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics.pairwise import cosine_similarity"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "topic_aggregate_intra_similarity_dict = {}\n",
    "\n",
    "for i, pids in topic_aggregate_dict.items():\n",
    "    num_pids = len(pids)\n",
    "    _filter = [_idx for _idx in range(len(topics)) if topics[_idx] in pids]\n",
    "    embeds = np.take(split_comments_embeds, _filter, axis=0)\n",
    "    \n",
    "    num_instance = len(embeds)\n",
    "#     print(num_instance)\n",
    "    ## [TODO] calculate the diversity of the condensed perspective\n",
    "    sim_mat = cosine_similarity(embeds, embeds)\n",
    "    \n",
    "    intra_similarity = (np.sum(sim_mat) - num_instance) / 2.0 / (num_instance * (num_instance-1) / 2)\n",
    "    topic_aggregate_intra_similarity_dict[i] = intra_similarity\n",
    "    \n",
    "#     print(\"pers:\", i, \"; num pers:\", num_pids)\n",
    "#     print(\"pids: \", pids)\n",
    "#     print(num_pids)\n",
    "#     print(intra_similarity)   "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "topic_aggregate_intra_similarity_dict"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Save to pickle file"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pickle"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fn = os.path.join(\n",
    "    P.FP_COMMENT_CLUSTERING_TOPIC_HIERARCHY_DIR, \n",
    "    \"{}_topic_aggregate_info.pkl\".format(BERTOPIC_MODEL_NAME)\n",
    ")\n",
    "\n",
    "with open(fn, \"wb\") as f:\n",
    "    pickle.dump({\n",
    "        \"topic_aggregate_dict\": topic_aggregate_dict,\n",
    "        \"topic_aggregate_embed_mean_dict\": topic_aggregate_embed_mean_dict,\n",
    "        \"topic_aggregate_intra_similarity_dict\": topic_aggregate_intra_similarity_dict,\n",
    "    }, f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "research_venv",
   "language": "python",
   "name": "research_venv"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
