{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# README\n",
    "\n",
    "### Purpose of this notebook\n",
    "- Predict the topic of the application sentences with respect to the comment cluster using K-Nearest Neighbor algorithm (KNN).\n",
    "- Form the summary based on the above prediction.\n",
    "\n",
    "### Steps\n",
    "1. Get the reduced embeddings and cluster label from the comment sentences.\n",
    "2. Apply KNN algorithm to predict the label and the confidence of application sentences.\n",
    "3. Generate summary according to the results from step 2."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from itertools import chain\n",
    "import os\n",
    "\n",
    "from tqdm import tqdm\n",
    "tqdm.pandas(desc=\"progress: \")\n",
    "\n",
    "from importlib import reload\n",
    "\n",
    "# Utility variable\n",
    "import sys, getopt\n",
    "sys.path.insert(0, '../..')\n",
    "\n",
    "# var\n",
    "import var.path as P\n",
    "\n",
    "# utils\n",
    "import utils.data as D\n",
    "import utils.preprocess as PP\n",
    "import utils.io as IO"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Hyper-parameters"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Test tuple setup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# random pick test data\n",
    "df_applicants = D.read_df_applicants()\n",
    "df_applications = D.read_df_applications()\n",
    "test_df = pd.read_csv(\"112_F_experiment.csv\")\n",
    "test_df['train_or_test'] = 'true_test'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "df_applicants = pd.concat([df_applicants, test_df])\n",
    "df_applicants"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# df_applicants = test_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_applications = pd.merge(\n",
    "    df_applications, df_applicants[['year', 'id', 'name', 'train_or_test']], how='left', on=['year', 'id']\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_applications.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_applications.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_applications.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_applications['train_or_test'].value_counts()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Number of pages"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_applications[df_applications['train_or_test'] == 'train'].num_pages.describe()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_applications[df_applications['train_or_test'] == 'test'].num_pages.describe()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_applications[df_applications['train_or_test'] == 'true_test'].num_pages.describe()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Self-statement length"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def calculate_word_count(ss):\n",
    "    if not ss:\n",
    "        return 0\n",
    "    \n",
    "    if type(ss) == list:\n",
    "        ss = list(chain.from_iterable(ss))\n",
    "        ss = ''.join(ss)\n",
    "    \n",
    "    ## check the language of the document\n",
    "    zh_char_count = sum([1 for ch in ss if PP.is_zh_character(ch)])\n",
    "    zh_char_rate = zh_char_count / len(ss)\n",
    "    \n",
    "    if zh_char_rate < 0.1: ## english document preprocess\n",
    "        tokens = len(ss.split(' '))\n",
    "    else: ## chinese document preprocess\n",
    "        tokens = len(ss)\n",
    "\n",
    "    return tokens"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_applications['ss_len'] = df_applications['self_statement'].progress_apply(calculate_word_count)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_applications[df_applications['train_or_test'] == 'train'].ss_len.describe()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_applications[df_applications['train_or_test'] == 'test'].ss_len.describe()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_applications[df_applications['train_or_test'] == 'true_test'].ss_len.describe()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Recommendation letters lenght"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_recommendation_letters = D.read_df_recommendation_letters()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_chunks_and_sents_and_refs_from_recommendation_letter(row):\n",
    "    _year = row['year']\n",
    "    _id = row['id']\n",
    "    rows = df_recommendation_letters.query('`year` == {} and `id` == {}'.format(_year, _id))\n",
    "    \n",
    "    try:\n",
    "        rls_sents = rows['all_paragraph_sent'].to_list()\n",
    "    except:\n",
    "        rls_sents = []\n",
    "        \n",
    "    if rls_sents == None:\n",
    "        rls_sents = []\n",
    "        \n",
    "    sents = []\n",
    "    for rl_sents in rls_sents:\n",
    "        for sent in rl_sents:\n",
    "            sents.append(sent)\n",
    "            \n",
    "    return sents"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_applications['rl_sents'] = df_applications.apply(get_chunks_and_sents_and_refs_from_recommendation_letter, axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_applications['rl_len'] = df_applications['rl_sents'].apply(calculate_word_count)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_applications[df_applications['train_or_test'] == 'train'].rl_len.describe()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_applications[df_applications['train_or_test'] == 'test'].rl_len.describe()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_applications[df_applications['train_or_test'] == 'true_test'].rl_len.describe()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### All document length"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_applications['all_len'] = df_applications['application_pages'].progress_apply(calculate_word_count)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_applications[df_applications['train_or_test'] == 'train'].all_len.describe()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_applications[df_applications['train_or_test'] == 'test'].all_len.describe()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_applications[df_applications['train_or_test'] == 'true_test'].all_len.describe()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Comments"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# df_applications = pd.merge(\n",
    "#     df_applications, df_applicants[['year', 'id', 'name']], how='right', on=['year', 'id']\n",
    "# )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "df_applications"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "len(df_applications)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_applications.name = df_applications.name.fillna('?')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tuples = df_applications.apply(lambda row: (row['year'], row['id'], row['name']), axis=1).to_list()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tuples = [{\n",
    "    'year': info[0],\n",
    "    'id': info[1],\n",
    "    'name': info[2],\n",
    "} for info in tuples]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "len(tuples)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tuples = tuples[START_IDX:END_IDX]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "len(tuples)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def dict_info_to_tuple_info(dict_info):\n",
    "    _year = dict_info['year']\n",
    "    _id = dict_info['id']\n",
    "    _name = dict_info['name']\n",
    "    tuple_info = (_year, _id, _name)\n",
    "    \n",
    "    return tuple_info"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "DATA_SHEET_SUMMARY_WEIGHT = {\n",
    "    'knn_conf': 0,\n",
    "    'topic_match': 1,\n",
    "    'claim': 0,\n",
    "    'future_plan': 0,\n",
    "    'evidence': 2,\n",
    "    'uniqueness': 0,\n",
    "}\n",
    "\n",
    "SELF_STATEMENT_SUMMARY_WEIGHT = {\n",
    "    'knn_conf': 0,\n",
    "    'topic_match': 1,\n",
    "    'claim': 2,\n",
    "    'future_plan': 0.25,\n",
    "    'evidence': 2,\n",
    "    'uniqueness': 0,\n",
    "}\n",
    "\n",
    "RECOMMENDATION_LETTER_SUMMARY_WEIGHT = {\n",
    "    'knn_conf': 0,\n",
    "    'topic_match': 1,\n",
    "    'claim': 2,\n",
    "    'future_plan': 0,\n",
    "    'evidence': 0,\n",
    "    'uniqueness': 0,\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def defaultdict_init_defaultdict_init_by_int():\n",
    "    return defaultdict(int)\n",
    "\n",
    "def defaultdict_init_defaultdict_init_by_float():\n",
    "    return defaultdict(float)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "empty_candidate_sents_info = {\n",
    "    \"sents\": [],\n",
    "    \"sents_avg_importance_dict\": {},\n",
    "    \"sents_topic_importance_dict\": {},\n",
    "    \"sents_topic_id_dict\": {},\n",
    "    \"topic_sent_dict\": {},\n",
    "    \"refs\": {},\n",
    "}\n",
    "\n",
    "empty_chunk_debug_info = {\n",
    "    \"chunks\": [],\n",
    "    \"predicted_topics\": [],\n",
    "    \"predicted_knn_confs\": [],\n",
    "    \"predicted_neighbors_sc_idx\": [],\n",
    "    \"knn_confidence\": [],\n",
    "    \"topic_match_score\": [],\n",
    "    \"claim_score\": [],\n",
    "    \"future_plan_score\": [],\n",
    "    \"evidence_score\": [],\n",
    "    \"uniqueness_score\": [],\n",
    "    \"importance\": [],\n",
    "    \"refs\": {},\n",
    "}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Get the reduced embeddings and labels from comment sentences"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Load BERTopic Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "topic_doc_tokenizer = BT.topic_doc_tokenizer\n",
    "custom_update_topics = BT.custom_update_topics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "topic_model = BERTopic.load(\n",
    "    os.path.join(P.FP_COMMENT_CLUSTERING_MODEL_DIR, BERTOPIC_MODEL_NAME),\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sbert_model = topic_model.embedding_model.embedding_model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Fetch class label and representatives for each topic"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "topic_rep_dict = topic_model.get_representative_docs()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "topic_rep_dict[-1] = ['0', '0', '0']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "topic_rep_dict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "topic_class_label = {}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "topic_info = topic_model.get_topic_info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def extract_topic_class_label(topic_rep):\n",
    "    chunks = topic_rep.split('_')\n",
    "    tid = int(chunks[0])\n",
    "    topic_class_label[tid] = chunks[1:]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "_ = topic_info['Name'].apply(extract_topic_class_label)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "topic_class_label"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Load comment sentences"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_split_comments = D.read_df_split_comments_no_duplicate(TRAIN_OR_ALL)\n",
    "split_comments = D.read_split_comments_no_duplicate(TRAIN_OR_ALL)\n",
    "df_tokenization_database = df_split_comments"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "len(df_split_comments)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "len(split_comments)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Get reduced embeddings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "reduced_split_comments_embeds = topic_model.umap_model['umap'].embedding_\n",
    "reduced_split_comments_embeds = topic_model.umap_model['norm'].transform(reduced_split_comments_embeds)\n",
    "reduced_split_comments_embeds.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Get the topic labels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import hdbscan"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# %%time\n",
    "topic_labels = topic_model.hdbscan_model.labels_\n",
    "topic_labels = topic_model._map_predictions(topic_labels)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Get the sentiment of the comment"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# from transformers import BertForSequenceClassification\n",
    "# from transformers import BertTokenizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# from torch.utils.data import Dataset\n",
    "# from torch.utils.data import DataLoader\n",
    "# from torch import Tensor"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# sentiment_analysis_model_name = 'IDEA-CCNL/Erlangshen-Roberta-110M-Sentiment'\n",
    "\n",
    "# sentiment_analysis_tokenizer = BertTokenizer.from_pretrained(sentiment_analysis_model_name)\n",
    "# sentiment_analysis_model = BertForSequenceClassification.from_pretrained(sentiment_analysis_model_name).to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# def sentiment_analysis_inference(text):\n",
    "#     dataset = Tor.BatchSentenceDataset(text)\n",
    "#     dataloader = DataLoader(dataset, batch_size=16, shuffle=False)\n",
    "    \n",
    "#     prob_batch = []\n",
    "#     with torch.no_grad():\n",
    "#         for batch in dataloader:\n",
    "#             encoding = sentiment_analysis_tokenizer(batch, padding=True, return_tensors='pt', truncation='longest_first', max_length=510)\n",
    "\n",
    "#             for key in encoding:\n",
    "#                 if isinstance(encoding[key], Tensor):\n",
    "#                     encoding[key] = encoding[key].to(device)\n",
    "\n",
    "#             output = sentiment_analysis_model(**encoding)\n",
    "#             postive_prob = torch.nn.functional.softmax(output.logits, dim=-1)[:, 1]\n",
    "#             prob_batch.append(postive_prob)\n",
    "            \n",
    "#     postive_probs = torch.cat(prob_batch)\n",
    "#      ## -1 represent negative, 1 represent neutral or positive\n",
    "#     sentiment_label = [1 if p > 0.3 else -1 for p in postive_probs]\n",
    "    \n",
    "#     return sentiment_label"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# import utils.torch as Tor"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # %%time\n",
    "# split_comments_sentiment = sentiment_analysis_inference(split_comments)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# import pickle\n",
    "\n",
    "# with open('./train_split_comments_sentiment.pkl', 'wb') as f:\n",
    "#     pickle.dump(split_comments_sentiment, f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pickle"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "split_comments_sentiment = D.read_split_comments_sentiment(TRAIN_OR_ALL)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "len(split_comments_sentiment)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Application Inference\n",
    "Predict the topic of the application sentences with respect to the comment cluster using K-Nearest Neighbor algorithm (KNN)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Read Dataframe"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Read achievements"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_achievements = D.read_df_achievements()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_achievements.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Read self-statement"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_applications.tail()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Read recommendation-letter "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_recommendation_letters = D.read_df_recommendation_letters()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "df_recommendation_letters.tail()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Read Summaries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_summary = D.read_df_summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_summary.tail()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Summary Generation Utility"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.neighbors import KNeighborsClassifier, RadiusNeighborsClassifier, NearestCentroid"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from itertools import chain"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_evidences(_year, _id):\n",
    "    rows = df_recommendation_letters.query('`year` == {} and `id` == {}'.format(_year, _id))\n",
    "    \n",
    "    try:\n",
    "        rls_chunks = rows['all_paragraph_chunk'].to_list()\n",
    "    except:\n",
    "        rls_chunks = []\n",
    "        \n",
    "    if rls_chunks == None:\n",
    "        rls_chunks = []\n",
    "        \n",
    "    chunks = list(chain.from_iterable(rls_chunks))\n",
    "            \n",
    "    return chunks"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_recommendation_letter_uniqueness_ref_sents(_year, _id):\n",
    "    row = df_achievements.query('`year` == {} and `id` == {}'.format(_year, _id))\n",
    "    \n",
    "    try:\n",
    "        ref_sents = row['achievement'].to_list() + row['self_statement_sent'].to_list()[0]\n",
    "    except:\n",
    "        ref_sents = []\n",
    "        \n",
    "    if ref_sents == None:\n",
    "        ref_sents = []\n",
    "        \n",
    "    return ref_sents"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_topic_prediction(topic_model, chunks, n_neighbors, method=\"k\", radius=0.02):\n",
    "    if method == \"k\":\n",
    "        neigh = KNeighborsClassifier(n_neighbors=n_neighbors)\n",
    "        neigh.fit(reduced_split_comments_embeds, topic_labels)\n",
    "    elif method == \"r\":\n",
    "        neigh = KNeighborsClassifier(radius=radius, outlier_label=-1)\n",
    "        neigh.fit(reduced_split_comments_embeds, topic_labels)\n",
    "        \n",
    "    ## get reduce chunk embeddings\n",
    "    chunk_embeds = topic_model.embedding_model.embed(chunks)\n",
    "    chunk_reduced_embeds = topic_model.umap_model.transform(chunk_embeds)\n",
    "    ## predict topic and confidence\n",
    "    predicted_topics = neigh.predict(chunk_reduced_embeds)\n",
    "    predicted_confs = neigh.predict_proba(chunk_reduced_embeds)\n",
    "    predicted_neighbors_idx = neigh.kneighbors(chunk_reduced_embeds, n_neighbors=n_neighbors, return_distance=False)\n",
    "    \n",
    "    return predicted_topics, predicted_confs, predicted_neighbors_idx"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics.pairwise import cosine_similarity"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def calculate_uniqueness_score(chunks, predicted_neighbors_sc_idx):\n",
    "    chunks_embed = sbert_model.encode(chunks, batch_size=128, show_progress_bar=False)\n",
    "    scores = []\n",
    "    \n",
    "    for chunk_embed, pred_neigh_idx in zip(chunks_embed, predicted_neighbors_sc_idx):\n",
    "        ## find the outlier comments from the neighbors\n",
    "        outliers_idx = [_idx for _idx in pred_neigh_idx if topic_labels[_idx] == -1]\n",
    "        ## filter out negative comments\n",
    "        outliers_idx = [_idx for _idx in outliers_idx if split_comments_sentiment[_idx] == 1]\n",
    "        outliers = [split_comments[_idx] for _idx in outliers_idx]\n",
    "        \n",
    "        if len(outliers) == 0:\n",
    "            scores.append(0)\n",
    "            continue\n",
    "        \n",
    "        ## compute the average cosine similarity among chunk and neighbors\n",
    "        outliers_embed = sbert_model.encode(outliers, batch_size=128, show_progress_bar=False)\n",
    "        avg_sim = cosine_similarity([chunk_embed], outliers_embed).mean()\n",
    "        \n",
    "        scores.append(avg_sim)\n",
    "        \n",
    "    return scores"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def calculate_candidate_sents_score(\n",
    "    chunks,\n",
    "    sents,\n",
    "    evidences,\n",
    "    rl_uniqueness_refs,\n",
    "    topic_model,\n",
    "    imp_weights,\n",
    "    debug,\n",
    "    n_neighbors=25,\n",
    "):\n",
    "    ## Deal with empty corpus\n",
    "    if chunks == None or sents == None or len(chunks) == 0 or len(sents) == 0:\n",
    "        return empty_candidate_sents_info, empty_chunk_debug_info\n",
    "    \n",
    "    ## Predict topics on text chunks\n",
    "    predicted_topics, predicted_knn_confs, predicted_neighbors_sc_idx = get_topic_prediction(\n",
    "        topic_model, chunks, n_neighbors\n",
    "    )\n",
    "    \n",
    "    ## get topic reps for each chunks\n",
    "    topic_class_reps = [topic_rep_dict[tid] for tid in predicted_topics]\n",
    "    \n",
    "    skip_calculate_score = np.zeros(len(chunks))\n",
    "    ## Calculate chunk candidates importance\n",
    "    ## knn confidence\n",
    "    knn_confidence = np.max(predicted_knn_confs, axis=1)\n",
    "    ## topic matching score\n",
    "    if imp_weights['topic_match'] > 0:\n",
    "        topic_match_score = S.topic_match_score(chunks, topic_class_reps, batch_size=BATCH_SIZE)\n",
    "    else:\n",
    "        topic_match_score = skip_calculate_score\n",
    "    ## claim score\n",
    "    if imp_weights['claim'] > 0:\n",
    "        claim_score = np.array([S.claim_score(c, chunks, batch_size=BATCH_SIZE) for c in chunks])\n",
    "    else:\n",
    "        claim_score = skip_calculate_score\n",
    "    ## future plan score\n",
    "    if imp_weights['future_plan'] > 0:\n",
    "        future_plan_score = S.future_plan_score(chunks, batch_size=BATCH_SIZE)\n",
    "    else:\n",
    "        future_plan_score = skip_calculate_score\n",
    "    ## evidence score\n",
    "    if imp_weights['evidence'] > 0:\n",
    "        evidence_score = S.evidence_score(chunks, evidences)\n",
    "    else:\n",
    "        evidence_score = skip_calculate_score\n",
    "    ## [TODO] uniqueness score\n",
    "    if imp_weights['uniqueness'] > 0:\n",
    "        uniqueness_score = calculate_uniqueness_score(chunks, predicted_neighbors_sc_idx)\n",
    "    else:\n",
    "        uniqueness_score = skip_calculate_score\n",
    "    \n",
    "    ## [TODO] recommendation letter uniqueness score\n",
    "#     if imp_weights['rl_uniqueness'] > 0:\n",
    "#         uniqueness_score = S.uniqueness_score(chunks, rl_uniqueness_refs, predicted_topics, predicted_neighbors_sc_idx)\n",
    "#     else:\n",
    "#         uniqueness_score = skip_calculate_score\n",
    "    \n",
    "    ## importance\n",
    "    importance = (\\\n",
    "        imp_weights['knn_conf'] * knn_confidence + \\\n",
    "        imp_weights['topic_match'] * topic_match_score + \\\n",
    "        imp_weights['claim'] * (1 - claim_score) + \\\n",
    "        imp_weights['future_plan'] * (1 - future_plan_score) + \\\n",
    "        imp_weights['evidence'] * evidence_score + \\\n",
    "        imp_weights['uniqueness'] * uniqueness_score \\\n",
    "     ) / sum(imp_weights.values())\n",
    "    \n",
    "    sents_avg_importance_dict = defaultdict(list)\n",
    "    sents_topic_importance_dict = defaultdict(defaultdict_init_defaultdict_init_by_float)\n",
    "    topic_sent_dict = defaultdict(set)\n",
    "    sents_topic_id_dict = defaultdict(defaultdict_init_defaultdict_init_by_int)\n",
    "    ## Aggregate sentence importance score over chunk importance score\n",
    "    for chunk, imp, topic in zip(chunks, importance, predicted_topics):\n",
    "        ## find the sentence cotaining the chunk\n",
    "        for sent in sents:\n",
    "            ## aggregate chunk importance\n",
    "            ## [TODO] use df_comment to refine the result (because split chunk may not be exact match to sent)\n",
    "            if chunk in sent:\n",
    "                sents_avg_importance_dict[sent].append(imp)\n",
    "                sents_topic_importance_dict[sent][topic] += imp\n",
    "                topic_sent_dict[topic].add(sent)\n",
    "                sents_topic_id_dict[sent][topic] += 1\n",
    "                \n",
    "    ## Calculate the importance score of the sentence\n",
    "    sents_avg_importance_dict = {\n",
    "        sent: np.mean(imp_list) for sent, imp_list in sents_avg_importance_dict.items()\n",
    "    }\n",
    "    \n",
    "    if debug:\n",
    "        for sent, imp in sents_avg_importance_dict.items():\n",
    "            print(sent, imp, sents_topic_id_dict[sent]) \n",
    "    \n",
    "    ## remember to update the following variable and function if the key values are changed\n",
    "    ## empty_candidate_sents_info\n",
    "    ## merge_candidate_sents_info\n",
    "    candidate_sents_info = {\n",
    "        \"sents\": sents,\n",
    "        \"sents_avg_importance_dict\": sents_avg_importance_dict,\n",
    "        \"sents_topic_importance_dict\": sents_topic_importance_dict,\n",
    "        \"sents_topic_id_dict\": sents_topic_id_dict,\n",
    "        \"topic_sent_dict\": topic_sent_dict,\n",
    "    }\n",
    "    \n",
    "    ## remember to update the following variable and function if the key values are changed\n",
    "    ## empty_chunk_debug_info\n",
    "    ## merge_chunk_debug_info\n",
    "    chunk_debug_info = {\n",
    "        \"chunks\": chunks,\n",
    "        \"predicted_topics\": predicted_topics,\n",
    "        \"predicted_knn_confs\": predicted_knn_confs,\n",
    "        \"predicted_neighbors_sc_idx\": predicted_neighbors_sc_idx,\n",
    "        \"knn_confidence\": knn_confidence,\n",
    "        \"topic_match_score\": topic_match_score,\n",
    "        \"claim_score\": claim_score,\n",
    "        \"future_plan_score\": future_plan_score,\n",
    "        \"evidence_score\": evidence_score,\n",
    "        \"uniqueness_score\": uniqueness_score,\n",
    "        \"importance\": importance,\n",
    "    }\n",
    "    \n",
    "    return candidate_sents_info, chunk_debug_info"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def find_summary_candidate_pipe(info, get_chunks_and_sents_and_refs_func, imp_weights, doc_source, debug=False):\n",
    "    ## get basic info\n",
    "    _year = info['year']\n",
    "    _id = info['id']\n",
    "    _name = info['name']\n",
    "    idx = (_year, _id, _name)\n",
    "    \n",
    "#     print(idx)\n",
    "    \n",
    "    ## get chunks and sents\n",
    "    chunks, sents, ref_dict = get_chunks_and_sents_and_refs_func(_year, _id)\n",
    "    ## get evidences, currently only return chunks from recommendation letter\n",
    "    evidences = get_evidences(_year, _id)\n",
    "    ## get recommendation letter uniqueness references, \n",
    "    ## i.e. sentences from data sheet and self-statement\n",
    "    rl_uniqueness_refs = get_recommendation_letter_uniqueness_ref_sents(_year, _id)\n",
    "    ## [TODO] calculate importance score for each summary\n",
    "    candidate_sents_info, chunk_debug_info = calculate_candidate_sents_score(\n",
    "        chunks, sents, evidences, rl_uniqueness_refs, topic_model, imp_weights, debug\n",
    "    )\n",
    "    candidate_sents_info['refs'] = ref_dict\n",
    "    chunk_debug_info['refs'] = ref_dict\n",
    "    \n",
    "    return candidate_sents_info, chunk_debug_info"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def merge_candidate_sents_info(old_info, new_info):\n",
    "    ## if old info is empty, return new info\n",
    "    if old_info == {} or old_info['sents'] == []:\n",
    "        return new_info\n",
    "    \n",
    "    ## if new info is empty, return old info\n",
    "    if new_info['sents'] == []:\n",
    "        return old_info\n",
    "    \n",
    "    info = {}\n",
    "    \n",
    "    info['sents'] = old_info['sents'] + new_info['sents']\n",
    "    info['sents_avg_importance_dict'] = old_info['sents_avg_importance_dict'] | new_info['sents_avg_importance_dict']\n",
    "    info['sents_topic_id_dict'] = old_info['sents_topic_id_dict'] | new_info['sents_topic_id_dict']\n",
    "    info['sents_topic_importance_dict'] = old_info['sents_topic_importance_dict'] | new_info['sents_topic_importance_dict']\n",
    "    info['refs'] = old_info['refs'] | new_info['refs']\n",
    "\n",
    "    info['topic_sent_dict'] = old_info['topic_sent_dict']\n",
    "    for topic, sents in new_info['topic_sent_dict'].items():\n",
    "        old_topic_set = old_info['topic_sent_dict'][topic]\n",
    "        new_topic_set = new_info['topic_sent_dict'][topic]\n",
    "        info['topic_sent_dict'][topic] = old_topic_set.union(new_topic_set)\n",
    "    \n",
    "    return info"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def merge_chunk_debug_info(old_info, new_info):\n",
    "    ## if old info is empty, return new info\n",
    "    if old_info == {} or old_info['chunks'] == []:\n",
    "        return new_info\n",
    "    \n",
    "    ## if new info is empty, return old info\n",
    "    if new_info['chunks'] == []:\n",
    "        return old_info\n",
    "    \n",
    "    info = {}\n",
    "    \n",
    "    info['chunks'] = old_info['chunks'] + new_info['chunks']\n",
    "    info['predicted_topics'] = np.concatenate((old_info['predicted_topics'], new_info['predicted_topics']))\n",
    "    info['predicted_knn_confs'] = np.concatenate((old_info['predicted_knn_confs'], new_info['predicted_knn_confs']))\n",
    "    info['predicted_neighbors_sc_idx'] = np.concatenate((old_info['predicted_neighbors_sc_idx'], new_info['predicted_neighbors_sc_idx']))\n",
    "    info['knn_confidence'] = np.concatenate((old_info['knn_confidence'], new_info['knn_confidence']))\n",
    "    info['topic_match_score'] = np.concatenate((old_info['topic_match_score'], new_info['topic_match_score']))\n",
    "    info['claim_score'] = np.concatenate((old_info['claim_score'], new_info['claim_score']))\n",
    "    info['future_plan_score'] = np.concatenate((old_info['future_plan_score'], new_info['future_plan_score']))\n",
    "    info['evidence_score'] = np.concatenate((old_info['evidence_score'], new_info['evidence_score']))\n",
    "    info['uniqueness_score'] = np.concatenate((old_info['uniqueness_score'], new_info['uniqueness_score']))\n",
    "    info['importance'] = np.concatenate((old_info['importance'], new_info['importance']))\n",
    "    info['refs'] = old_info['refs'] | new_info['refs']\n",
    "    \n",
    "    return info"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Find summaries sentence candidates"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Find candidates from data sheet"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "candidate_sents_info_buffer = defaultdict(dict)\n",
    "chunk_debug_info_buffer = defaultdict(dict)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_chunks_and_sents_and_refs_from_data_sheet(_year, _id):\n",
    "    row = df_achievements.query('`year` == {} and `id` == {}'.format(_year, _id))\n",
    "    \n",
    "    try:\n",
    "        chunks = row['achievement'].to_list()\n",
    "        ## [TODO] deal with nan achievement result\n",
    "        sents = [\"{}，{}\".format(a, r) for a, r in \n",
    "                 zip(row['achievement'].to_list(), row['achievement_result'].to_list())]\n",
    "    except:\n",
    "        chunks = []\n",
    "        sents = []\n",
    "        \n",
    "    if chunks == None:\n",
    "        chunks = []\n",
    "    if sents == None:\n",
    "        sents = []\n",
    "        \n",
    "    ref_dict = defaultdict(str)\n",
    "        \n",
    "    return chunks, sents, ref_dict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# %%time\n",
    "## claim score + future plan score\n",
    "\n",
    "IO.print_dividing_line()\n",
    "IO.print_dividing_line(\"Processing data sheet ...\")\n",
    "\n",
    "for dict_info in tqdm(tuples):\n",
    "    idx = dict_info_to_tuple_info(dict_info)\n",
    "    candidate_sents_info, chunk_debug_info = find_summary_candidate_pipe(\n",
    "        dict_info, get_chunks_and_sents_and_refs_from_data_sheet, \n",
    "        DATA_SHEET_SUMMARY_WEIGHT, '個人資料表', debug=DEBUG\n",
    "    )\n",
    "\n",
    "    candidate_sents_info_buffer[idx] = merge_candidate_sents_info(candidate_sents_info_buffer[idx], candidate_sents_info)\n",
    "    chunk_debug_info_buffer[idx] = merge_chunk_debug_info(chunk_debug_info_buffer[idx], chunk_debug_info)\n",
    "\n",
    "#     IO.print_dividing_line()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Find candidates from self-statement"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_chunks_and_sents_and_refs_from_self_statement(_year, _id):\n",
    "    row = df_applications.query('`year` == {} and `id` == {}'.format(_year, _id))\n",
    "    \n",
    "    try:\n",
    "        chunks = row['self_statement_chunk'].to_list()[0]\n",
    "        sents = row['self_statement_sent'].to_list()[0]\n",
    "    except:\n",
    "        chunks = []\n",
    "        sents = []\n",
    "\n",
    "    if chunks == None:\n",
    "        chunks = []\n",
    "    if sents == None:\n",
    "        sents = []\n",
    "        \n",
    "    ref_dict = defaultdict(str)\n",
    "        \n",
    "    return chunks, sents, ref_dict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# %%time\n",
    "## claim score + future plan score\n",
    "\n",
    "IO.print_dividing_line()\n",
    "IO.print_dividing_line(\"Processing self-statement ...\")\n",
    "\n",
    "for dict_info in tqdm(tuples):\n",
    "    idx = dict_info_to_tuple_info(dict_info)\n",
    "    candidate_sents_info, chunk_debug_info = find_summary_candidate_pipe(\n",
    "        dict_info, get_chunks_and_sents_and_refs_from_self_statement, \n",
    "        SELF_STATEMENT_SUMMARY_WEIGHT, '自傳', debug=DEBUG\n",
    "    )\n",
    "\n",
    "    candidate_sents_info_buffer[idx] = merge_candidate_sents_info(candidate_sents_info_buffer[idx], candidate_sents_info)\n",
    "    chunk_debug_info_buffer[idx] = merge_chunk_debug_info(chunk_debug_info_buffer[idx], chunk_debug_info)\n",
    "\n",
    "#     IO.print_dividing_line()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Find candidates from recommendation letter"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_chunks_and_sents_and_refs_from_recommendation_letter(_year, _id):\n",
    "    rows = df_recommendation_letters.query('`year` == {} and `id` == {}'.format(_year, _id))\n",
    "    \n",
    "    try:\n",
    "        rls_chunks = rows['all_paragraph_chunk'].to_list()\n",
    "        rls_sents = rows['all_paragraph_sent'].to_list()\n",
    "        rls_info = rows['info'].to_list()\n",
    "    except:\n",
    "        rls_chunks = []\n",
    "        rls_sents = []\n",
    "        rls_info = []\n",
    "        \n",
    "    if rls_chunks == None:\n",
    "        rls_chunks = []\n",
    "    if rls_sents == None:\n",
    "        rls_sents = []\n",
    "    if rls_info == None:\n",
    "        rls_info = []\n",
    "        \n",
    "        \n",
    "    chunks = []\n",
    "    sents = []\n",
    "    ref_dict = defaultdict(str)\n",
    "    ## concat several recommendation letter into one document\n",
    "    ## however, it is possible to process the recommendation letter individually\n",
    "    for rl_chunks, rl_sents, rl_info in zip(rls_chunks, rls_sents, rls_info):\n",
    "        for chunk in rl_chunks:\n",
    "            ## [TODO] replace with info string from the dataframe after improving preprocess\n",
    "            ref_dict[(\"chunk\", len(chunks))] = \"，\".join(rl_info) \n",
    "            chunks.append(chunk)\n",
    "        for sent in rl_sents:\n",
    "            ## [TODO] replace with info string from the dataframe after improving preprocess\n",
    "            ref_dict[(\"sent\", len(sents))] = \"，\".join(rl_info)\n",
    "            sents.append(sent)\n",
    "            \n",
    "    return chunks, sents, ref_dict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# %%time\n",
    "## claim score + future plan score\n",
    "\n",
    "IO.print_dividing_line()\n",
    "IO.print_dividing_line(\"Processing recommendation letter ...\")\n",
    "\n",
    "for dict_info in tqdm(tuples):\n",
    "    idx = dict_info_to_tuple_info(dict_info)\n",
    "    candidate_sents_info, chunk_debug_info = find_summary_candidate_pipe(\n",
    "        dict_info, get_chunks_and_sents_and_refs_from_recommendation_letter, \n",
    "        RECOMMENDATION_LETTER_SUMMARY_WEIGHT, '推薦信', debug=DEBUG\n",
    "    )\n",
    "\n",
    "    candidate_sents_info_buffer[idx] = merge_candidate_sents_info(candidate_sents_info_buffer[idx], candidate_sents_info)\n",
    "    chunk_debug_info_buffer[idx] = merge_chunk_debug_info(chunk_debug_info_buffer[idx], chunk_debug_info)\n",
    "\n",
    "#     IO.print_dividing_line()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Generate pseudo summary"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## [TODO] top-k sentence selection for each perspective"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pickle"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## save all data\n",
    "fn = \"112_experiment_all_data.pkl\"\n",
    "_dir = os.path.join(P.FP_SIGNIFICANCE_PSEUDO_SUMMARY_DIR, 'custom_bertopic', TRAIN_OR_ALL, 'all_data')\n",
    "\n",
    "if not os.path.exists(_dir):\n",
    "    os.makedirs(_dir)\n",
    "\n",
    "all_data_fp = os.path.join(_dir, fn)\n",
    "\n",
    "with open(all_data_fp, \"wb\") as f:\n",
    "    pickle.dump({\n",
    "        \"candidate_sents_info_buffer\": candidate_sents_info_buffer,\n",
    "        \"chunk_debug_info_buffer\": chunk_debug_info_buffer,\n",
    "    }, f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"Finish generating pseudo summary from {} to {}\".format(START_IDX, END_IDX))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "research_venv",
   "language": "python",
   "name": "research_venv"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
