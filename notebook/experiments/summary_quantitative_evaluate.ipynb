{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import os\n",
    "import torch\n",
    "from collections import defaultdict\n",
    "from itertools import combinations, chain\n",
    "import pickle\n",
    "import copy\n",
    "from docx import Document\n",
    "from zhon import hanzi\n",
    "\n",
    "import re\n",
    "from zhon import hanzi\n",
    "\n",
    "from tqdm import tqdm\n",
    "tqdm.pandas(desc=\"progress: \")\n",
    "\n",
    "from importlib import reload\n",
    "\n",
    "# Utility variable\n",
    "import sys, getopt\n",
    "sys.path.insert(0, '../..')\n",
    "\n",
    "# var\n",
    "import var.var as V\n",
    "import var.path as P\n",
    "\n",
    "# utils\n",
    "import utils.data as D\n",
    "import utils.io as IO\n",
    "import utils.preprocess as PP"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from torch import Tensor"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Process Command Line Arguments"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "opts, args = getopt.getopt(sys.argv[1:], \"d:e:m:l:s:f:t\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "SIGNIFICANCE_MODEL_SAVE_DIR_NAME = \"significance_pHAN_cmt_cos_dist_wo_cmt_aug_all_2022-12-12_mixed_3\"\n",
    "EPOCH = 20\n",
    "MAX_SENT = 8\n",
    "UNI_MAX_SENT = 2\n",
    "MAX_SENT_LEN = 60\n",
    "LAMBDA = 0.3\n",
    "NORM_RATIO = 4\n",
    "VAL_OR_TEST = 'test'\n",
    "debug = False\n",
    "EVIDENCE_SCORE_THRESHOLD = 0.75\n",
    "\n",
    "for opt, arg in opts:\n",
    "    if opt == '-d':\n",
    "        SIGNIFICANCE_MODEL_SAVE_DIR_NAME = arg\n",
    "    elif opt == '-e':\n",
    "        EPOCH = int(arg)\n",
    "    elif opt == '-s':\n",
    "        MAX_SENT = int(arg)\n",
    "    elif opt == '-m':\n",
    "        MAX_SENT_LEN = int(arg)\n",
    "    elif opt == '-l':\n",
    "        LAMBDA = float(arg)\n",
    "    elif opt == '-t':\n",
    "        VAL_OR_TEST = 'test'\n",
    "    elif opt == '-f':\n",
    "        debug = True"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "if 'train' in SIGNIFICANCE_MODEL_SAVE_DIR_NAME:\n",
    "    TRAIN_OR_ALL = 'train'\n",
    "elif 'all' in SIGNIFICANCE_MODEL_SAVE_DIR_NAME:\n",
    "    TRAIN_OR_ALL = 'all'\n",
    "    \n",
    "if 'wo' in SIGNIFICANCE_MODEL_SAVE_DIR_NAME:\n",
    "    COMMENT_AUGMENTATION = False\n",
    "else:\n",
    "    COMMENT_AUGMENTATION = True"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### BERT NSP Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "GPU_NUM = 0\n",
    "device = torch.device(GPU_NUM)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "BERT_NSP_MODEL_NAME = 'bert-base-chinese'\n",
    "BERT_NSP_TOKENIZER_NAME = 'bert-base-chinese'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import BertTokenizerFast, BertForNextSentencePrediction"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of the model checkpoint at bert-base-chinese were not used when initializing BertForNextSentencePrediction: ['cls.predictions.transform.dense.weight', 'cls.predictions.decoder.weight', 'cls.predictions.bias', 'cls.predictions.transform.LayerNorm.bias', 'cls.predictions.transform.LayerNorm.weight', 'cls.predictions.transform.dense.bias']\n",
      "- This IS expected if you are initializing BertForNextSentencePrediction from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing BertForNextSentencePrediction from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n"
     ]
    }
   ],
   "source": [
    "bert_nsp_tokenizer = BertTokenizerFast.from_pretrained(BERT_NSP_TOKENIZER_NAME)\n",
    "bert_nsp_model = BertForNextSentencePrediction.from_pretrained(BERT_NSP_MODEL_NAME).to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "SBERT_MODEL_NAME = 'ckiplab/bert-base-chinese'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/pclightyear/research_venv/lib/python3.10/site-packages/huggingface_hub/snapshot_download.py:6: FutureWarning: snapshot_download.py has been made private and will no longer be available from version 0.11. Please use `from huggingface_hub import snapshot_download` to import the only public function in this module. Other members of the file may be changed without a deprecation notice.\n",
      "  warnings.warn(\n",
      "/home/pclightyear/research_venv/lib/python3.10/site-packages/huggingface_hub/file_download.py:588: FutureWarning: `cached_download` is the legacy way to download files from the HF hub, please consider upgrading to `hf_hub_download`\n",
      "  warnings.warn(\n",
      "WARNING:root:No sentence-transformers model found with name /home/pclightyear/.cache/torch/sentence_transformers/ckiplab_bert-base-chinese. Creating a new one with MEAN pooling.\n",
      "Some weights of the model checkpoint at /home/pclightyear/.cache/torch/sentence_transformers/ckiplab_bert-base-chinese were not used when initializing BertModel: ['cls.predictions.transform.dense.weight', 'cls.predictions.decoder.weight', 'cls.predictions.bias', 'cls.predictions.decoder.bias', 'cls.predictions.transform.LayerNorm.bias', 'cls.predictions.transform.LayerNorm.weight', 'cls.predictions.transform.dense.bias']\n",
      "- This IS expected if you are initializing BertModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing BertModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
      "Some weights of BertModel were not initialized from the model checkpoint at /home/pclightyear/.cache/torch/sentence_transformers/ckiplab_bert-base-chinese and are newly initialized: ['bert.pooler.dense.bias', 'bert.pooler.dense.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    }
   ],
   "source": [
    "from sentence_transformers import SentenceTransformer\n",
    "\n",
    "sbert_model = SentenceTransformer(SBERT_MODEL_NAME).to(device)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Utils"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "def defaultdict_init_defaultdict_init_by_int():\n",
    "    return defaultdict(int)\n",
    "\n",
    "def defaultdict_init_defaultdict_init_by_float():\n",
    "    return defaultdict(float)\n",
    "\n",
    "def defaultdict_init_defaultdict_init_by_str():\n",
    "    return defaultdict(str)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Build the reference citation dictionary"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_recommendation_letters = D.read_df_recommendation_letters()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "df_recommendation_letters.tail()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "rl_info_dict = defaultdict(defaultdict_init_defaultdict_init_by_str)\n",
    "\n",
    "for _, row in df_recommendation_letters.iterrows():\n",
    "    _year = int(row['year'])\n",
    "    _id = int(row['id'])\n",
    "\n",
    "    rl_sent = row['all_paragraph_sent']\n",
    "    info = \"，\".join([info for info in row['info'] if len(info) <= 10])\n",
    "    \n",
    "    if info == \"\":\n",
    "        continue\n",
    "        \n",
    "    sent_info_dict = defaultdict_init_defaultdict_init_by_str()\n",
    "    \n",
    "    for sent in rl_sent:\n",
    "        sent_info_dict[sent] = info\n",
    "        \n",
    "    rl_info_dict[(_year, _id)] = rl_info_dict[(_year, _id)] | sent_info_dict    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Calculate the evidence score for each sentence"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "significance_pseudo_summary_dir = os.path.join(P.FP_SIGNIFICANCE_PSEUDO_SUMMARY_DIR, 'custom_bertopic', TRAIN_OR_ALL)\n",
    "significance_all_data_dir = os.path.join(significance_pseudo_summary_dir, 'all_data')\n",
    "\n",
    "uniqueness_pseudo_summary_dir = os.path.join(P.FP_UNIQUENESS_PSEUDO_SUMMARY_DIR, 'custom_bertopic', TRAIN_OR_ALL)\n",
    "uniqueness_all_data_dir = os.path.join(uniqueness_pseudo_summary_dir, 'all_data')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "i = 0\n",
    "sent_evidence_score_dict = defaultdict(defaultdict_init_defaultdict_init_by_float)\n",
    "\n",
    "for file in tqdm(os.listdir(significance_all_data_dir)):\n",
    "    fn = os.path.join(significance_all_data_dir, file)\n",
    "    \n",
    "#     IO.print_dividing_line()\n",
    "#     if i >= 1:\n",
    "#         break\n",
    "    \n",
    "    if os.path.isdir(fn):\n",
    "        continue\n",
    "\n",
    "#     print(fn)\n",
    "        \n",
    "    with open(fn, \"rb\") as f:\n",
    "        group_data = pickle.load(f)\n",
    "\n",
    "    ## process group data\n",
    "    candidate_sents_info_buffer = group_data[\"candidate_sents_info_buffer\"]\n",
    "    chunk_debug_info_buffer = group_data[\"chunk_debug_info_buffer\"]\n",
    "    \n",
    "#     print(candidate_sents_info_buffer)\n",
    "#     print(chunk_debug_info_buffer)\n",
    "\n",
    "    for info, debug_info in chunk_debug_info_buffer.items():\n",
    "        buffer_dict = defaultdict_init_defaultdict_init_by_float()\n",
    "        \n",
    "#         print(info, debug_info)\n",
    "        sents = candidate_sents_info_buffer[info]['sents']\n",
    "        chunks = debug_info['chunks']\n",
    "        chunk_evidence_scores = debug_info['evidence_score']\n",
    "    \n",
    "#         print(info)\n",
    "#         print(sents)\n",
    "#         print(chunks)\n",
    "#         print(chunk_evidence_scores)\n",
    "        \n",
    "        for chunk, chunk_evidence_score in zip(chunks, chunk_evidence_scores):\n",
    "            # find corresponding sentences\n",
    "            for sent in sents:\n",
    "                ## aggregate sent evidence score\n",
    "                if chunk in sent:\n",
    "                    buffer_dict[sent] = max(\n",
    "                        buffer_dict[sent], chunk_evidence_score\n",
    "                    )\n",
    "                    \n",
    "        sent_evidence_score_dict[info] = buffer_dict\n",
    "    \n",
    "    i += 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "len(sent_evidence_score_dict)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## All data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "all_candidate_sents_info_buffer = {}\n",
    "all_chunk_debug_info_buffer = {}\n",
    "\n",
    "for file in tqdm(os.listdir(significance_all_data_dir)):\n",
    "    fn = os.path.join(significance_all_data_dir, file)\n",
    "    \n",
    "    if os.path.isdir(fn):\n",
    "        continue\n",
    "        \n",
    "    with open(fn, \"rb\") as f:\n",
    "        group_data = pickle.load(f)\n",
    "        \n",
    "    candidate_sents_info_buffer = group_data[\"candidate_sents_info_buffer\"]\n",
    "    chunk_debug_info_buffer = group_data[\"chunk_debug_info_buffer\"]\n",
    "    \n",
    "    all_candidate_sents_info_buffer |= candidate_sents_info_buffer\n",
    "    all_chunk_debug_info_buffer |= chunk_debug_info_buffer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "len(all_candidate_sents_info_buffer)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "len(all_chunk_debug_info_buffer)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Long sentence post process utils"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sentence_transformers.util import cos_sim"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_sent_len(s):\n",
    "    re_alphanumeric = '[a-zA-Z0-9_]+'\n",
    "    re_ch_p = '[{}]'.format(hanzi.characters + hanzi.punctuation)\n",
    "    \n",
    "    l = 0\n",
    "    \n",
    "    ## find all english and number token\n",
    "    l += len(re.findall(re_alphanumeric, s))\n",
    "    s = re.sub(re_alphanumeric, '', s)\n",
    "    \n",
    "    ## remove whitespace\n",
    "    s = re.sub('\\s', '', s)\n",
    "    \n",
    "    ## count chinese character\n",
    "    l += len(re.findall(re_ch_p, s))\n",
    "    \n",
    "    return l"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "def find_longest_consecutive_sequence(seq):\n",
    "    try:\n",
    "        assert len(seq) >= 1\n",
    "    except:\n",
    "        print(seq)\n",
    "        assert 1 == 0\n",
    "    \n",
    "    cand_seqs = []\n",
    "    \n",
    "    if len(seq) == 1:\n",
    "        return seq\n",
    "\n",
    "    cur_num = seq[0]\n",
    "    seq_buf = seq[:1]\n",
    "\n",
    "    for num in seq[1:]:\n",
    "        if num == cur_num + 1:\n",
    "            seq_buf.append(num)\n",
    "        else:\n",
    "            cand_seqs.append(seq_buf)\n",
    "            seq_buf = [num]\n",
    "\n",
    "        cur_num = num\n",
    "\n",
    "    cand_seqs.append(seq_buf)\n",
    "    sorted_cand_seqs = sorted(cand_seqs, key=lambda l: -len(l))\n",
    "    \n",
    "    return sorted_cand_seqs[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "def is_reasonable_sent(chunks, tokenizer, model, debug=False):\n",
    "    ## get all sublist with length >= 2\n",
    "    sublist_idx = []\n",
    "    for start_idx, end_idx in combinations(range(len(chunks)+1), 2):\n",
    "        if end_idx - start_idx > 1:\n",
    "            sublist_idx.append((start_idx, end_idx))\n",
    "    \n",
    "    ## split sublist into two list with len >= 1\n",
    "    text_pair = []\n",
    "    for start_idx, end_idx in sublist_idx:\n",
    "        sublist = chunks[start_idx:end_idx]\n",
    "        for pivot in range(1, len(sublist)):\n",
    "            l_sublist = sublist[0:pivot]\n",
    "            r_sublist = sublist[pivot:len(sublist)]\n",
    "            text_pair.append(('，'.join(l_sublist)+'。', '，'.join(r_sublist)+'。'))\n",
    "    \n",
    "    inputs = tokenizer(text_pair, return_tensors='pt', padding=True)\n",
    "\n",
    "    for key in inputs:\n",
    "        if isinstance(inputs[key], Tensor):\n",
    "            inputs[key] = inputs[key].to(device)\n",
    "    \n",
    "    with torch.no_grad():\n",
    "        outputs = model(**inputs)\n",
    "        results = torch.argmax(outputs.logits, dim=-1)\n",
    "        \n",
    "    if debug:\n",
    "        print(outputs)\n",
    "        for p in text_pair:\n",
    "            print(p)\n",
    "        print(results)\n",
    "    \n",
    "    return bool(sum(results) == 0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'＂＃＄％＆＇（）＊＋，－／：；＜＝＞＠［＼］＾＿｀｛｜｝～｟｠｢｣､\\u3000、〃〈〉《》「」『』【】〔〕〖〗〘〙〚〛〜〝〞〟〰〾〿–—‘’‛“”„‟…‧﹏﹑﹔·！？｡。'"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "hanzi.punctuation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "split_punc = '\\n＂＃＄％＆＇＊＋，－／：；＜＝＞＠＼＾＿｀｜～､\\u3000、〃〜〝〞〟〰〾〿–—‘’‛“”„‟…‧﹏﹑﹔·！？｡。'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "parenthesis_punc = '＂（）［］｛｝｟｠｢｣〈〉《》「」『』【】〔〕〖〗〘〙〚〛'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "def long_sentence_post_process(s, debug_info, ext_summary, debug=False):\n",
    "    ## get debug chunk info\n",
    "    debug_chunks_idx = []\n",
    "    for i, chunk in enumerate(debug_info['chunks']):\n",
    "        if chunk in s:\n",
    "            debug_chunks_idx.append(i)\n",
    "\n",
    "    if len(debug_chunks_idx) == 0:\n",
    "        return \"\"\n",
    "            \n",
    "    ## find the longest consecutive sequence\n",
    "    debug_chunks_idx = find_longest_consecutive_sequence(debug_chunks_idx)\n",
    "    \n",
    "    if debug:\n",
    "        for _idx in debug_chunks_idx:\n",
    "            print(debug_info['chunks'][_idx])\n",
    "    \n",
    "    ## directly split s to get all chunks\n",
    "    cand_chunks = re.split('[{}]'.format(split_punc), s)\n",
    "    cand_chunks = [c for c in cand_chunks if c]\n",
    "    \n",
    "    ## compute the len of each chunk\n",
    "    chunks_len = [get_sent_len(cc) for cc in cand_chunks]\n",
    "    ## brutal force to find text span that is below length limit\n",
    "    span_tuple = combinations(range(len(cand_chunks)+1), 2)\n",
    "    f_span_tuple = []\n",
    "\n",
    "    if debug:\n",
    "        print(\"cand chunks:\", cand_chunks)\n",
    "    \n",
    "    for start_idx, end_idx in span_tuple:\n",
    "        spans = cand_chunks[start_idx:end_idx]\n",
    "    #     print(spans)\n",
    "        ## check if satisfy length limit\n",
    "        chunk_len = sum(chunks_len[start_idx:end_idx]) + end_idx - start_idx - 1\n",
    "        if chunk_len <= MAX_SENT_LEN:\n",
    "            f_span_tuple.append((start_idx, end_idx))\n",
    "    \n",
    "    if debug:\n",
    "        print(\"span tuples:\", f_span_tuple)\n",
    "    \n",
    "    ## remove text span that is the subset of other text span\n",
    "    filtered_span_tuple = []\n",
    "    for i, (start_idx, end_idx) in enumerate(f_span_tuple):\n",
    "        spans = cand_chunks[start_idx:end_idx]\n",
    "        unique = True\n",
    "        for j, (_start_idx, _end_idx) in enumerate(f_span_tuple):\n",
    "            if i == j:\n",
    "                continue\n",
    "            if _start_idx <= start_idx and end_idx <= _end_idx:\n",
    "                unique = False\n",
    "\n",
    "        if unique:\n",
    "#             print('，'.join(spans))\n",
    "            filtered_span_tuple.append((start_idx, end_idx))\n",
    "\n",
    "    if debug:\n",
    "        print(\"spans after brutal force search\")\n",
    "        print(filtered_span_tuple)\n",
    "    \n",
    "    ## remove unreasonable sentence\n",
    "    buf = []\n",
    "    for start_idx, end_idx in filtered_span_tuple:\n",
    "        chunks = cand_chunks[start_idx:end_idx]\n",
    "        if end_idx - start_idx == 1:\n",
    "            buf.append((start_idx, end_idx))\n",
    "        elif is_reasonable_sent(chunks, bert_nsp_tokenizer, bert_nsp_model):\n",
    "            buf.append((start_idx, end_idx))\n",
    "            \n",
    "    filtered_span_tuple = buf\n",
    "\n",
    "    if debug:\n",
    "        print(\"spans after nsp filter\")\n",
    "        print(filtered_span_tuple)\n",
    "    \n",
    "    if len(filtered_span_tuple) == 0:\n",
    "        return ''\n",
    "    \n",
    "    cand_score = []\n",
    "    ## calculate final score\n",
    "    for start_idx, end_idx in filtered_span_tuple:\n",
    "        cc = cand_chunks[start_idx:end_idx]\n",
    "        if debug:\n",
    "#             print(cc)\n",
    "            pass\n",
    "        \n",
    "        ## aggregate imp score from real chunks\n",
    "        ## should contain inportant info (high importance)\n",
    "        cand_importance = 0\n",
    "        for _c in cc:\n",
    "            for _idx in debug_chunks_idx:\n",
    "                if _c in debug_info['chunks'][_idx]:\n",
    "                    cand_importance += debug_info['importance'][_idx]\n",
    "                    break\n",
    "        \n",
    "        cand_importance /= len(cc)\n",
    "        \n",
    "#         ## info should be novel (dissimilar with existing summary)\n",
    "        cc_embed = sbert_model.encode('，'.join(cc), show_progress_bar=False)\n",
    "        ext_summary_embed = sbert_model.encode(ext_summary, show_progress_bar=False)\n",
    "        cand_novel = 1 - cos_sim(cc_embed, ext_summary_embed)[0][0]\n",
    "        \n",
    "        score = LAMBDA * cand_importance + (1 - LAMBDA) * cand_novel * NORM_RATIO\n",
    "        cand_score.append(score)\n",
    "        if debug:\n",
    "            print('imp', float(cand_importance))\n",
    "            print('novel', float(cand_novel))\n",
    "            print(score)\n",
    "        \n",
    "    final_span_idx = np.argmax(cand_score)\n",
    "    final_start_idx, final_end_idx = filtered_span_tuple[final_span_idx]\n",
    "    final_chunks = cand_chunks[final_start_idx:final_end_idx]\n",
    "    final_sent = '，'.join(final_chunks) + '。'\n",
    "    \n",
    "    return final_sent"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "import string\n",
    "from zhon import hanzi"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "def sentence_post_process(s):\n",
    "    s = s.strip()\n",
    "    try:\n",
    "        ## remove trailing number\n",
    "        while s[0] in string.punctuation:\n",
    "            s = s[1:]\n",
    "    except:\n",
    "        return ''\n",
    "    s = s.strip()\n",
    "    \n",
    "    ## remove mojibake\n",
    "    s = s.replace('\\uf06c', '')\n",
    "    s = s.replace('。，', '，')\n",
    "    s = s.replace('，。', '，')\n",
    "    s = s.replace('，nan', '')\n",
    "    s = s.replace('&lt；', '')\n",
    "    \n",
    "    ## remove zh number bullet\n",
    "    ch_number = \"一二三四五六七八九十\"\n",
    "    p = '[{}\\d]、'.format(ch_number)\n",
    "    s = re.sub(p, '', s)\n",
    "    \n",
    "    ## remove number bullet\n",
    "    p = '((?<!\\d)\\d+\\.(?!\\d)|★)'\n",
    "    s = re.sub(p, '',s)\n",
    "    \n",
    "    s = s.strip()\n",
    "    try:\n",
    "        ## remove trailing number\n",
    "        while s[-1] in string.digits:\n",
    "            s = s[:-1]\n",
    "    except:\n",
    "        return ''\n",
    "    s = s.strip()\n",
    "    \n",
    "    s = s.strip()\n",
    "    try:\n",
    "        ## remove trailing non stop punctuation\n",
    "        while s[-1] in hanzi.non_stops:\n",
    "            s = s[:-1]\n",
    "    except:\n",
    "        return ''\n",
    "    s = s.strip()\n",
    "    \n",
    "    return s"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Post process the summary"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Load summary dict"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Utils"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "_dict = {\n",
    "    \"# The content is removed due to confidential concerns.\"\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "default_talents = [\"# The content is removed due to confidential concerns.\"]\n",
    "\n",
    "def generate_docx(doc, sum_sent, talents=default_talents, debug=False):    \n",
    "    no_duplicate_sents = []\n",
    "    \n",
    "    for talent in talents:\n",
    "        sents = sum_sent[talent]\n",
    "        \n",
    "        if len(sents) == 0:\n",
    "            continue\n",
    "        \n",
    "        doc.add_heading(_dict[talent], level=2)\n",
    "\n",
    "        sum_sents_buffer = []\n",
    "        \n",
    "        for sent in sents:\n",
    "            if sent not in no_duplicate_sents:\n",
    "                no_duplicate_sents.append(sent)\n",
    "                sum_sents_buffer.append(sent)\n",
    "        \n",
    "        for sent in sum_sents_buffer:\n",
    "#             doc.add_paragraph(sent)\n",
    "            doc.add_paragraph(sent, style='List Bullet')\n",
    "            \n",
    "#     for talent in default_talents:\n",
    "#         if talent not in talents:\n",
    "#             doc.add_heading(talent, level=2)\n",
    "#             doc.add_paragraph(\"無\")\n",
    "            \n",
    "    return doc"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [],
   "source": [
    "debug_tuple = [\n",
    "    \"# The content is removed due to confidential concerns.\"\n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [],
   "source": [
    "## STEP 0: load the summary data\n",
    "if COMMENT_AUGMENTATION:\n",
    "    significance_summary_docx_dir = os.path.join(\n",
    "        P.FP_SIGNIFICANCE_SUMMARY_DIR, TRAIN_OR_ALL, 'w', SIGNIFICANCE_MODEL_SAVE_DIR_NAME, \"epoch_{}_max_sent_{}\".format(EPOCH, MAX_SENT)\n",
    "    )\n",
    "    summary_docx_dir = os.path.join(\n",
    "        P.FP_SUMMARY_GENERATION_DIR, TRAIN_OR_ALL, 'w', SIGNIFICANCE_MODEL_SAVE_DIR_NAME, \"epoch_{}_max_sent_{}\".format(EPOCH, MAX_SENT)\n",
    "    )\n",
    "else:\n",
    "    significance_summary_docx_dir = os.path.join(\n",
    "        P.FP_SIGNIFICANCE_SUMMARY_DIR, TRAIN_OR_ALL, 'wo', SIGNIFICANCE_MODEL_SAVE_DIR_NAME, \"epoch_{}_max_sent_{}\".format(EPOCH, MAX_SENT)\n",
    "    )\n",
    "    summary_docx_dir = os.path.join(\n",
    "        P.FP_SUMMARY_GENERATION_DIR, TRAIN_OR_ALL, 'wo', SIGNIFICANCE_MODEL_SAVE_DIR_NAME, \"epoch_{}_max_sent_{}\".format(EPOCH, MAX_SENT)\n",
    "    )\n",
    "    \n",
    "uniqueness_summary_docx_dir = os.path.join(\n",
    "    P.FP_UNIQUENESS_SUMMARY_DIR, TRAIN_OR_ALL\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [],
   "source": [
    "if not os.path.exists(summary_docx_dir):\n",
    "    os.makedirs(summary_docx_dir)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "significance_summary_docx_dir"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "uniqueness_summary_docx_dir"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "summary_docx_dir"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [],
   "source": [
    "## load significance summary data\n",
    "fn_significance_summary_dict = os.path.join(significance_summary_docx_dir, \"summary_dict.pkl\")\n",
    "with open(fn_significance_summary_dict, 'rb') as f:\n",
    "    significance_summary_dict = pickle.load(f)\n",
    "    \n",
    "## load uniqueness summary data\n",
    "fn_uniqueness_summary_dict = os.path.join(uniqueness_summary_docx_dir, \"uniqueness_summary_dict.pkl\")\n",
    "with open(fn_uniqueness_summary_dict, 'rb') as f:\n",
    "    uniqueness_summary_dict = pickle.load(f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "uniqueness_summary_docx_dir"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "len(significance_summary_dict), len(uniqueness_summary_dict)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [],
   "source": [
    "debug = False"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Load applicant info"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_comments = D.read_df_comments()\n",
    "df_applicants = D.read_df_applicants()\n",
    "test_df = pd.read_csv(\"112_F_experiment.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_applicants = pd.concat([df_applicants, test_df])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [],
   "source": [
    "applicant_group_info = {}\n",
    "\n",
    "if VAL_OR_TEST == 'test':\n",
    "    for _, row in df_applicants.iterrows():\n",
    "        _year = row['year']\n",
    "        _id = row['id']\n",
    "        _name = row['name']\n",
    "        _group = row['group']\n",
    "        \n",
    "        applicant_group_info[(_year, _id, _name)] = _group"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [],
   "source": [
    "debug_tuple = [(112, 23010512, '莊詠宸')]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "pp_summary_dict = {}\n",
    "pp_summary_no_tag_dict = {}\n",
    "\n",
    "i = 0\n",
    "\n",
    "for info, sig_summary_info in tqdm(significance_summary_dict.items()):\n",
    "    if info not in uniqueness_summary_dict.keys():\n",
    "        continue\n",
    "    \n",
    "    ## STEP 1: Post process summary\n",
    "    _year = info[0]\n",
    "    _id = info[1]\n",
    "    _name = info[2]\n",
    "    _group = applicant_group_info[info]\n",
    "\n",
    "    if debug:\n",
    "        if info not in debug_tuple:\n",
    "            continue\n",
    "\n",
    "    i += 1\n",
    "            \n",
    "    uni_summary = uniqueness_summary_dict[info]\n",
    "            \n",
    "    sig_summary = sig_summary_info['summary']\n",
    "    title_weight = sig_summary_info['title_weight']\n",
    "\n",
    "    if debug:\n",
    "        print(info)\n",
    "        print(\"significance summary before post process\")\n",
    "        print(sig_summary)\n",
    "        print(\"=\"*10)\n",
    "\n",
    "    pp_summary = defaultdict(list)\n",
    "    pp_summary_no_tag = defaultdict(list)\n",
    "    buf_summary = copy.deepcopy(sig_summary)\n",
    "    \n",
    "    ## append uniqueness summary\n",
    "    cnt = 0\n",
    "    while cnt < UNI_MAX_SENT and len(uni_summary) > 0:\n",
    "        uni_summary = [sent for sent in uni_summary if not PP.is_empty_sent(sent)]\n",
    "        \n",
    "        if len(uni_summary) == 0:\n",
    "            break\n",
    "        \n",
    "        uni_summary_sent_embed = sbert_model.encode(uni_summary, show_progress_bar=False)\n",
    "        \n",
    "        buf_summary_sent = list(chain.from_iterable(buf_summary.values()))\n",
    "        buf_summary_sent = [sent for sent in buf_summary_sent if not PP.is_empty_sent(sent)]\n",
    "        \n",
    "        if len(buf_summary_sent) == 0:\n",
    "            buf_summary['獨特表現'].append(uni_summary[0])\n",
    "            sig_summary['獨特表現'].append(uni_summary[0])\n",
    "            cnt += 1\n",
    "            uni_summary[0] = ''\n",
    "            uni_summary = [sent for sent in uni_summary if not PP.is_empty_sent(sent)]\n",
    "            continue\n",
    "        \n",
    "        buf_summary_sent_embed = sbert_model.encode(buf_summary_sent, show_progress_bar=False)\n",
    "        \n",
    "#         print(uni_summary)\n",
    "        \n",
    "        sim_mat = np.array(cos_sim(uni_summary_sent_embed, buf_summary_sent_embed))\n",
    "        uni_sent_disimilarity = 1 - np.mean(sim_mat, axis=-1)\n",
    "        \n",
    "        idx = np.argmax(uni_sent_disimilarity)\n",
    "        \n",
    "        if debug:\n",
    "            print(\"disimilarity\", uni_sent_disimilarity[idx])\n",
    "            print(\"sent\", uni_summary[idx])\n",
    "            print(\"max sim\", np.max(sim_mat[idx]))\n",
    "        \n",
    "        ## append unique sentence that is most disimilar to significance summary\n",
    "        if np.max(sim_mat[idx]) < 0.94:\n",
    "            buf_summary['獨特表現'].append(uni_summary[idx])\n",
    "            sig_summary['獨特表現'].append(uni_summary[idx])\n",
    "            cnt += 1\n",
    "        uni_summary[idx] = ''\n",
    "        uni_summary = [sent for sent in uni_summary if not PP.is_empty_sent(sent)]\n",
    "    \n",
    "    if debug:\n",
    "        IO.print_dividing_line()\n",
    "    \n",
    "    for talent, sum_sents in sig_summary.items():\n",
    "        for sum_sent in sum_sents:\n",
    "            _bidx = buf_summary[talent].index(sum_sent)\n",
    "            rl_infos = rl_info_dict[(_year, _id)]\n",
    "            sent_evidence = sent_evidence_score_dict[info]\n",
    "\n",
    "            tag = \"\"\n",
    "\n",
    "            if sum_sent in rl_infos.keys():\n",
    "                ## 1. add citations (only for rl)\n",
    "                rl_info = rl_infos[sum_sent]\n",
    "\n",
    "                if info != '':\n",
    "                    tag = rl_info\n",
    "            else:\n",
    "                ## 2. add verified mark (only for not rl)\n",
    "                evidence = sent_evidence[sum_sent]\n",
    "\n",
    "                if evidence > EVIDENCE_SCORE_THRESHOLD:\n",
    "                    tag = \"已驗證\"\n",
    "\n",
    "            ## trim sentence that is too long\n",
    "            ## get the length of the sentence\n",
    "            sum_sent_len = get_sent_len(sum_sent)\n",
    "\n",
    "            if sum_sent_len > MAX_SENT_LEN:                \n",
    "                if debug:\n",
    "                    print('sum_sent_to_trim:', sum_sent)\n",
    "\n",
    "                debug_info = all_chunk_debug_info_buffer[info]\n",
    "\n",
    "                ext_summary = list(chain.from_iterable(buf_summary.values()))\n",
    "                ext_summary = [sent for sent in ext_summary if sent != sum_sent]\n",
    "                ext_summary = '。'.join(ext_summary)\n",
    "\n",
    "                if debug:\n",
    "                    print('ext_summary:', ext_summary)\n",
    "                sum_sent = long_sentence_post_process(sum_sent, debug_info, ext_summary, debug)\n",
    "\n",
    "                if debug:\n",
    "                    print('sum_sent_after_trim:', sum_sent, \"len:\", get_sent_len(sum_sent))\n",
    "                    print('-'*10)\n",
    "\n",
    "            sum_sent = sentence_post_process(sum_sent)\n",
    "            ## replace sum sent in original summary\n",
    "            buf_summary[talent][_bidx] = sum_sent\n",
    "\n",
    "            ## [TODO] process sentence with unclosed parenthesis (remove unreasonable sentence)\n",
    "            if sum_sent == '':\n",
    "                continue\n",
    "\n",
    "            pp_summary_no_tag[talent].append(sum_sent)\n",
    "                \n",
    "#             if tag:\n",
    "#                 sum_sent = \"{}（{}）\".format(sum_sent, tag)\n",
    "\n",
    "            pp_summary[talent].append(sum_sent)\n",
    "\n",
    "    talent_list = sorted(title_weight.items(), key=lambda i: -i[1])\n",
    "    talent_list = [t[0] for t in talent_list]\n",
    "    talent_list.append('獨特表現')\n",
    "    \n",
    "    if debug:\n",
    "        print('='*10)\n",
    "        print(\"summary after post process\")\n",
    "        print(pp_summary)\n",
    "        IO.print_dividing_line()\n",
    "\n",
    "    pp_summary_dict[info] = pp_summary\n",
    "    pp_summary_no_tag_dict[info] = pp_summary_no_tag\n",
    "    \n",
    "#     if not debug:\n",
    "#         ## STEP 2: Write post processed summary to docx file\n",
    "#         doc = Document()\n",
    "#         doc = generate_docx(doc, pp_summary, talent_list)\n",
    "\n",
    "#         fn = \"{}.docx\".format(\"_\".join(map(str, info)))\n",
    "#         if debug:\n",
    "#             print(fn)\n",
    "            \n",
    "#         if VAL_OR_TEST == 'all':\n",
    "#             doc.save(os.path.join(summary_docx_dir, fn))\n",
    "#         elif VAL_OR_TEST == 'test':\n",
    "#             _group_dir = os.path.join(summary_docx_dir, _group, 'docx')\n",
    "            \n",
    "#             if not os.path.exists(_group_dir):\n",
    "#                 os.mkdir(_group_dir)\n",
    "            \n",
    "#             doc.save(os.path.join(_group_dir, fn))\n",
    "            \n",
    "            \n",
    "i"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pp_summary_no_tag_dict"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Add our summary to test_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "metadata": {},
   "outputs": [],
   "source": [
    "our_summary_list = []\n",
    "\n",
    "for _, row in test_df.iterrows():\n",
    "    _year = row['year']\n",
    "    _id = row['id']\n",
    "    _name = row['name']\n",
    "    \n",
    "    summary_dict = pp_summary_no_tag_dict[(_year, _id, _name)]\n",
    "    summary = list(chain.from_iterable(summary_dict.values()))\n",
    "    summary = ''.join(summary)\n",
    "    \n",
    "    our_summary_list.append(summary)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_df['our summary'] = our_summary_list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Quantitative Experiments"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_test_comments = pd.read_csv('112_comments.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_test_comments.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_test_comments.iloc[0][4:].to_list()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Statistics for comments"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "comments_list = []\n",
    "\n",
    "for _, row in test_df.iterrows():\n",
    "    _year = row['year']\n",
    "    _id = row['id']\n",
    "    _name = row['name']\n",
    "    \n",
    "    ## query comments\n",
    "    row = df_test_comments.query(\"`year` == @_year and `id` == @_id\")\n",
    "    \n",
    "    comments = []\n",
    "    \n",
    "    if len(row) > 0:\n",
    "        for i in range(4, 4+5):\n",
    "            comments.append(row.iat[0, i])\n",
    "        comments = [c for c in comments if not PP.is_empty_sent(c)]\n",
    "        \n",
    "    print(comments)\n",
    "    print(_year, _id, _name)\n",
    "    print(len(comments))\n",
    "    \n",
    "    comments_list.append(comments)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Avg len"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 168,
   "metadata": {},
   "outputs": [],
   "source": [
    "def calculate_word_count(ss):\n",
    "    if not ss:\n",
    "        return 0\n",
    "    \n",
    "    ## check the language of the document\n",
    "    zh_char_count = sum([1 for ch in ss if PP.is_zh_character(ch)])\n",
    "    zh_char_rate = zh_char_count / len(ss)\n",
    "    \n",
    "    if zh_char_rate < 0.1: ## english document preprocess\n",
    "        tokens = len(ss.split(' '))\n",
    "    else: ## chinese document preprocess\n",
    "        tokens = len(ss)\n",
    "\n",
    "    return tokens"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 177,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "test_comments = list(chain.from_iterable(test_df['comments'].to_list()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pd.Series(list(map(calculate_word_count, test_comments))).describe()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Avg comment per application"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 113,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_df['comments'] = comments_list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_df['comments'].apply(len).describe()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Prepare data pair"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_df.columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 138,
   "metadata": {},
   "outputs": [],
   "source": [
    "row_data = []\n",
    "our_summary_test = []\n",
    "chatgpt_summary_test = []\n",
    "comment_test = []\n",
    "\n",
    "for _, row in test_df.iterrows():\n",
    "    ours = row['chat gpt summary']\n",
    "    chatgpt = row['our summary']\n",
    "    comments = row['comments']\n",
    "    \n",
    "    for c in comments:\n",
    "        our_summary_test.append(ours)\n",
    "        chatgpt_summary_test.append(chatgpt)\n",
    "        comment_test.append(c)\n",
    "        row_data.append({\n",
    "            'chatgpt': chatgpt,\n",
    "            'ours': ours,\n",
    "            'comment': c\n",
    "        })"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_test_pair = pd.DataFrame(row_data)\n",
    "df_test_pair.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "len(our_summary_test), len(chatgpt_summary_test), len(comment_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Cosine Similarity"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 182,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sentence_transformers.util import cos_sim"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 183,
   "metadata": {},
   "outputs": [],
   "source": [
    "our_summary_test_embed = sbert_model.encode(our_summary_test)\n",
    "chatgpt_summary_test_embed = sbert_model.encode(chatgpt_summary_test)\n",
    "comment_test_embed = sbert_model.encode(comment_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 193,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([0.9260, 0.7706, 0.4829, 0.8467, 0.7977, 0.7653, 0.7873, 0.8894, 0.8538,\n",
       "        0.6683, 0.7522, 0.9457, 0.8995, 0.8650, 0.8305, 0.8306, 0.9190, 0.8940,\n",
       "        0.8617, 0.9130, 0.6883, 0.8256, 0.9646, 0.8966, 0.9221, 0.8510, 0.8722,\n",
       "        0.8225, 0.7977, 0.8136, 0.9149, 0.8047, 0.7436, 0.9153, 0.9401, 0.8621,\n",
       "        0.8346, 0.9183, 0.8446, 0.8653, 0.8705, 0.7727, 0.8741, 0.8380, 0.8270,\n",
       "        0.6944, 0.9113, 0.8405, 0.8368, 0.9052, 0.5723, 0.5894, 0.8876, 0.8496,\n",
       "        0.8417, 0.7801, 0.8857, 0.8595, 0.9413, 0.9128, 0.8731, 0.9409, 0.7675,\n",
       "        0.7889, 0.8755, 0.8966, 0.9276, 0.8308, 0.8685, 0.9420, 0.9324, 0.8664,\n",
       "        0.8071, 0.9354, 0.9565, 0.9002, 0.8960, 0.7641])"
      ]
     },
     "execution_count": 193,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "## ours\n",
    "cos_sim(our_summary_test_embed, comment_test_embed).diagonal()."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### BERTScore"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 121,
   "metadata": {},
   "outputs": [],
   "source": [
    "from bert_score import score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 122,
   "metadata": {},
   "outputs": [],
   "source": [
    "def calculate_bert_score(cands, refs, rescale=False, verbose=False):\n",
    "    return score(\n",
    "        cands,\n",
    "        refs,\n",
    "        lang=\"zh\",\n",
    "    #     model_type=MODEL_TYPE,\n",
    "    #     num_layers=LAYER,\n",
    "        verbose=verbose,\n",
    "        device=0,\n",
    "        batch_size=64,\n",
    "    #     idf=False,\n",
    "        rescale_with_baseline=rescale\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 123,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of the model checkpoint at bert-base-chinese were not used when initializing BertModel: ['cls.predictions.transform.dense.weight', 'cls.predictions.decoder.weight', 'cls.seq_relationship.bias', 'cls.predictions.bias', 'cls.predictions.transform.LayerNorm.bias', 'cls.predictions.transform.LayerNorm.weight', 'cls.seq_relationship.weight', 'cls.predictions.transform.dense.bias']\n",
      "- This IS expected if you are initializing BertModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing BertModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n"
     ]
    }
   ],
   "source": [
    "## ours\n",
    "ours_bs_P, ours_bs_R, ours_bs_F1 = \\\n",
    "    calculate_bert_score(our_summary_test, comment_test, rescale=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 126,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Ours\n",
      "p: 0.526666\n",
      "r: 0.628861\n",
      "f: 0.572000\n"
     ]
    }
   ],
   "source": [
    "print(\"Ours\")\n",
    "    \n",
    "print(\"p: {:4f}\".format(torch.mean(ours_bs_P)))\n",
    "print(\"r: {:4f}\".format(torch.mean(ours_bs_R)))\n",
    "print(\"f: {:4f}\".format(torch.mean(ours_bs_F1)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 124,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of the model checkpoint at bert-base-chinese were not used when initializing BertModel: ['cls.predictions.transform.dense.weight', 'cls.predictions.decoder.weight', 'cls.seq_relationship.bias', 'cls.predictions.bias', 'cls.predictions.transform.LayerNorm.bias', 'cls.predictions.transform.LayerNorm.weight', 'cls.seq_relationship.weight', 'cls.predictions.transform.dense.bias']\n",
      "- This IS expected if you are initializing BertModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing BertModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n"
     ]
    }
   ],
   "source": [
    "## chatgpt\n",
    "chatgpt_bs_P, chatgpt_bs_R, chatgpt_bs_F1 = \\\n",
    "    calculate_bert_score(chatgpt_summary_test, comment_test, rescale=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 127,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ChatGPT\n",
      "p: 0.517770\n",
      "r: 0.632340\n",
      "f: 0.568303\n"
     ]
    }
   ],
   "source": [
    "print(\"ChatGPT\")\n",
    "    \n",
    "print(\"p: {:4f}\".format(torch.mean(chatgpt_bs_P)))\n",
    "print(\"r: {:4f}\".format(torch.mean(chatgpt_bs_R)))\n",
    "print(\"f: {:4f}\".format(torch.mean(chatgpt_bs_F1)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### ROUGE-r"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 131,
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import BertTokenizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tokenizer = BertTokenizer.from_pretrained('bert-base-multilingual-cased')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 133,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_tokenized_sentences(batch_sent, return_sent=False):\n",
    "    batch_tokens = tokenizer(\n",
    "        batch_sent, \n",
    "        return_tensors=None, \n",
    "        return_token_type_ids=False, \n",
    "        return_attention_mask=False,\n",
    "        add_special_tokens=False\n",
    "    )\n",
    "    \n",
    "    batch_tokens = batch_tokens['input_ids']\n",
    "    tokenized_sents_id = [' '.join(str(_id) for _id in tokens) for tokens in batch_tokens]\n",
    "    tokenized_sents = tokenizer.batch_decode(batch_tokens)\n",
    "    \n",
    "    if return_sent:\n",
    "        return tokenized_sents_id, tokenized_sents\n",
    "    else:\n",
    "        return tokenized_sents_id"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 134,
   "metadata": {},
   "outputs": [],
   "source": [
    "from rouge_score import rouge_scorer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 135,
   "metadata": {},
   "outputs": [],
   "source": [
    "rouge_metrics = ['rouge1', 'rouge2', 'rougeL']\n",
    "cls_metrics = ['precision', 'recall', 'f1']\n",
    "\n",
    "scorer = rouge_scorer.RougeScorer(rouge_metrics)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 136,
   "metadata": {},
   "outputs": [],
   "source": [
    "def calculate_rouge_score(cand, ref):\n",
    "    ## preprocess\n",
    "    ref = get_tokenized_sentences([ref])[0]\n",
    "    cand = get_tokenized_sentences([cand])[0]\n",
    "        \n",
    "    return scorer.score(ref, cand)  ## reference, candidate"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Prepare data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## ours\n",
    "\n",
    "df_rouge_ours = pd.json_normalize(df_test_pair.progress_apply(\n",
    "    lambda row: calculate_rouge_score(row['ours'], row['comment']), axis=1\n",
    "))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## chatgpt\n",
    "\n",
    "df_rouge_chatgpt = pd.json_normalize(df_test_pair.progress_apply(\n",
    "    lambda row: calculate_rouge_score(row['chatgpt'], row['comment']), axis=1\n",
    "))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 164,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Ours\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>rouge1</th>\n",
       "      <th>rouge2</th>\n",
       "      <th>rougeL</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>precision</th>\n",
       "      <td>0.087311</td>\n",
       "      <td>0.028224</td>\n",
       "      <td>0.055257</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>recall</th>\n",
       "      <td>0.460914</td>\n",
       "      <td>0.139751</td>\n",
       "      <td>0.319635</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>f1</th>\n",
       "      <td>0.135570</td>\n",
       "      <td>0.042953</td>\n",
       "      <td>0.086609</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "             rouge1    rouge2    rougeL\n",
       "precision  0.087311  0.028224  0.055257\n",
       "recall     0.460914  0.139751  0.319635\n",
       "f1         0.135570  0.042953  0.086609"
      ]
     },
     "execution_count": 164,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# rouge_metrics: 'rouge1', 'rouge2', 'rougeL'\n",
    "# cls_metrics: 'precision', 'recall', 'f1'\n",
    "score_dict = {}\n",
    "\n",
    "for rm in rouge_metrics:\n",
    "    rouge_score_dict = {}\n",
    "    r_scores = df_rouge_ours[rm]\n",
    "    \n",
    "    for i, cm in enumerate(cls_metrics):\n",
    "        scores = r_scores.apply(lambda t: t[i])\n",
    "        score = scores.mean()\n",
    "        rouge_score_dict[cm] = score\n",
    "        \n",
    "    score_dict[rm] = rouge_score_dict\n",
    "\n",
    "print(\"Ours\")\n",
    "pd.DataFrame(score_dict)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 165,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ChatGPT\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>rouge1</th>\n",
       "      <th>rouge2</th>\n",
       "      <th>rougeL</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>precision</th>\n",
       "      <td>0.081790</td>\n",
       "      <td>0.025533</td>\n",
       "      <td>0.050778</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>recall</th>\n",
       "      <td>0.459849</td>\n",
       "      <td>0.133773</td>\n",
       "      <td>0.311973</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>f1</th>\n",
       "      <td>0.126288</td>\n",
       "      <td>0.038353</td>\n",
       "      <td>0.079070</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "             rouge1    rouge2    rougeL\n",
       "precision  0.081790  0.025533  0.050778\n",
       "recall     0.459849  0.133773  0.311973\n",
       "f1         0.126288  0.038353  0.079070"
      ]
     },
     "execution_count": 165,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# rouge_metrics: 'rouge1', 'rouge2', 'rougeL'\n",
    "# cls_metrics: 'precision', 'recall', 'f1'\n",
    "score_dict = {}\n",
    "\n",
    "for rm in rouge_metrics:\n",
    "    rouge_score_dict = {}\n",
    "    r_scores = df_rouge_chatgpt[rm]\n",
    "    \n",
    "    for i, cm in enumerate(cls_metrics):\n",
    "        scores = r_scores.apply(lambda t: t[i])\n",
    "        score = scores.mean()\n",
    "        rouge_score_dict[cm] = score\n",
    "        \n",
    "    score_dict[rm] = rouge_score_dict\n",
    "\n",
    "print(\"ChatGPT\")\n",
    "pd.DataFrame(score_dict)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "research_venv",
   "language": "python",
   "name": "research_venv"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
